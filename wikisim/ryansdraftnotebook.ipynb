{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[u'10,', u'9,', u'8,', u'7,', u'6,', u'5,', u'4,', u'3,', u'2,', u'1', u'is', u'the', u'fourth', u'album', u'by', u'Midnight Oil', u'that', u'was', u'released', u'on', u'vinyl', u'in', u'1982', u'under', u'the', u'Columbia Records', u'label.', u'It', u'peaked', u'at', u'No.', u'3', u'on', u'the', u'Australian', u'Kent Music Report', u'Albums', u'Chart', u'and', u'remained', u'on', u'the', u'chart', u'for', u'171', u'weeks.', u'The', u\"album's\", u'closing', u'track', u'\"Somebody\\'s', u'Trying', u'to', u'Tell', u'Me', u'Something\"', u'contains', u'a', u'note', u'held', u'by', u'the', u'group', u'for', u'what', u'seems', u'like', u'an', u'eternity,', u'which', u'would', u'continue', u'into', u'the', u\"album's\", u'runout', u'groove,', u'and', u'emulated', u'on', u'the', u'CD', u'version', u'for', u'just', u'over', u'40', u'seconds.', u'This', u'is', u'an', u'approximation', u'of', u'a', u'locked groove', u',', u'a', u'gimmick', u'used', u'a', u'number', u'of', u'times', u'on', u'vinyl', u'albums', u'(such', u'as', u'\"', u'Diamond Dogs', u'\"', u'and', u'\"', u\"Sgt. Pepper's Lonely Hearts Club Band\", u'\")', u'where', u'the', u'ending', u'sound', u'would', u'continue', u'into', u'the', u'runout', u'groove,', u'with', u'which', u'the', u'sound', u'would', u'continue', u'on', u'until', u'the', u'turntable', u'arm', u'was', u'lifted', u'off,', u'or', u'the', u'automatic', u'return', u'on', u'some', u'turntables', u'would', u'kick', u'in.', u'In', u'October', u'2010,', u'the', u'album', u'was', u'listed', u'in', u'the', u'top', u'30', u'in', u'the', u'book,', u'\"', u'100 Best Australian Albums', u'\"', u'with', u\"1987's\", u'\"', u'Diesel and Dust', u'\"', u'at', u'No.', u'1.']\n",
      "[[53, 65, u'Midnight Oil'], [111, 127, u'Columbia Records'], [172, 189, u'Kent Music Report'], [518, 531, u'locked groove'], [594, 606, u'Diamond Dogs'], [615, 652, u\"Sgt. Pepper's Lonely Hearts Club Band\"], [922, 948, u'100 Best Australian Albums'], [965, 980, u'Diesel and Dust']]\n",
      "[[37, 49, u'fourth album'], [44, 49, u'album'], [53, 65, u'Midnight Oil'], [62, 65, u'Oil'], [87, 92, u'vinyl'], [111, 127, u'Columbia Records'], [161, 171, u'Australian'], [172, 189, u'Kent Music Report'], [177, 182, u'Music'], [190, 202, u'Albums Chart'], [223, 228, u'chart'], [264, 269, u'track'], [300, 309, u'Something'], [322, 326, u'note'], [339, 344, u'group'], [368, 376, u'eternity'], [416, 429, u'runout groove'], [423, 429, u'groove'], [451, 453, u'CD'], [499, 512, u'approximation'], [518, 531, u'locked groove'], [525, 531, u'groove'], [536, 543, u'gimmick'], [551, 557, u'number'], [570, 575, u'vinyl'], [576, 582, u'albums'], [594, 606, u'Diamond Dogs'], [602, 606, u'Dogs'], [615, 652, u\"Sgt. Pepper's Lonely Hearts Club Band\"], [629, 642, u'Lonely Hearts'], [636, 642, u'Hearts'], [673, 678, u'sound'], [703, 716, u'runout groove'], [710, 716, u'groove'], [733, 738, u'sound'], [767, 776, u'turntable'], [777, 780, u'arm'], [804, 813, u'automatic'], [829, 839, u'turntables'], [876, 881, u'album'], [914, 918, u'book'], [926, 941, u'Best Australian'], [931, 941, u'Australian'], [965, 980, u'Diesel and Dust'], [976, 980, u'Dust']]\n",
      "\n",
      "correct: 7\n",
      "found: 45\n",
      "correct: 7\n",
      "actual: 8\n",
      "0.155555555556 0.875\n",
      "\n",
      "2\n",
      "[u'2.13.61,', u'Inc.', u'is', u'a', u'publisher', u'and', u'record', u'company', u'founded', u'by', u'musician', u'Henry Rollins', u'and', u'named', u'after', u'his', u'date', u'of', u'birth', u'(February', u'13,', u'1961).', u'The', u'company', u'has', u'released', u'albums', u'by', u'the', u'Rollins Band', u',', u'all', u'of', u\"Rollins's\", u'spoken-word', u'work,', u'and', u'numerous', u'books.', u'It', u'is', u'based', u'in', u'Los Angeles, California', u'.']\n",
      "[[68, 81, u'Henry Rollins'], [176, 188, u'Rollins Band'], [261, 284, u'Los Angeles, California']]\n",
      "[[19, 28, u'publisher'], [33, 47, u'record company'], [40, 47, u'company'], [59, 67, u'musician'], [68, 81, u'Henry Rollins'], [74, 81, u'Rollins'], [102, 115, u'date of birth'], [110, 115, u'birth'], [141, 148, u'company'], [162, 168, u'albums'], [172, 188, u'the Rollins Band'], [176, 188, u'Rollins Band'], [208, 219, u'spoken-word'], [215, 219, u'word'], [239, 244, u'books'], [261, 284, u'Los Angeles, California'], [274, 284, u'California']]\n",
      "\n",
      "correct: 3\n",
      "found: 17\n",
      "correct: 3\n",
      "actual: 3\n",
      "0.176470588235 1.0\n",
      "\n",
      "3\n",
      "[u'251', u'Menlove', u'Avenue', u'in', u'Liverpool', u',', u'England,', u'named', u'Mendips,', u'is', u'the', u'childhood', u'home', u'of', u'John Lennon', u',', u'singer', u'and', u'songwriter', u'with', u'The Beatles', u'.', u'The', u'Grade II listed building', u'is', u'preserved', u'by', u'the', u'National Trust', u'.']\n",
      "[[22, 31, u'Liverpool'], [83, 94, u'John Lennon'], [124, 135, u'The Beatles'], [142, 166, u'Grade II listed building'], [187, 201, u'National Trust']]\n",
      "[[4, 18, u'Menlove Avenue'], [22, 31, u'Liverpool'], [34, 41, u'England'], [49, 56, u'Mendips'], [65, 74, u'childhood'], [75, 79, u'home'], [83, 94, u'John Lennon'], [88, 94, u'Lennon'], [97, 103, u'singer'], [108, 118, u'songwriter'], [124, 135, u'The Beatles'], [128, 135, u'Beatles'], [142, 157, u'Grade II listed'], [148, 150, u'II'], [158, 166, u'building'], [183, 201, u'the National Trust'], [187, 201, u'National Trust'], [196, 201, u'Trust']]\n",
      "\n",
      "correct: 4\n",
      "found: 18\n",
      "correct: 4\n",
      "actual: 5\n",
      "0.222222222222 0.8\n",
      "\n",
      "4\n",
      "[u'25 May', u'-', u'A', u'fire', u'during', u'the', u'745th', u'performance', u'there', u'of', u'\"', u'Mignon', u'\"', u'largely', u'destroys', u'the', u'second', u'Salle', u'Favart,', u'home', u'of', u'the', u'Op\\xe9ra-Comique', u'in', u'Paris;', u'84', u'people', u'are', u'recorded', u'dead.']\n",
      "[[0, 6, u'25 May'], [56, 62, u'Mignon'], [119, 132, u'Op\\xe9ra-Comique']]\n",
      "[[11, 15, u'fire'], [33, 44, u'performance'], [56, 62, u'Mignon'], [86, 92, u'second'], [93, 105, u'Salle Favart'], [99, 105, u'Favart'], [107, 111, u'home'], [119, 132, u'Op\\xe9ra-Comique'], [136, 141, u'Paris'], [146, 152, u'people'], [166, 170, u'dead']]\n",
      "\n",
      "correct: 2\n",
      "found: 11\n",
      "correct: 2\n",
      "actual: 3\n",
      "0.181818181818 0.666666666667\n",
      "\n",
      "5\n",
      "[u'461', u'Ocean', u'Boulevard', u'is', u'the', u'second', u'studio album', u'by', u'the', u'British', u'rock', u'musician', u'Eric Clapton', u'that', u'marked', u'his', u'return', u'to', u'form', u'after', u'recovering', u'from', u'a', u'three-year', u'addiction', u'to', u'heroin.', u'The', u'album', u'was', u'released', u'in', u'late', u'July', u'1974', u'for', u'RSO', u'Records,', u'shortly', u'after', u'the', u'record', u'company', u'released', u'the', u'hit', u'single', u'\"', u'I Shot the Sheriff', u'\"', u'in', u'early', u'July', u'the', u'same', u'year.', u'The', u'album', u'topped', u'various', u'international', u'charts', u'and', u'sold', u'more', u'than', u'two', u'million', u'copies.', u'It', u'was', u'also', u'one', u'of', u'the', u'first', u'\"pop', u'music\"', u'albums', u'to', u'be', u'released', u'in', u'the', u'USSR', u'.']\n",
      "[[34, 46, u'studio album'], [76, 88, u'Eric Clapton'], [293, 311, u'I Shot the Sheriff'], [498, 502, u'USSR']]\n",
      "[[4, 19, u'Ocean Boulevard'], [10, 19, u'Boulevard'], [27, 33, u'second'], [34, 46, u'studio album'], [41, 46, u'album'], [54, 61, u'British'], [62, 66, u'rock'], [67, 75, u'musician'], [76, 88, u'Eric Clapton'], [81, 88, u'Clapton'], [155, 164, u'addiction'], [180, 185, u'album'], [221, 232, u'RSO Records'], [252, 266, u'record company'], [259, 266, u'company'], [280, 290, u'hit single'], [284, 290, u'single'], [293, 311, u'I Shot the Sheriff'], [304, 311, u'Sheriff'], [347, 352, u'album'], [368, 381, u'international'], [382, 388, u'charts'], [458, 467, u'pop music'], [462, 467, u'music'], [469, 475, u'albums'], [498, 502, u'USSR']]\n",
      "\n",
      "correct: 4\n",
      "found: 26\n",
      "correct: 4\n",
      "actual: 4\n",
      "0.153846153846 1.0\n",
      "\n",
      "6\n",
      "[u'Aagtdorp', u'()', u'is', u'a', u'village', u'in', u'the', u'Dutch', u'province', u'of', u'North Holland', u'.', u'It', u'is', u'a', u'part', u'of', u'the', u'municipality', u'of', u'Bergen', u',', u'and', u'lies', u'about', u'northwest', u'of', u'Alkmaar', u'.', u'Aagtdorp', u'is', u'the', u'birthplace', u'of', u'the', u'popular', u'dance', u'move', u'the', u'\"dab\"', u'where', u'the', u'performer', u'puts', u'the', u'inside', u'of', u'his', u'elbow', u'to', u'his', u'mouth', u'and', u'throws', u'his', u'other', u'arm', u'up', u'in', u'the', u'air']\n",
      "[[32, 37, u'Dutch'], [50, 63, u'North Holland'], [102, 108, u'Bergen'], [139, 146, u'Alkmaar']]\n",
      "[[0, 8, u'Aagtdorp'], [17, 24, u'village'], [32, 37, u'Dutch'], [38, 46, u'province'], [50, 63, u'North Holland'], [56, 63, u'Holland'], [86, 98, u'municipality'], [102, 108, u'Bergen'], [126, 135, u'northwest'], [139, 146, u'Alkmaar'], [149, 157, u'Aagtdorp'], [165, 175, u'birthplace'], [183, 190, u'popular'], [191, 196, u'dance'], [207, 210, u'dab'], [222, 231, u'performer'], [255, 260, u'elbow'], [268, 273, u'mouth'], [295, 298, u'arm'], [309, 312, u'air']]\n",
      "\n",
      "correct: 4\n",
      "found: 20\n",
      "correct: 4\n",
      "actual: 4\n",
      "0.2 1.0\n",
      "\n",
      "7\n",
      "[u'\"\\xc4\"', u'and', u'\"\\xe4\"', u'are', u'both', u'characters', u'that', u'represent', u'either', u'a', u'letter', u'from', u'several', u'extended', u'Latin alphabet', u's,', u'or', u'the', u'letter', u'A', u'with', u'an', u'umlaut mark or diaeresis', u'.']\n",
      "[[85, 99, u'Latin alphabet'], [117, 118, u'A'], [127, 151, u'umlaut mark or diaeresis']]\n",
      "[[1, 2, u'\\xc4'], [21, 31, u'characters'], [56, 62, u'letter'], [76, 99, u'extended Latin alphabet'], [85, 99, u'Latin alphabet'], [91, 99, u'alphabet'], [110, 116, u'letter'], [127, 151, u'umlaut mark or diaeresis'], [134, 138, u'mark'], [142, 151, u'diaeresis']]\n",
      "\n",
      "correct: 2\n",
      "found: 10\n",
      "correct: 2\n",
      "actual: 3\n",
      "0.2 0.666666666667\n",
      "\n",
      "8\n",
      "[u'Abarim', u'(', u'Hebrew', u':', u'\\u05d4\\u05b8\\u05e8\\u05b5\\u05d9', u'\\u05d4\\u05b8\\u05e2\\u05b2\\u05d1\\u05b8\\u05e8\\u05b4\\u05d9\\u05dd,', u'\"Avarim\",\"Har', u'Ha-\\'Avarim\",', u'or', u'\"Harei', u'Ha-\\'Avarim\";', u'Septuagint', u'\"to', u'oros', u'to', u'Abarim,', u'en', u'to', u'peran', u'tou', u'Iordanou\",', u'mountain', u'Abarim,', u'mountains', u'of', u'Abarim)', u'is', u'a', u'mountain', u'range', u'across', u'Jordan', u',', u'to', u'the', u'east', u'and', u'south-east', u'of', u'the', u'Dead Sea', u',', u'extending', u'from', u'Mount Nebo', u'\\u2014', u'its', u'highest', u'point', u'\\u2014', u'in', u'the', u'north,', u'perhaps', u'to', u'the', u'Arabian', u'desert', u'in', u'the', u'south.', u'The', u'Vulgate', u'(', u'Deuteronomy', u'32:49)', u'gives', u'its', u'etymological', u'meaning', u'as', u'\"passages\".', u'Its', u'northern', u'part', u'was', u'called', u'Phasga', u'(or', u'Pisgah),', u'and', u'the', u'highest', u'peak', u'of', u'Phasga', u'was', u'Mount', u'Nebo', u'(', u'Numbers', u'23:14;', u'27:12;', u'21:20;', u'32:47;', u'Deuteronomy', u'3:27;', u'34:1;', u'32:49).']\n",
      "[[9, 15, u'Hebrew'], [209, 215, u'Jordan'], [252, 260, u'Dead Sea'], [278, 288, u'Mount Nebo'], [373, 380, u'Vulgate'], [383, 394, u'Deuteronomy'], [477, 483, u'Phasga'], [545, 552, u'Numbers']]\n",
      "[[0, 6, u'Abarim'], [9, 15, u'Hebrew'], [18, 34, u'\\u05d4\\u05b8\\u05e8\\u05b5\\u05d9 \\u05d4\\u05b8\\u05e2\\u05b2\\u05d1\\u05b8\\u05e8\\u05b4\\u05d9\\u05dd'], [46, 49, u'Har'], [50, 52, u'Ha'], [73, 75, u'Ha'], [86, 96, u'Septuagint'], [109, 115, u'Abarim'], [144, 152, u'mountain'], [153, 159, u'Abarim'], [161, 170, u'mountains'], [174, 180, u'Abarim'], [187, 201, u'mountain range'], [196, 201, u'range'], [209, 215, u'Jordan'], [248, 260, u'the Dead Sea'], [252, 260, u'Dead Sea'], [278, 288, u'Mount Nebo'], [284, 288, u'Nebo'], [295, 308, u'highest point'], [303, 308, u'point'], [340, 354, u'Arabian desert'], [348, 354, u'desert'], [369, 380, u'The Vulgate'], [373, 380, u'Vulgate'], [383, 394, u'Deuteronomy'], [412, 424, u'etymological'], [425, 432, u'meaning'], [452, 465, u'northern part'], [477, 483, u'Phasga'], [488, 494, u'Pisgah'], [505, 517, u'highest peak'], [513, 517, u'peak'], [521, 527, u'Phasga'], [532, 542, u'Mount Nebo'], [538, 542, u'Nebo'], [545, 552, u'Numbers'], [581, 592, u'Deuteronomy']]\n",
      "\n",
      "correct: 8\n",
      "found: 38\n",
      "correct: 8\n",
      "actual: 8\n",
      "0.210526315789 1.0\n",
      "\n",
      "9\n",
      "[u'Abay', u'(Ibrahim)', u'Qunanbayuli', u'()', u'(August', u'10,', u'1845', u'\\u2013', u'July', u'6,', u'1904)', u'was', u'a', u'Kazakh', u'poet,', u'composer', u'and', u'philosopher.', u'He', u'was', u'also', u'a', u'cultural', u'reformer', u'toward', u'European', u'and', u'Russian', u'cultures', u'on', u'the', u'basis', u'of', u'enlightened', u'Islam', u'.']\n",
      "[[69, 75, u'Kazakh'], [205, 210, u'Islam']]\n",
      "[[0, 4, u'Abay'], [6, 13, u'Ibrahim'], [31, 40, u'August 10'], [49, 55, u'July 6'], [69, 75, u'Kazakh'], [76, 80, u'poet'], [82, 90, u'composer'], [95, 106, u'philosopher'], [122, 130, u'cultural'], [147, 155, u'European'], [160, 167, u'Russian'], [168, 176, u'cultures'], [184, 189, u'basis'], [205, 210, u'Islam']]\n",
      "\n",
      "correct: 2\n",
      "found: 14\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.142857142857 1.0\n",
      "\n",
      "10\n",
      "[u'Abba', u'Eban', u'(;', u';', u'born', u'Aubrey', u'Solomon', u'Meir', u'Eban;', u'later', u'adopted', u'Abba', u'Solomon', u'Meir', u'Eban;', u'2', u'February', u'1915', u'\\u2013', u'17', u'November', u'2002)', u'was', u'an', u'Israel', u'i', u'diplomat', u'and', u'politician,', u'and', u'a', u'scholar', u'of', u'the', u'Arabic', u'and', u'Hebrew', u'languages.']\n",
      "[[127, 133, u'Israel']]\n",
      "[[0, 9, u'Abba Eban'], [5, 9, u'Eban'], [20, 39, u'Aubrey Solomon Meir'], [27, 34, u'Solomon'], [35, 39, u'Meir'], [40, 44, u'Eban'], [60, 64, u'Abba'], [65, 72, u'Solomon'], [73, 77, u'Meir'], [78, 82, u'Eban'], [105, 118, u'November 2002'], [127, 133, u'Israel'], [136, 144, u'diplomat'], [149, 159, u'politician'], [167, 174, u'scholar'], [182, 188, u'Arabic'], [193, 199, u'Hebrew'], [200, 209, u'languages']]\n",
      "\n",
      "correct: 1\n",
      "found: 18\n",
      "correct: 1\n",
      "actual: 1\n",
      "0.0555555555556 1.0\n",
      "\n",
      "11\n",
      "[u'ABBA', u'was', u'a', u'Swedish', u'pop', u'music', u'group.']\n",
      "[[0, 4, u'ABBA']]\n",
      "[[0, 4, u'ABBA'], [11, 18, u'Swedish'], [19, 28, u'pop music'], [23, 28, u'music'], [29, 34, u'group']]\n",
      "\n",
      "correct: 1\n",
      "found: 5\n",
      "correct: 1\n",
      "actual: 1\n",
      "0.2 1.0\n",
      "\n",
      "12\n",
      "[u'Abbotsford', u'is', u'a', u'city', u'in', u'Clark', u'(mostly)', u'and', u'Marathon', u'counties', u'in', u'the', u'U.S. state', u'of', u'Wisconsin', u'.', u'It', u'is', u'part', u'of', u'the', u'Wausau, Wisconsin', u'Metropolitan Statistical Area', u'.', u'The', u'population', u'was', u'2,310', u'at', u'the', u'2010', u'census.', u'Of', u'this,', u'1,616', u'were', u'in', u'Clark', u'County,', u'and', u'694', u'were', u'in', u'Marathon', u'County.', u'Abbotsford', u'is', u'nicknamed', u'\"Wisconsin\\'s', u'First', u'City\"', u'due', u'to', u'its', u'alphabetical', u'place', u'on', u'a', u'list', u'of', u'Wisconsin', u'cities.']\n",
      "[[24, 29, u'Clark'], [43, 51, u'Marathon'], [68, 78, u'U.S. state'], [82, 91, u'Wisconsin'], [112, 129, u'Wausau, Wisconsin'], [130, 159, u'Metropolitan Statistical Area']]\n",
      "[[0, 10, u'Abbotsford'], [16, 20, u'city'], [24, 29, u'Clark'], [43, 51, u'Marathon'], [52, 60, u'counties'], [68, 78, u'U.S. state'], [73, 78, u'state'], [82, 91, u'Wisconsin'], [112, 129, u'Wausau, Wisconsin'], [120, 129, u'Wisconsin'], [130, 159, u'Metropolitan Statistical Area'], [166, 176, u'population'], [199, 205, u'census'], [230, 242, u'Clark County'], [260, 275, u'Marathon County'], [277, 287, u'Abbotsford'], [302, 313, u\"Wisconsin's\"], [314, 319, u'First'], [320, 324, u'City'], [337, 349, u'alphabetical'], [369, 378, u'Wisconsin'], [379, 385, u'cities']]\n",
      "\n",
      "correct: 6\n",
      "found: 22\n",
      "correct: 6\n",
      "actual: 6\n",
      "0.272727272727 1.0\n",
      "\n",
      "13\n",
      "[u'A', u'-', u'B', u'-', u'C', u'-', u'D', u'-', u'E', u'-', u'F', u'-', u'G', u'-', u'H', u'-', u'I', u'-', u'J', u'-', u'K', u'-', u'L', u'-', u'M', u'-', u'N', u'-', u'O', u'-', u'P', u'-', u'Q', u'-', u'R', u'-', u'S', u'-', u'T', u'-', u'U', u'-', u'V', u'-', u'Y', u'-', u'Z']\n",
      "[[0, 1, u'A'], [4, 5, u'B'], [8, 9, u'C'], [12, 13, u'D'], [16, 17, u'E'], [20, 21, u'F'], [24, 25, u'G'], [28, 29, u'H'], [32, 33, u'I'], [36, 37, u'J'], [40, 41, u'K'], [44, 45, u'L'], [48, 49, u'M'], [52, 53, u'N'], [56, 57, u'O'], [60, 61, u'P'], [64, 65, u'Q'], [68, 69, u'R'], [72, 73, u'S'], [76, 77, u'T'], [80, 81, u'U'], [84, 85, u'V'], [92, 93, u'Z']]\n",
      "[[4, 5, u'B'], [8, 9, u'C'], [12, 13, u'D'], [16, 17, u'E'], [20, 21, u'F'], [24, 25, u'G'], [28, 29, u'H'], [36, 37, u'J'], [40, 41, u'K'], [44, 45, u'L'], [48, 49, u'M'], [52, 53, u'N'], [56, 57, u'O'], [60, 61, u'P'], [64, 65, u'Q'], [68, 69, u'R'], [76, 77, u'T'], [84, 85, u'V'], [88, 89, u'Y'], [92, 93, u'Z']]\n",
      "\n",
      "correct: 19\n",
      "found: 20\n",
      "correct: 19\n",
      "actual: 23\n",
      "0.95 0.826086956522\n",
      "\n",
      "14\n",
      "[u'A', u'-', u'B', u'-', u'C', u'-', u'D', u'-', u'E', u'-', u'F', u'-', u'G', u'-', u'H', u'-', u'I', u'-', u'J', u'-', u'K', u'-', u'L', u'-', u'M', u'-', u'N', u'-', u'O', u'-', u'P', u'-', u'Q', u'-', u'R', u'-', u'S', u'-', u'T', u'-', u'U', u'-', u'V', u'-', u'Y', u'-', u'Z']\n",
      "[[0, 1, u'A'], [4, 5, u'B'], [8, 9, u'C'], [16, 17, u'E'], [20, 21, u'F'], [24, 25, u'G'], [28, 29, u'H'], [32, 33, u'I'], [36, 37, u'J'], [40, 41, u'K'], [44, 45, u'L'], [48, 49, u'M'], [52, 53, u'N'], [56, 57, u'O'], [60, 61, u'P'], [64, 65, u'Q'], [68, 69, u'R'], [72, 73, u'S'], [76, 77, u'T'], [80, 81, u'U'], [84, 85, u'V'], [88, 89, u'Y'], [92, 93, u'Z']]\n",
      "[[4, 5, u'B'], [8, 9, u'C'], [12, 13, u'D'], [16, 17, u'E'], [20, 21, u'F'], [24, 25, u'G'], [28, 29, u'H'], [36, 37, u'J'], [40, 41, u'K'], [44, 45, u'L'], [48, 49, u'M'], [52, 53, u'N'], [56, 57, u'O'], [60, 61, u'P'], [64, 65, u'Q'], [68, 69, u'R'], [76, 77, u'T'], [84, 85, u'V'], [88, 89, u'Y'], [92, 93, u'Z']]\n",
      "\n",
      "correct: 19\n",
      "found: 20\n",
      "correct: 19\n",
      "actual: 23\n",
      "0.95 0.826086956522\n",
      "\n",
      "15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17d5c9cd4bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mtrueMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTextMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0msolrMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSolrMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-17d5c9cd4bfa>\u001b[0m in \u001b[0;36mgetSolrMentions\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtotalMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mention_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mtotalAppearances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_solr_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"print 'item: ' + text[item[0]:item[1]]\n",
      "\u001b[0;32m<ipython-input-3-17d5c9cd4bfa>\u001b[0m in \u001b[0;36mget_solr_count\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mqstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://localhost:8983/solr/enwiki20160305/select'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'indent'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rows'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'response'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         settings = self.merge_environment_settings(\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mprep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         )\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mmerge_environment_settings\u001b[0;34m(self, url, proxies, stream, verify, cert)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mverify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mcert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         return {'verify': verify, 'proxies': proxies, 'stream': stream,\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mmerge_setting\u001b[0;34m(request_setting, session_setting, dict_class)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_setting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_setting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \"\"\"Determines appropriate setting for a given request, taking into account\n\u001b[1;32m     44\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mexplicit\u001b[0m \u001b[0msetting\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msetting\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end, text],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in line['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(line['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mentions.append([curStart, curStart + len(line['text'][curWord]), line['text'][curWord]])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "                    \n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    textData = []\n",
    "    # get rid of extra un-needed Solr data\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        \n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb])\n",
    "    \n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [4][1] is index of type of word\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.001\n",
    "    \n",
    "    for item in textData:\n",
    "        totalMentions = get_mention_count(item[2])\n",
    "        totalAppearances = get_solr_count(item[2].replace(\".\", \"\"))\n",
    "        \n",
    "        \"\"\"print 'item: ' + text[item[0]:item[1]]\n",
    "        if totalAppearances == 0:\n",
    "            print 'prob: ' + text[item[0]:item[1]] + ' --> 0'\n",
    "        else:\n",
    "            print 'prob: ' + text[item[0]:item[1]] + ' --> ' + str(totalMentions/totalAppearances)\"\"\"\n",
    "        \n",
    "        if (totalAppearances > 0 and\n",
    "                (totalMentions/totalAppearances) >= mentionPThrsh\n",
    "                and (item[4][1][0:2] == 'NN' or item[4][1] == 'JJ')):\n",
    "            mentions.append([item[0], item[1], item[2]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line)\n",
    "        solrMentions = getSolrMentions(\" \".join(line['text']))\n",
    "        \n",
    "        print line['text']\n",
    "        print trueMentions\n",
    "        print str(solrMentions) + '\\n'\n",
    "        \n",
    "        \"\"\"solrMentions0 = tagme.mentions(\" \".join(line['text']))\n",
    "        solrMentions = []\n",
    "        for item in solrMentions0.mentions:\n",
    "            solrMentions.append([item.begin, item.end, item.mention])\"\"\"\n",
    "        \n",
    "        ## get statistical results from true mentions and solr mentions\n",
    "        \n",
    "        aNumber = len(solrMentions)/len(trueMentions)\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionStartsAndEnds(phraseData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phraseData object and appends it's mentions with the start and end\n",
    "        index of each mention in the original string.\n",
    "    Args:\n",
    "        phraseData: [['words','split','like','this'],[[wordId,entityId,frequency,start,end],...]]\n",
    "    Return:\n",
    "        The same phraseData but with each mention containing the start and end of that\n",
    "        mention in the source text\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in phraseData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(phraseData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mention.append(0) # frequency placeholder\n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(phraseData['text'][curWord])) # end of the mention\n",
    "\n",
    "    return phraseData\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase.encode('utf-8'))\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitText = []\n",
    "    mentions = []\n",
    "    \n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(phrase[item[1]:item[3]])\n",
    "        \n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [6][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.005\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    i = 0\n",
    "    for item in textData:    \n",
    "        totalMentions = get_mention_count(phrase[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(phrase[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if (totalAppearances > 0 \n",
    "                and (totalMentions/totalAppearances) >= mentionPThrsh \n",
    "                and phrase[item[1]:item[3]] not in stopWords\n",
    "                and (item[6][1] == 'NNP' or item[6][1] == 'NNPS' or item[6][1] == 'NN')):\n",
    "            mentions.append([i, '0', 0, item[1], item[3]])\n",
    "            \n",
    "        splitText.append(phrase[item[1]:item[3]])\n",
    "        i += 1\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(phrase, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in phrase.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in phrase['mentions']:\n",
    "        results = sorted(anchor2concept(phrase['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][3] < truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][4] > truthSet[truthIndex][3]:\n",
    "                # overlap with mine behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][3] == truthSet[truthIndex][3]:\n",
    "            # same mention (same start atleast)\n",
    "            if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][3] > truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][3] < truthSet[truthIndex][4]:\n",
    "                # overlap with truth behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][3] < truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][4] > truthSet[truthIndex][3]:\n",
    "                # overlap with mine behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][3] == truthSet[truthIndex][3]:\n",
    "            # same mention (same start atleast)\n",
    "            if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][3] > truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][3] < truthSet[truthIndex][4]:\n",
    "                # overlap with truth behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSentenceOfMention():\n",
    "    pass\n",
    "    \n",
    "def getSurroundingSentences(phrase, axis):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding sentences.\n",
    "    Return:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones: [[w3,w4,w5],[w0,w1,w2,w6,w7,w8]]\n",
    "    \"\"\"\n",
    "    \n",
    "    frstSentenceStart = 0\n",
    "    # end of first sentence is just start of middle sentence\n",
    "    mdlSentenceStart = 0\n",
    "    mdlSentenceEnd = 0\n",
    "    # start of last sentence is just end of middle sentence\n",
    "    lstSentenceEnd = 0\n",
    "    \n",
    "    # get start index of middle sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(axis,-1,-1):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            mdlSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of middle sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(axis, len(phrase)):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            mdlSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            mdlSentenceEnd = len(phrase)\n",
    "            \n",
    "    # get start index of first sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(mdlSentenceStart - 2, -1, -1):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            frstSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of last sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(mdlSentenceEnd + 1, len(phrase)):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            lstSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            lstSentenceEnd = len(phrase)\n",
    "            \n",
    "    sentences = [phrase[mdlSentenceStart:axis]+phrase[axis+1:mdlSentenceEnd],\n",
    "                phrase[frstSentenceStart:mdlSentenceStart]+phrase[mdlSentenceEnd:lstSentenceEnd]]\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "def getSurroundingWords(phrase, axis, branchSize):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding words.\n",
    "        branchSize: The amount of words to the left and right to get.\n",
    "    Return:\n",
    "        The words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = axis - branchSize\n",
    "    imax = axis + branchSize\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(phrase):\n",
    "        imax = len(phrase)\n",
    "        \n",
    "    # return surrounding part of word minus the axis word\n",
    "    return (phrase[imin:axis] + phrase[axis+1:imax])\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestMultiContextMatch(mention, context, contextSurround, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words in the sentence of the target.\n",
    "        contextSurround: The words in the sentences that surround the target.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put texts in right format\n",
    "    text = \" \".join(context)\n",
    "    textSurround = \" \".join(contextSurround)\n",
    "    text = escapeStringSolr(text)\n",
    "    textSurround = escapeStringSolr(textSurround)\n",
    "    mention = escapeStringSolr(mention)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    if len(contextSurround) > 0:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text.decode('string_escape')+')^1 text:('+textSurround.decode('string_escape')+')^0 title:('+mention.decode('string_escape')+')^1.35',\n",
    "                'wt':'json'}\n",
    "    else:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text+') title:('+mention+')^1.35',\n",
    "                'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestContextMatch(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that suround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    text = (\" \".join(context)).encode('utf-8')\n",
    "    text = escapeStringSolr(text)\n",
    "    mention = escapeStringSolr(mention.encode('utf-8'))\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+text.decode('string_escape')+') title:(' + mention.decode('string_escape') + ')^0.6',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "    \n",
    "def wikifyPopular(phrase, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[0], candidates[i][0][0], candidates[i][0][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "# the orginal version, with just surrounding words.\n",
    "def wikifyContexty(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding contextBranchSize words.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            context = getSurroundingWords(phrase['text'], mention[0], ctxBrchSz)\n",
    "            bestIndex = bestContextMatch(phrase['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], mention[2], mention[3]])\n",
    "        else:\n",
    "            topCandidates.append([mention[0], 0, -1, -1]) # a bad mention\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "# new version with surrounding sentences\n",
    "def wikifyContext(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding sentences and its own\n",
    "        serving as context.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            contexts = getSurroundingSentences(phrase['text'], mention[0])\n",
    "            bestIndex = bestMultiContextMatch(phrase['text'][mention[0]], contexts[0], contexts[1], candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], candidates[i][bestIndex][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(phrase, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(mentionsGiven):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "    else:\n",
    "        phrase = mentionStartsAndEnds(phrase)\n",
    "    \n",
    "        \n",
    "    wikified = [phrase['text']] # second index with proposed entities filled later\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        phrase['mentions'] = [item for item in phrase['mentions']\n",
    "                    if  len(phrase['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(phrase, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(phrase, candidates))\n",
    "    elif method == 'context':\n",
    "        wikified.append(wikifyContext(phrase, candidates, ctxBrchSz = len(phrase['text'])))\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if item[2] >= MIN_FREQUENCY]\n",
    "    \n",
    "    \"\"\"# remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\"\"\"\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_'), 0, item['from'], item['to']])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def wikilineLine(inLine):\n",
    "    \"\"\"\n",
    "    Puts the inLine in the right format if it came from wikipedia.\n",
    "    \"\"\"\n",
    "    newLine = {'text':[], 'mentions':[]}\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['context','popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "\n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'], False, method = mthd, maxC = 7)\n",
    "            else:\n",
    "                trueEntities = mentionStartsAndEnds(line)['mentions'] # the ground truth\n",
    "                \n",
    "                # original split string\n",
    "                resultS = wikifyEval(line, True, method = mthd, maxC = 7)\n",
    "                # unsplit string\n",
    "                resultM = wikifyEval(\" \".join(line['text']), False, method = mthd, maxC = 7)\n",
    "                \n",
    "            #resultM = [[],[]]\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'Pre-Split Precision':totalPrecS/totalLines, \n",
    "                                               'Manual Split Precision':totalPrecM/totalLines,\n",
    "                                              'Pre-Split Recall':totalRecS/totalLines, \n",
    "                                               'Manual Split Recall':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, method='popular')\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -In wikifyContext make the current sentence worth 1 and each surrounding sentence worth 0.5.\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = sorted(anchor2concept(\"David Edgar\"), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "\n",
    "for tmpp in tmp:\n",
    "    print 'id: ' + str(tmpp[0]) + ', title: ' + id2title(tmpp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase = {u'text': [u'Voller', u'presidential', u'preferences', u'How', u'will', u'American', u'voters', u'compensate', u'in', u'the', u'next', u'search', u'for', u'a', u'president?', u'WASHINGTON', u'-', u'Now', u'that', u'the', u'38th', u'president', u'has', u'been', u'laid', u'to', u'rest,', u'the', u'capital', u'can', u'take', u'up', u'the', u'main', u'business', u'of', u'2007:', u'trying', u'to', u'figure', u'out', u'who', u'will', u'be', u'the', u'44th.', u'What', u'type', u'of', u'leader', u'does', u'the', u'country', u'want?', u'Here', u'is', u'my', u'sense', u'of', u'it,', u'based', u'on', u'talking', u'to', u'politicians,', u'strategists', u'and', u'voters', u'here', u'and', u'around', u'the', u'nation.', u'No', u'ideologues,', u'please', u'There', u'was', u'a', u'time', u'when', u'President George W. Bush', u\"'s\", u'ideological', u'certitude', u'was', u'politically', u'appealing', u'and', u'perhaps', u'functionally', u'necessary.', u'That', u'time', u'has', u'long', u'since', u'passed.', u'The', u'country', u'is', u'tired,', u'even', u'fearful,', u'of', u'leaders', u'with', u'fervent', u'beliefs', u'that', u'seem', u'impervious', u'to', u'new', u'(or', u'even', u'old)', u'facts.', u'Voters', u'see', u'the', u'war', u'in', u'Iraq', u'as', u'an', u'\"idea,\"', u'not', u'a', u'solution', u'-', u'and', u'Americans', u'do', u'not', u'like', u'ideas', u'that', u'do', u'not', u'work.', u'Voters', u'likely', u'will', u'view', u'Bush', u\"'s\", u'\"surge\"', u'of', u'troops', u'into', u'Iraq', u'as', u'new', u'evidence', u'of', u'failure,', u'and', u'the', u'dangers', u'of', u'a', u'leader', u'who', u'depends', u'on', u'preconceived', u'ideas.', u'Serious', u'student', u'Presidential', u'elections', u'are', u'a', u'never-ending', u'series', u'of', u'mid-course', u'corrections.', u'Voters', u'look', u'to', u'compensate', u'for', u'the', u'leadership', u'weaknesses', u'of', u'the', u'incumbent.', u'An', u'example', u'comes', u'from', u'the', u'life', u'and', u'career', u'of', u'Gerald Ford', u'.', u'In', u'1976,', u'voters', u'wanted', u'a', u'pure', u'antidote', u'to', u'Richard Nixon', u\"'s\", u'paranoid', u'megalomania.', u'Once', u'Ford', u'pardoned', u'Nixon', u',', u'he', u'could', u'not', u'be', u'that', u'candidate.', u'Instead,', u'Americans', u'chose', u'Jimmy Carter', u',', u'a', u'peanut', u'farmer', u'who', u'had', u'never', u'worked', u'in', u'Washington', u',', u'and', u'who', u'promised', u'never', u'to', u'lie', u'to', u'the', u'American people', u'.', u'The', u'counterpoint', u'thinking', u'continues.', u'Voters', u'in', u'2008', u'are', u'going', u'to', u'want', u'someone', u'who', u'prides', u'himself', u'(or', u'herself)', u'on', u'spending', u'time', u'in', u'the', u'library', u'-', u'who', u'has', u'a', u'hands-on', u'curiosity', u'about', u'the', u'details.', u'Washington', u'experience', u'not', u'necessary', u'Voters', u'these', u'days', u'not', u'only', u'do', u'not', u'value', u'Washington', u'experience', u'-', u'or', u'any', u'office-holding', u'experience', u'-', u'it', u'can', u'make', u'them', u'suspicious.', u'That', u'is', u'what', u'strategists', u'and', u'polltakers', u'for', u'Sen.', u'Evan Bayh', u'found', u'when', u'they', u'studied', u'whether', u'he', u'should', u'run', u'for', u'president.', u'They', u'found', u'that', u'his', u'remarkably', u'deep', u'resume', u'-', u'the', u'son', u'of', u'a', u'senator,', u'he', u'was', u'the', u'\"boy', u'governor\"', u'of', u'Indiana', u'before', u'going', u'to', u'the', u'Senate', u'-', u'was', u'as', u'handicap.', u'Americans', u'always', u'are', u'dubious', u'about', u'the', u'capital,', u'but', u'that', u'sentiment', u'seems', u'particularly', u'strong.', u'Bayh', u'decided', u'not', u'to', u'run.', u'\"`', u'Washington', u\"'\", u\"doesn't\", u'make', u'the', u'case,\"', u'said', u'Dan Pfeiffer', u',', u'who', u'worked', u'for', u'Bayh', u'.', u'No', u'more', u'boomer', u'obsessions', u'Not', u'all', u'elections', u'are', u'about', u'change,', u'but', u'2008', u'will', u'be.', u'Americans', u'are', u'moderately', u'upbeat', u'about', u'the', u\"country's\", u'prospects,', u'but', u'deeply', u'worried', u'about', u'the', u'world', u'-', u'and', u'they', u'have', u'come', u'to', u'realize', u'that', u'they', u\"can't\", u'separate', u'one', u'from', u'the', u'other.', u'One', u'thing', u'for', u'sure,', u'says', u'Pfeiffer', u',', u'voters', u'are', u'tired', u'of', u'arguing', u'about', u'the', u'culture', u'of', u'the', u'1960s', u'and', u'other', u'Boomer', u'issues.', u'\"There', u'is', u'a', u'sense', u'that', u'the', u'2004', u'election', u'was', u'too', u'much', u'about', u'who', u'did', u'or', u'did', u'not', u'do', u'what', u'in', u'Vietnam', u',\"', u'said', u'Pfeiffer', u',', u'referring', u'to', u'the', u'Bush campaign', u'against', u'Sen.', u'John Kerry', u'.', u'In', u'2000,', u'Bush', u'won', u'in', u'part', u'by', u'selling', u'himself', u'as', u'a', u'\"grown', u'up\"', u'Boomer', u'answer', u'to', u'Bill Clinton', u'.', u'\"Voters', u'are', u'tired', u'of', u'that', u'era', u'and', u'its', u'concerns,\"', u'said', u'Pfeiffer', u'said.', u'\"They', u'want', u'to', u'move', u'on.\"', u'Know', u'the', u'middle', u'class', u'Bushes', u'have', u'a', u'congenital', u'family', u'problem', u'with', u'this,', u'and', u'it', u'leaves', u'an', u'opening', u'for', u'someone', u'-', u'of', u'either', u'party', u'-', u'who', u'can', u'prove', u'that', u'he', u'or', u'she', u'really', u'understands', u'the', u'strains', u'of', u'middle', u'class', u'life.', u\"It's\", u'not', u'just', u'about', u'money,', u'but', u'about', u'cultural', u'assaults', u'and', u'the', u'lack', u'of', u'time', u'for', u'family', u'in', u'an', u'era', u'when', u'both', u'parents', u'or', u'partners', u'need', u'to', u'work.', u'In', u'his', u'forthcoming', u'book,', u'Positively', u'American,', u'Sen.', u'Charles Schumer', u'of', u'New York', u'imagines', u'the', u'hard', u'life', u'of', u'a', u'fictitious', u'middle', u'class', u'family', u'-', u'and', u'offers', u'a', u'series', u'of', u'governmental', u'proposals', u'to', u'address', u'them.', u'A', u'shrewd', u'student', u'of', u'the', u'American', u'mood,', u'Schumer', u'is', u'aiming', u'in', u'the', u'right', u'direction.', u'The', u'next', u'president', u'will', u'need', u'to', u'show', u'that', u'he', u'or', u'she', u'understands', u'that', u'family.'], u'mentions': [[15, u'Washington,_D.C.', 0, 106, 116], [81, u'George_W._Bush', 0, 459, 483], [123, u'Iraq', 0, 743, 747], [145, u'George_W._Bush', 0, 853, 857], [151, u'Iraq', 0, 884, 888], [199, u'Gerald_Ford', 0, 1191, 1202], [209, u'Richard_Nixon', 0, 1247, 1260], [214, u'Gerald_Ford', 0, 1291, 1295], [216, u'Richard_Nixon', 0, 1305, 1310], [227, u'Jimmy_Carter', 0, 1370, 1382], [237, u'Washington,_D.C.', 0, 1425, 1435], [247, u'Demographics_of_the_United_States', 0, 1475, 1490], [281, u'Washington,_D.c.', 0, 1685, 1695], [293, u'Washington,_D.c.', 0, 1761, 1771], [314, u'Evan_Bayh', 0, 1898, 1907], [344, u'Indiana', 0, 2065, 2072], [349, u'United_States_Senate', 0, 2093, 2099], [367, u'Evan_Bayh', 0, 2213, 2217], [373, u'Washington,_D.C.', 0, 2241, 2251], [380, u'Dan_Pfeiffer', 0, 2283, 2295], [385, u'Evan_Bayh', 0, 2313, 2317], [435, u'Dan_Pfeiffer', 0, 2600, 2608], [450, u'Boomer', 0, 2680, 2686], [472, u'Vietnam_War', 0, 2785, 2792], [475, u'Dan_Pfeiffer', 0, 2801, 2809], [480, u'George_W._Bush_presidential_campaign,_2004', 0, 2829, 2842], [483, u'John_Kerry', 0, 2856, 2866], [487, u'George_W._Bush', 0, 2878, 2882], [501, u'Bill_Clinton', 0, 2947, 2959], [513, u'Dan_Pfeiffer', 0, 3016, 3024], [593, u'Charles_Schumer', 0, 3459, 3474], [595, u'New_York', 0, 3478, 3486], [624, u'Charles_Schumer', 0, 3650, 3657]]}\n",
    "wikified = [phrase['text']]\n",
    "cands = generateCandidates(phrase, 7)\n",
    "wikified.append(wikifyContext(phrase, cands, ctxBrchSz = len(phrase['text'])))\n",
    "\n",
    "for mention in wikified[1]:\n",
    "    mention[1] = id2title(mention[1])\n",
    "    \n",
    "print (\" \".join(wikified[0])).encode('utf-8').strip()\n",
    "print wikified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(33509L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text = \" \".join([\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"])\n",
    "print text\n",
    "\n",
    "text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "text = text.replace('+', r'\\+')\n",
    "text = text.replace(\"-\", \"\\-\")\n",
    "text = text.replace(\"&&\", \"\\&&\")\n",
    "text = text.replace(\"||\", \"\\||\")\n",
    "text = text.replace(\"!\", \"\\!\")\n",
    "text = text.replace(\"(\", \"\\(\")\n",
    "text = text.replace(\")\", \"\\)\")\n",
    "text = text.replace(\"{\", \"\\{\")\n",
    "text = text.replace(\"}\", \"\\}\")\n",
    "text = text.replace(\"[\", \"\\[\")\n",
    "text = text.replace(\"]\", \"\\]\")\n",
    "text = text.replace(\"^\", \"\\^\")\n",
    "text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "text = text.replace(\"~\", \"\\~\")\n",
    "text = text.replace(\"*\", \"\\*\")\n",
    "text = text.replace(\"?\", \"\\?\")\n",
    "text = text.replace(\":\", \"\\:\")\n",
    "\n",
    "text = text.decode('string_escape')\n",
    "\n",
    "print text + '\\n\\n'\n",
    "\n",
    "addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "r = requests.post(addr, params=params, data=text)\n",
    "textData = r.json()['tags']\n",
    "\n",
    "print textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phraseData = {\"text\": [\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"], \"mentions\": [[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]}\n",
    "print str(phraseData) + '\\n'\n",
    "phraseData = mentionStartsAndEnds(phraseData)\n",
    "print phraseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in sorted(anchor2concept('Muller'), key=itemgetter(1), reverse = True):\n",
    "    print id2title(item[0]) + ' ----- ' + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "tomatoes_mentions = tagme.mentions(\"I definitely like ice cream better than tomatoes.\")\n",
    "\n",
    "for mention in tomatoes_mentions.mentions:\n",
    "    print mention.begin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
