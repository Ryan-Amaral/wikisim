{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase)\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitText = []\n",
    "    \n",
    "    for datum in textData:\n",
    "        splitText.append(phrase[datum[1]:datum[3]])\n",
    "    \n",
    "    mentions = []\n",
    "    \n",
    "    for i in range(len(splitText)):\n",
    "        mentions.append([i, '0'])\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(phrase, maxC):\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in phrase['mentions']:\n",
    "        results = sorted(anchor2concept(phrase['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def wikifyPopular(candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate to choose (just all 0s) for each mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidateIndices = []\n",
    "    for cands in candidates:\n",
    "        topCandidateIndices.append(0)\n",
    "            \n",
    "    return proposedEntities\n",
    "\n",
    "def wikifyPopularMentionsGiven(data):\n",
    "    \n",
    "    proposedEntities = []\n",
    "\n",
    "    for mention in data['mentions']:\n",
    "        results = sorted(anchor2concept(data['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        \n",
    "        if len(results) > 0: # some results\n",
    "            print mention\n",
    "            entity = results[0]\n",
    "            proposedEntities.append([mention[0], entity[0], entity[1]])\n",
    "            print id2title(entity[0])\n",
    "            \n",
    "    return proposedEntities\n",
    "\n",
    "def wikifyEval(phrase, mentionsGiven, maxC = 20, method='popular', strict = True):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(mentionsGiven):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "        \n",
    "    wikified = [phrase['text']] # second index with proposed entities filled later\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        phrase['mentions'] = [item for item in phrase['mentions']\n",
    "                    if  len(phrase['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(phrase, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(candidates))\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if item[2] >= MIN_FREQUENCY]\n",
    "    \n",
    "    # remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\n",
    "        \n",
    "    return wikified\n",
    "\n",
    "def wikifyEvalMentionsGiven(data, method='popular', strict = True):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the data and returns the most likely entity based on the given method.\n",
    "    Args:\n",
    "        phrase: The split phrase along with the mentions.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The data with its most likely entities.\n",
    "    \"\"\"\n",
    "    wikified = [data['text']]\n",
    "    \n",
    "    print data\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopularMentionsGiven(data))\n",
    "        \n",
    "    print\n",
    "    print\n",
    "    \n",
    "    # small clean-up to do\n",
    "    if strict == True:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if not (item[2] < MIN_FREQUENCY or len(phrase[item[0]]) < MIN_MENTION_LENGTH)]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_')])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "#pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name']\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'].encode('utf-8').strip(), False, mthd, True)\n",
    "                \n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "            else:\n",
    "                # original split string\n",
    "                resultS = wikifyEval(line, True, mthd, True)\n",
    "                # unsplit string\n",
    "                resultM = wikifyEval((\" \".join(line['text'])).encode('utf-8').strip(), False, mthd, True)\n",
    "                \n",
    "                \n",
    "                trueEntities = line['mentions']\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            #precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM)\n",
    "            #print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "            \n",
    "            print str(totalLines) + '\\n'\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'Pre-Split Precision':totalPrecS/totalLines, \n",
    "                                               'Manual Split Precision':totalPrecM/totalLines,\n",
    "                                              'Pre-Split Recall':totalRecS/totalLines, \n",
    "                                               'Manual Split Recall':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, 'popular', True)\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = sorted(anchor2concept(\"David Edgar\"), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "\n",
    "for tmpp in tmp:\n",
    "    print 'id: ' + str(tmpp[0]) + ', title: ' + id2title(tmpp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I', 'walked', 'down', 'to', 'the park', 'and', 'found', 'a duck', 'studying', 'quantum mechanical systems'], 'mentions': [[0, '0'], [1, '0'], [2, '0'], [3, '0'], [4, '0'], [5, '0'], [6, '0'], [7, '0'], [8, '0'], [9, '0']]}\n",
      "[[(2172378L, 63L), (25346998L, 52L), (14750L, 47L), (35960803L, 37L), (34038262L, 29L), (33893279L, 27L), (31131499L, 25L), (79222L, 23L), (3363030L, 18L), (29128L, 17L), (1884846L, 17L), (43191L, 14L), (669931L, 12L), (4764461L, 12L), (40250L, 11L), (519796L, 11L), (10978553L, 9L), (2526953L, 8L), (3345314L, 8L), (7822134L, 8L)], [(3802L, 206L), (463755L, 3L), (9405478L, 2L), (33509L, 1L), (383364L, 1L), (5051251L, 1L)], [(1084904L, 102L), (962449L, 45L), (1484606L, 43L), (241027L, 41L), (177240L, 18L), (578491L, 6L), (18183L, 2L), (8008410L, 2L), (19009006L, 2L), (50873L, 1L), (251024L, 1L), (387230L, 1L), (2335667L, 1L), (3397863L, 1L), (8664816L, 1L)], [(6851L, 1L), (12213L, 1L), (918351L, 1L), (2985420L, 1L), (3552005L, 1L), (3642104L, 1L), (8213135L, 1L)], [(3653957L, 1L), (4648263L, 1L), (6948083L, 1L)], [(18152L, 19L), (1944911L, 6L), (40800679L, 6L), (113500L, 4L), (13604537L, 4L), (15131247L, 4L), (1280983L, 3L), (21442974L, 3L), (28747144L, 3L), (40482110L, 3L), (847624L, 2L), (1883448L, 2L), (2102277L, 2L), (9543870L, 2L), (13049050L, 2L), (22316162L, 2L), (7962L, 1L), (8092L, 1L), (17867L, 1L), (29945L, 1L)], [(99071L, 5L), (2038320L, 3L), (6047571L, 2L), (9331L, 1L), (167302L, 1L), (168885L, 1L), (245335L, 1L), (419237L, 1L), (447986L, 1L), (533069L, 1L), (733355L, 1L), (2424150L, 1L), (9019997L, 1L), (18950003L, 1L)], [(2960722L, 4L), (3312535L, 1L)], [(2351973L, 8L), (21189546L, 2L), (162619L, 1L), (447779L, 1L), (489094L, 1L), (526101L, 1L)], [(25202L, 1L)]]\n"
     ]
    }
   ],
   "source": [
    "split = splitWords('I walked down to the park and found a duck studying quantum mechanical systems')\n",
    "print split\n",
    "cands = generateCandidates(split, 20)\n",
    "\n",
    "print cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Walking'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2title(33509L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
