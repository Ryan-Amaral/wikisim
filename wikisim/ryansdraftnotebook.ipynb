{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffs:\n",
      "['\"IN : ,\"', 0.092312986]\n",
      "['IN : .', 0.06629444400000001]\n",
      "['DT : .', 0.043011235]\n",
      "['\", : ,\"', 0.037971903]\n",
      "['\"DT : ,\"', 0.034344888000000004]\n",
      "['IN : CC', 0.031946389]\n",
      "['DT : IN', 0.02865779]\n",
      "['NNP : DT', 0.028027932000000002]\n",
      "['IN : IN', 0.027639607000000004]\n",
      "['NN : DT', 0.027124422]\n",
      "\n",
      "Raw Diffs:\n",
      "['NNP : NNP', 16188L]\n",
      "['IN : NNP', 7519L]\n",
      "['NN : NNP', 7500L]\n",
      "['NNP : DT', 7204L]\n",
      "['NN : DT', 6989L]\n",
      "['IN : NN', 6581L]\n",
      "['DT : NN', 5213L]\n",
      "['DT : IN', 4984L]\n",
      "['NNP : IN', 4379L]\n",
      "['JJ : IN', 4047L]\n",
      "\n",
      "Mention Solos:\n",
      "['CC : POS', 0.000664403]\n",
      "['TO : POS', 0.00059058]\n",
      "['VB : .', 0.000553669]\n",
      "['VBZ : POS', 0.000442935]\n",
      "['\". : ,\"', 0.000332201]\n",
      "['\"WRB : ,\"', 0.00029529]\n",
      "['VB : POS', 0.00029529]\n",
      "['\"POS : ,\"', 0.000258379]\n",
      "['VBG : POS', 0.000258379]\n",
      "['\", : POS\"', 0.000221468]\n",
      "\n",
      "Non-Mention Solos:\n",
      "['VBN : DT', 1899L, 0.007396963]\n",
      "['NNS : DT', 1567L, 0.00610376]\n",
      "['VBZ : JJ', 1559L, 0.006072599]\n",
      "['NN : CD', 1343L, 0.005231238]\n",
      "['PRP : DT', 1140L, 0.004440515]\n",
      "['PRP : VBN', 976L, 0.003801704]\n",
      "['NNP : PRP$', 669L, 0.002605881]\n",
      "['NN : PRP$', 642L, 0.002500711]\n",
      "['NN : VB', 581L, 0.002263104]\n",
      "['PRP : RB', 539L, 0.002099506]\n",
      "['VBN : CD', 510L, 0.001986546]\n",
      "['NN : PRP', 505L, 0.00196707]\n",
      "['JJ : NONE', 413L, 0.001608713]\n",
      "['NN : NONE', 396L, 0.001542495]\n",
      "['VBD : VB', 310L, 0.001207508]\n",
      "[') : DT', 303L, 0.001180242]\n",
      "['NNS : VBN', 290L, 0.001129605]\n",
      "['VBN : PRP$', 288L, 0.001121814]\n",
      "['PRP : NNP', 285L, 0.001110129]\n",
      "['NNS : NNS', 283L, 0.001102338]\n",
      "['VB : DT', 266L, 0.00103612]\n",
      "['PRP : NN', 258L, 0.001004959]\n",
      "['VBN : VB', 249L, 0.000969902]\n",
      "['CD : NONE', 248L, 0.000966007]\n",
      "['CD : RB', 245L, 0.000954321]\n",
      "['NNS : VB', 227L, 0.000884208]\n",
      "['CD : PRP$', 217L, 0.000845256]\n",
      "['CD : PRP', 214L, 0.00083357]\n",
      "['VBN : NNS', 208L, 0.000810199]\n",
      "['NNS : RB', 205L, 0.000798514]\n",
      "['PRP : VBD', 189L, 0.000736191]\n",
      "['WDT : VBN', 182L, 0.000708924]\n",
      "['WDT : DT', 180L, 0.000701134]\n",
      "['NNS : PRP$', 175L, 0.000681658]\n",
      "['VBP : DT', 169L, 0.000658287]\n",
      "['. : DT', 163L, 0.000634916]\n",
      "['TO : VBN', 158L, 0.00061544]\n",
      "['PRP : CD', 157L, 0.000611545]\n",
      "['CC : NONE', 157L, 0.000611545]\n",
      "['IN : PRP$', 156L, 0.000607649]\n",
      "['WP : IN', 154L, 0.000599859]\n",
      "['PRP : VB', 153L, 0.000595964]\n",
      "['WP : DT', 142L, 0.000553117]\n",
      "['TO : PRP', 134L, 0.000521955]\n",
      "['\", : PRP\"', 133L, 0.00051806]\n",
      "['IN : NNPS', 127L, 0.000494689]\n",
      "['VBP : VBN', 126L, 0.000490794]\n",
      "['PRP$ : TO', 119L, 0.000463527]\n",
      "['NNS : PRP', 117L, 0.000455737]\n",
      "['PRP : PRP$', 116L, 0.000451842]\n",
      "['MD : VBN', 116L, 0.000451842]\n",
      "['PRP : TO', 116L, 0.000451842]\n",
      "['DT : NNPS', 115L, 0.000447947]\n",
      "['NONE : JJ', 111L, 0.000432366]\n",
      "['VBD : PRP$', 109L, 0.000424576]\n",
      "['\", : CD\"', 108L, 0.00042068]\n",
      "['RB : PRP$', 107L, 0.000416785]\n",
      "['NNS : VBG', 106L, 0.00041289]\n",
      "['PRP$ : PRP', 106L, 0.00041289]\n",
      "['NNS : NONE', 103L, 0.000401204]\n",
      "['NONE : (', 99L, 0.000385624]\n",
      "['RB : RB', 98L, 0.000381728]\n",
      "['. : CD', 98L, 0.000381728]\n",
      "['POS : NNP', 97L, 0.000377833]\n",
      "['TO : NONE', 96L, 0.000373938]\n",
      "['JJ : VB', 95L, 0.000370043]\n",
      "['MD : VB', 93L, 0.000362253]\n",
      "['RB : VB', 93L, 0.000362253]\n",
      "['WP : VBN', 91L, 0.000354462]\n",
      "['NNPS : DT', 91L, 0.000354462]\n",
      "['NNP : JJS', 90L, 0.000350567]\n",
      "['PRP$ : NONE', 90L, 0.000350567]\n",
      "['RB : PRP', 90L, 0.000350567]\n",
      "['VBZ : JJS', 89L, 0.000346672]\n",
      "['\", : PRP$\"', 86L, 0.000334986]\n",
      "['DT : JJS', 83L, 0.000323301]\n",
      "['WDT : NN', 81L, 0.00031551]\n",
      "['RBS : NN', 81L, 0.00031551]\n",
      "['WP : RB', 79L, 0.00030772]\n",
      "['JJS : NNP', 78L, 0.000303825]\n",
      "['WDT : RB', 77L, 0.000299929]\n",
      "['RBS : IN', 76L, 0.000296034]\n",
      "['VBN : RB', 75L, 0.000292139]\n",
      "['VBZ : VB', 75L, 0.000292139]\n",
      "['JJR : CD', 74L, 0.000288244]\n",
      "['NONE : CD', 74L, 0.000288244]\n",
      "['VB : NONE', 74L, 0.000288244]\n",
      "['PRP$ : DT', 74L, 0.000288244]\n",
      "['JJS : DT', 73L, 0.000284349]\n",
      "['IN : RBS', 71L, 0.000276558]\n",
      "['TO : PRP$', 70L, 0.000272663]\n",
      "['VBG : VB', 68L, 0.000264873]\n",
      "['WP : NNP', 67L, 0.000260978]\n",
      "['VBD : JJS', 67L, 0.000260978]\n",
      "['VBP : CD', 67L, 0.000260978]\n",
      "['\", : NNS\"', 66L, 0.000257082]\n",
      "['VBN : PRP', 61L, 0.000237606]\n",
      "['PRP : PRP', 61L, 0.000237606]\n",
      "['VBD : NONE', 61L, 0.000237606]\n",
      "['NONE : DT', 60L, 0.000233711]\n",
      "[') : NNP', 60L, 0.000233711]\n",
      "['MD : NN', 59L, 0.000229816]\n",
      "['WDT : NNP', 56L, 0.000218131]\n",
      "['MD : DT', 53L, 0.000206445]\n",
      "['PRP : VBG', 52L, 0.00020255]\n",
      "['NN : JJR', 52L, 0.00020255]\n",
      "['WDT : JJ', 51L, 0.000198655]\n",
      "['NNP : NNPS', 50L, 0.000194759]\n",
      "['RB : NONE', 49L, 0.000190864]\n",
      "['DT : PRP$', 49L, 0.000190864]\n",
      "['PRP : VBZ', 48L, 0.000186969]\n",
      "['IN : JJR', 47L, 0.000183074]\n",
      "[': : IN', 47L, 0.000183074]\n",
      "['VBP : RB', 47L, 0.000183074]\n",
      "['VB : PRP', 46L, 0.000179179]\n",
      "['PRP : NNS', 46L, 0.000179179]\n",
      "['VB : CD', 45L, 0.000175283]\n",
      "['. : VBP', 45L, 0.000175283]\n",
      "[': : CD', 43L, 0.000167493]\n",
      "['CD : VB', 42L, 0.000163598]\n",
      "['. : TO', 42L, 0.000163598]\n",
      "['JJS : NNS', 41L, 0.000159703]\n",
      "['. : PRP$', 41L, 0.000159703]\n",
      "['PRP$ : RB', 41L, 0.000159703]\n",
      "['EX : DT', 40L, 0.000155808]\n",
      "['VBZ : PRP', 39L, 0.000151912]\n",
      "['WDT : VB', 39L, 0.000151912]\n",
      "['VB : VBZ', 38L, 0.000148017]\n",
      "['WP : TO', 38L, 0.000148017]\n",
      "['CC : JJS', 37L, 0.000144122]\n",
      "['WP : NN', 36L, 0.000140227]\n",
      "['RBS : NNS', 36L, 0.000140227]\n",
      "['JJS : JJ', 35L, 0.000136332]\n",
      "['VBZ : PRP$', 34L, 0.000132436]\n",
      "[') : CD', 34L, 0.000132436]\n",
      "['JJ : WRB', 34L, 0.000132436]\n",
      "['NNP : EX', 33L, 0.000128541]\n",
      "['VB : VBG', 33L, 0.000128541]\n",
      "['PDT : NN', 33L, 0.000128541]\n",
      "['NNP : JJR', 32L, 0.000124646]\n",
      "['VBG : VBN', 32L, 0.000124646]\n",
      "['JJR : NNP', 32L, 0.000124646]\n",
      "['VBN : NONE', 31L, 0.000120751]\n",
      "['VBD : VBZ', 30L, 0.000116856]\n",
      "['TO : VBG', 30L, 0.000116856]\n",
      "['NNP : WP$', 30L, 0.000116856]\n",
      "['TO : RP', 30L, 0.000116856]\n",
      "['VB : PRP$', 30L, 0.000116856]\n",
      "['VBG : PRP', 29L, 0.00011296]\n",
      "['DT : RBS', 29L, 0.00011296]\n",
      "['VBP : PRP', 29L, 0.00011296]\n",
      "['WP : JJ', 29L, 0.00011296]\n",
      "['MD : TO', 28L, 0.000109065]\n",
      "['WP : VB', 28L, 0.000109065]\n",
      "['RP : JJ', 28L, 0.000109065]\n",
      "['RP : NNP', 28L, 0.000109065]\n",
      "['MD : NNP', 27L, 0.00010517]\n",
      "['WDT : PRP', 27L, 0.00010517]\n",
      "['VBD : MD', 27L, 0.00010517]\n",
      "['NNPS : VBD', 27L, 0.00010517]\n",
      "['. : PRP', 27L, 0.00010517]\n",
      "['NONE : TO', 26L, 0.000101275]\n",
      "['CC : JJR', 26L, 0.000101275]\n",
      "['RP : DT', 26L, 0.000101275]\n",
      "['CD : JJR', 26L, 0.000101275]\n",
      "['NNP : RP', 26L, 0.000101275]\n",
      "['VBZ : VBD', 26L, 0.000101275]\n",
      "['VBZ : RBS', 25L, 9.74e-05]\n",
      "['NNPS : PRP', 25L, 9.74e-05]\n",
      "['PRP$ : VBN', 25L, 9.74e-05]\n",
      "['NNPS : VBN', 25L, 9.74e-05]\n",
      "[': : DT', 24L, 9.35e-05]\n",
      "['VBP : VBG', 24L, 9.35e-05]\n",
      "['\", : JJS\"', 24L, 9.35e-05]\n",
      "['VBG : RB', 24L, 9.35e-05]\n",
      "['VBD : RP', 24L, 9.35e-05]\n",
      "['NN : RBS', 23L, 8.96e-05]\n",
      "['VBZ : VBZ', 23L, 8.96e-05]\n",
      "['NN : NNPS', 23L, 8.96e-05]\n",
      "['MD : JJ', 23L, 8.96e-05]\n",
      "['JJS : PRP$', 23L, 8.96e-05]\n",
      "['VBZ : JJR', 23L, 8.96e-05]\n",
      "['EX : CD', 23L, 8.96e-05]\n",
      "['WP : PRP', 22L, 8.57e-05]\n",
      "['RBS : JJ', 22L, 8.57e-05]\n",
      "['WDT : TO', 22L, 8.57e-05]\n",
      "['VBP : VBP', 22L, 8.57e-05]\n",
      "['NNPS : VB', 22L, 8.57e-05]\n",
      "['NNS : MD', 22L, 8.57e-05]\n",
      "['RBR : IN', 21L, 8.18e-05]\n",
      "['JJR : DT', 21L, 8.18e-05]\n",
      "['NNS : JJR', 21L, 8.18e-05]\n",
      "['NNP : RBR', 21L, 8.18e-05]\n",
      "['NN : RP', 21L, 8.18e-05]\n",
      "['WP : CD', 21L, 8.18e-05]\n",
      "['VBD : NNPS', 20L, 7.79e-05]\n",
      "['VBG : NONE', 20L, 7.79e-05]\n",
      "['MD : PRP$', 20L, 7.79e-05]\n",
      "['WP$ : VBD', 20L, 7.79e-05]\n",
      "['VBD : JJR', 19L, 7.4e-05]\n",
      "['RB : JJS', 19L, 7.4e-05]\n",
      "['PRP : RP', 19L, 7.4e-05]\n",
      "['WRB : JJ', 19L, 7.4e-05]\n",
      "['NNPS : CD', 18L, 7.01e-05]\n",
      "['PRP : VBP', 18L, 7.01e-05]\n",
      "['RB : WRB', 18L, 7.01e-05]\n",
      "[') : IN', 18L, 7.01e-05]\n",
      "['PRP : NONE', 18L, 7.01e-05]\n",
      "['JJ : NNPS', 18L, 7.01e-05]\n",
      "['CC : RP', 17L, 6.62e-05]\n",
      "['WDT : CD', 17L, 6.62e-05]\n",
      "['CD : WRB', 17L, 6.62e-05]\n",
      "['EX : RB', 17L, 6.62e-05]\n",
      "['VBN : VBD', 16L, 6.23e-05]\n",
      "['NNPS : RB', 16L, 6.23e-05]\n",
      "['RP : NNS', 15L, 5.84e-05]\n",
      "['NNS : RP', 15L, 5.84e-05]\n",
      "['PRP : JJS', 15L, 5.84e-05]\n",
      "['NNPS : JJ', 15L, 5.84e-05]\n",
      "['VBP : NONE', 15L, 5.84e-05]\n",
      "['RBS : CC', 15L, 5.84e-05]\n",
      "['TO : JJR', 15L, 5.84e-05]\n",
      "['FW : IN', 15L, 5.84e-05]\n",
      "['NNPS : NONE', 15L, 5.84e-05]\n",
      "['VBN : JJS', 14L, 5.45e-05]\n",
      "['\", : VB\"', 14L, 5.45e-05]\n",
      "['POS : JJ', 14L, 5.45e-05]\n",
      "['MD : PRP', 14L, 5.45e-05]\n",
      "['NNS : JJS', 14L, 5.45e-05]\n",
      "['CD : JJS', 14L, 5.45e-05]\n",
      "['NNS : WP', 14L, 5.45e-05]\n",
      "['PRP$ : PRP$', 14L, 5.45e-05]\n",
      "['. : VBN', 14L, 5.45e-05]\n",
      "['NNS : WRB', 13L, 5.06e-05]\n",
      "['PDT : NNP', 13L, 5.06e-05]\n",
      "['WP : PRP$', 13L, 5.06e-05]\n",
      "['VB : JJR', 13L, 5.06e-05]\n",
      "['NNP : RBS', 13L, 5.06e-05]\n",
      "['FW : VBD', 13L, 5.06e-05]\n",
      "['NONE : NNPS', 13L, 5.06e-05]\n",
      "['RP : VB', 13L, 5.06e-05]\n",
      "['JJR : CC', 13L, 5.06e-05]\n",
      "['TO : JJS', 13L, 5.06e-05]\n",
      "['DT : FW', 12L, 4.67e-05]\n",
      "['NONE : FW', 12L, 4.67e-05]\n",
      "['MD : CC', 12L, 4.67e-05]\n",
      "['DT : JJR', 12L, 4.67e-05]\n",
      "['WRB : VBP', 12L, 4.67e-05]\n",
      "['VB : WDT', 12L, 4.67e-05]\n",
      "['PDT : JJ', 12L, 4.67e-05]\n",
      "['RP : CD', 12L, 4.67e-05]\n",
      "['TO : PDT', 12L, 4.67e-05]\n",
      "['WP : NNS', 12L, 4.67e-05]\n",
      "['NNPS : NNPS', 12L, 4.67e-05]\n",
      "['NNS : NNPS', 12L, 4.67e-05]\n",
      "['EX : JJ', 12L, 4.67e-05]\n",
      "['JJ : JJR', 12L, 4.67e-05]\n",
      "['MD : RB', 12L, 4.67e-05]\n",
      "['JJS : VBD', 12L, 4.67e-05]\n",
      "['VBG : NNPS', 11L, 4.28e-05]\n",
      "['RB : RP', 11L, 4.28e-05]\n",
      "['JJR : NONE', 11L, 4.28e-05]\n",
      "['POS : NONE', 11L, 4.28e-05]\n",
      "['IN : EX', 11L, 4.28e-05]\n",
      "['VBP : PRP$', 11L, 4.28e-05]\n",
      "['WDT : VBP', 11L, 4.28e-05]\n",
      "['RB : JJR', 11L, 4.28e-05]\n",
      "['PRP$ : CD', 11L, 4.28e-05]\n",
      "['JJS : NONE', 11L, 4.28e-05]\n",
      "['VBP : JJR', 11L, 4.28e-05]\n",
      "['JJS : VBZ', 11L, 4.28e-05]\n",
      "['VBN : JJR', 11L, 4.28e-05]\n",
      "['NN : EX', 10L, 3.9e-05]\n",
      "['IN : PDT', 10L, 3.9e-05]\n",
      "['WP : CC', 10L, 3.9e-05]\n",
      "['JJR : JJ', 10L, 3.9e-05]\n",
      "['MD : NNS', 10L, 3.9e-05]\n",
      "['POS : PRP', 10L, 3.9e-05]\n",
      "['TO : VBZ', 10L, 3.9e-05]\n",
      "['TO : NNPS', 10L, 3.9e-05]\n",
      "['PRP$ : VB', 10L, 3.9e-05]\n",
      "['POS : DT', 10L, 3.9e-05]\n",
      "['JJ : EX', 10L, 3.9e-05]\n",
      "['MD : CD', 9L, 3.51e-05]\n",
      "['RBR : VBN', 9L, 3.51e-05]\n",
      "['NNPS : PRP$', 9L, 3.51e-05]\n",
      "['NNPS : NNS', 9L, 3.51e-05]\n",
      "['VBG : JJS', 9L, 3.51e-05]\n",
      "['VB : WRB', 9L, 3.51e-05]\n",
      "['WP : VBG', 9L, 3.51e-05]\n",
      "['NONE : JJS', 9L, 3.51e-05]\n",
      "['PRP$ : WRB', 9L, 3.51e-05]\n",
      "['PRP : RBR', 9L, 3.51e-05]\n",
      "['JJS : VBP', 9L, 3.51e-05]\n",
      "['. : JJS', 9L, 3.51e-05]\n",
      "['. : NNPS', 8L, 3.12e-05]\n",
      "['NN : FW', 8L, 3.12e-05]\n",
      "['RBS : VBN', 8L, 3.12e-05]\n",
      "['WDT : RP', 8L, 3.12e-05]\n",
      "[': : VB', 8L, 3.12e-05]\n",
      "['NN : $', 8L, 3.12e-05]\n",
      "['VBZ : NNPS', 8L, 3.12e-05]\n",
      "['PDT : NNS', 8L, 3.12e-05]\n",
      "['POS : NNS', 8L, 3.12e-05]\n",
      "['WP : RP', 8L, 3.12e-05]\n",
      "['RBR : DT', 8L, 3.12e-05]\n",
      "[') : JJ', 8L, 3.12e-05]\n",
      "['CC : EX', 8L, 3.12e-05]\n",
      "['VBZ : RP', 8L, 3.12e-05]\n",
      "['. : MD', 8L, 3.12e-05]\n",
      "['NN : RBR', 8L, 3.12e-05]\n",
      "['RB : WDT', 8L, 3.12e-05]\n",
      "['WRB : VB', 8L, 3.12e-05]\n",
      "['CD : FW', 8L, 3.12e-05]\n",
      "['JJR : VBD', 8L, 3.12e-05]\n",
      "['NONE : VBG', 8L, 3.12e-05]\n",
      "['VB : NNPS', 8L, 3.12e-05]\n",
      "['PRP : JJR', 8L, 3.12e-05]\n",
      "['TO : WDT', 8L, 3.12e-05]\n",
      "['RB : WP', 8L, 3.12e-05]\n",
      "['JJS : PRP', 8L, 3.12e-05]\n",
      "['MD : VBG', 8L, 3.12e-05]\n",
      "['MD : RP', 8L, 3.12e-05]\n",
      "['WP$ : NNS', 8L, 3.12e-05]\n",
      "['JJS : VBG', 8L, 3.12e-05]\n",
      "['PRP$ : WP', 7L, 2.73e-05]\n",
      "['CC : WRB', 7L, 2.73e-05]\n",
      "['RB : RBS', 7L, 2.73e-05]\n",
      "[': : PRP$', 7L, 2.73e-05]\n",
      "['EX : VBN', 7L, 2.73e-05]\n",
      "['RBR : NNS', 7L, 2.73e-05]\n",
      "[') : VBG', 7L, 2.73e-05]\n",
      "['RBR : JJ', 7L, 2.73e-05]\n",
      "['TO : WP', 7L, 2.73e-05]\n",
      "['VBN : WDT', 7L, 2.73e-05]\n",
      "['\", : MD\"', 7L, 2.73e-05]\n",
      "['WDT : PRP$', 7L, 2.73e-05]\n",
      "['JJS : TO', 7L, 2.73e-05]\n",
      "['IN : RP', 7L, 2.73e-05]\n",
      "['JJR : PRP', 7L, 2.73e-05]\n",
      "[') : VB', 7L, 2.73e-05]\n",
      "['PRP$ : NNPS', 7L, 2.73e-05]\n",
      "['JJR : TO', 7L, 2.73e-05]\n",
      "[': : VBN', 6L, 2.34e-05]\n",
      "['JJ : FW', 6L, 2.34e-05]\n",
      "['VBP : WRB', 6L, 2.34e-05]\n",
      "['VBZ : RBR', 6L, 2.34e-05]\n",
      "['PRP$ : WDT', 6L, 2.34e-05]\n",
      "['\", : RBS\"', 6L, 2.34e-05]\n",
      "['JJS : VB', 6L, 2.34e-05]\n",
      "['\", : JJR\"', 6L, 2.34e-05]\n",
      "['WP$ : CC', 6L, 2.34e-05]\n",
      "['VBN : NNPS', 6L, 2.34e-05]\n",
      "['VB : JJS', 6L, 2.34e-05]\n",
      "['RBR : NNP', 6L, 2.34e-05]\n",
      "['FW : VBZ', 6L, 2.34e-05]\n",
      "['JJS : RBS', 6L, 2.34e-05]\n",
      "['VBG : RBS', 6L, 2.34e-05]\n",
      "['CC : WP$', 6L, 2.34e-05]\n",
      "['VBP : $', 6L, 2.34e-05]\n",
      "['JJR : JJR', 6L, 2.34e-05]\n",
      "['\", : NNPS\"', 6L, 2.34e-05]\n",
      "['VB : RBS', 6L, 2.34e-05]\n",
      "['VBP : RP', 6L, 2.34e-05]\n",
      "['JJR : VBG', 6L, 2.34e-05]\n",
      "[') : NN', 6L, 2.34e-05]\n",
      "['DT : EX', 6L, 2.34e-05]\n",
      "['NNP : $', 6L, 2.34e-05]\n",
      "['VBZ : MD', 5L, 1.95e-05]\n",
      "['\", : FW\"', 5L, 1.95e-05]\n",
      "['JJS : RB', 5L, 1.95e-05]\n",
      "['CD : MD', 5L, 1.95e-05]\n",
      "['VB : RP', 5L, 1.95e-05]\n",
      "['. : EX', 5L, 1.95e-05]\n",
      "[') : VBD', 5L, 1.95e-05]\n",
      "['VBZ : WRB', 5L, 1.95e-05]\n",
      "['RP : CC', 5L, 1.95e-05]\n",
      "['JJ : RBS', 5L, 1.95e-05]\n",
      "['WRB : DT', 5L, 1.95e-05]\n",
      "['VBG : WRB', 5L, 1.95e-05]\n",
      "['VB : WP', 5L, 1.95e-05]\n",
      "['NNPS : VBG', 5L, 1.95e-05]\n",
      "['JJ : RP', 5L, 1.95e-05]\n",
      "['WDT : VBG', 5L, 1.95e-05]\n",
      "['VBD : RBS', 5L, 1.95e-05]\n",
      "['VBN : WRB', 5L, 1.95e-05]\n",
      "['RB : RBR', 5L, 1.95e-05]\n",
      "['CD : RBS', 5L, 1.95e-05]\n",
      "['PRP : RBS', 5L, 1.95e-05]\n",
      "['WP : RBS', 4L, 1.56e-05]\n",
      "['FW : DT', 4L, 1.56e-05]\n",
      "[') : PRP$', 4L, 1.56e-05]\n",
      "['VBZ : WDT', 4L, 1.56e-05]\n",
      "['JJR : RB', 4L, 1.56e-05]\n",
      "['$ : CD', 4L, 1.56e-05]\n",
      "['FW : PRP', 4L, 1.56e-05]\n",
      "['. : NONE', 4L, 1.56e-05]\n",
      "['RB : EX', 4L, 1.56e-05]\n",
      "['RBR : NONE', 4L, 1.56e-05]\n",
      "['JJ : $', 4L, 1.56e-05]\n",
      "['RBR : CC', 4L, 1.56e-05]\n",
      "[': : EX', 4L, 1.56e-05]\n",
      "['JJS : WDT', 4L, 1.56e-05]\n",
      "[': : PRP', 4L, 1.56e-05]\n",
      "['FW : VBG', 4L, 1.56e-05]\n",
      "['VBG : MD', 4L, 1.56e-05]\n",
      "['$ : VBD', 4L, 1.56e-05]\n",
      "['RP : PRP$', 4L, 1.56e-05]\n",
      "['NNPS : VBP', 4L, 1.56e-05]\n",
      "['WDT : RBR', 4L, 1.56e-05]\n",
      "['JJR : VBP', 4L, 1.56e-05]\n",
      "['VB : MD', 4L, 1.56e-05]\n",
      "['VB : PDT', 4L, 1.56e-05]\n",
      "['POS : VBN', 4L, 1.56e-05]\n",
      "['CC : FW', 4L, 1.56e-05]\n",
      "['( : $', 4L, 1.56e-05]\n",
      "['RP : WRB', 4L, 1.56e-05]\n",
      "['JJS : JJS', 4L, 1.56e-05]\n",
      "['PRP : MD', 4L, 1.56e-05]\n",
      "['JJR : PRP$', 4L, 1.56e-05]\n",
      "['CD : EX', 4L, 1.56e-05]\n",
      "['. : JJR', 4L, 1.56e-05]\n",
      "['PRP : PDT', 3L, 1.17e-05]\n",
      "['PRP$ : JJR', 3L, 1.17e-05]\n",
      "['RBS : VBD', 3L, 1.17e-05]\n",
      "['RBR : VBD', 3L, 1.17e-05]\n",
      "['VBP : RBS', 3L, 1.17e-05]\n",
      "['\", : WRB\"', 3L, 1.17e-05]\n",
      "['RB : NNPS', 3L, 1.17e-05]\n",
      "['POS : VBP', 3L, 1.17e-05]\n",
      "['NNPS : TO', 3L, 1.17e-05]\n",
      "[') : TO', 3L, 1.17e-05]\n",
      "['VB : EX', 3L, 1.17e-05]\n",
      "[') : VBP', 3L, 1.17e-05]\n",
      "[') : $', 3L, 1.17e-05]\n",
      "['RP : VBN', 3L, 1.17e-05]\n",
      "['VBN : MD', 3L, 1.17e-05]\n",
      "['JJS : CD', 3L, 1.17e-05]\n",
      "['WRB : PRP', 3L, 1.17e-05]\n",
      "['RBR : CD', 3L, 1.17e-05]\n",
      "['VBG : JJR', 3L, 1.17e-05]\n",
      "['VBP : WP', 3L, 1.17e-05]\n",
      "['WP$ : PRP', 3L, 1.17e-05]\n",
      "['VBP : RBR', 3L, 1.17e-05]\n",
      "['\", : RP\"', 3L, 1.17e-05]\n",
      "['VBP : NNPS', 3L, 1.17e-05]\n",
      "['PRP : (', 3L, 1.17e-05]\n",
      "['RB : FW', 3L, 1.17e-05]\n",
      "['VB : (', 3L, 1.17e-05]\n",
      "['POS : VBG', 3L, 1.17e-05]\n",
      "[') : RB', 3L, 1.17e-05]\n",
      "['JJ : RBR', 3L, 1.17e-05]\n",
      "['NNPS : WP', 3L, 1.17e-05]\n",
      "['WRB : MD', 3L, 1.17e-05]\n",
      "[': : FW', 3L, 1.17e-05]\n",
      "['MD : NONE', 3L, 1.17e-05]\n",
      "['JJR : WP', 3L, 1.17e-05]\n",
      "['NNP : SYM', 3L, 1.17e-05]\n",
      "['NONE : JJR', 3L, 1.17e-05]\n",
      "['PRP : WRB', 3L, 1.17e-05]\n",
      "['CC : $', 3L, 1.17e-05]\n",
      "['POS : RB', 3L, 1.17e-05]\n",
      "['NNS : RBS', 3L, 1.17e-05]\n",
      "[') : CC', 3L, 1.17e-05]\n",
      "['NNPS : VBZ', 3L, 1.17e-05]\n",
      "['VBN : RBR', 3L, 1.17e-05]\n",
      "['WDT : WRB', 3L, 1.17e-05]\n",
      "['PRP : WP', 3L, 1.17e-05]\n",
      "['MD : VBD', 3L, 1.17e-05]\n",
      "['FW : (', 3L, 1.17e-05]\n",
      "['VBP : WDT', 3L, 1.17e-05]\n",
      "['WRB : CD', 3L, 1.17e-05]\n",
      "['SYM : NN', 3L, 1.17e-05]\n",
      "['WP$ : VBN', 3L, 1.17e-05]\n",
      "['EX : IN', 3L, 1.17e-05]\n",
      "['WP$ : IN', 3L, 1.17e-05]\n",
      "['FW : VB', 2L, 7.79e-06]\n",
      "['JJR : VBN', 2L, 7.79e-06]\n",
      "['RBS : DT', 2L, 7.79e-06]\n",
      "[') : NONE', 2L, 7.79e-06]\n",
      "['WRB : TO', 2L, 7.79e-06]\n",
      "['RP : TO', 2L, 7.79e-06]\n",
      "['RBS : VBP', 2L, 7.79e-06]\n",
      "['VBZ : FW', 2L, 7.79e-06]\n",
      "['PRP : $', 2L, 7.79e-06]\n",
      "['( : DT', 2L, 7.79e-06]\n",
      "['MD : WRB', 2L, 7.79e-06]\n",
      "['$ : $', 2L, 7.79e-06]\n",
      "['RP : PRP', 2L, 7.79e-06]\n",
      "['NONE : PRP$', 2L, 7.79e-06]\n",
      "['VBD : WP$', 2L, 7.79e-06]\n",
      "['NNPS : MD', 2L, 7.79e-06]\n",
      "[') : WDT', 2L, 7.79e-06]\n",
      "['JJR : WDT', 2L, 7.79e-06]\n",
      "['VB : :', 2L, 7.79e-06]\n",
      "['VBZ : VBP', 2L, 7.79e-06]\n",
      "['RP : VBD', 2L, 7.79e-06]\n",
      "['NNP : PDT', 2L, 7.79e-06]\n",
      "['FW : CD', 2L, 7.79e-06]\n",
      "['DT : SYM', 2L, 7.79e-06]\n",
      "['JJR : VB', 2L, 7.79e-06]\n",
      "['\"PRP : ,\"', 2L, 7.79e-06]\n",
      "['NNPS : JJR', 2L, 7.79e-06]\n",
      "['WRB : VBG', 2L, 7.79e-06]\n",
      "[': : WP', 2L, 7.79e-06]\n",
      "['FW : NONE', 2L, 7.79e-06]\n",
      "[': : JJS', 2L, 7.79e-06]\n",
      "['RBR : RBR', 2L, 7.79e-06]\n",
      "['VBG : PDT', 2L, 7.79e-06]\n",
      "[': : $', 2L, 7.79e-06]\n",
      "['VBD : $', 2L, 7.79e-06]\n",
      "['RBS : VBG', 2L, 7.79e-06]\n",
      "['$ : IN', 2L, 7.79e-06]\n",
      "[') : WP', 2L, 7.79e-06]\n",
      "['WDT : JJR', 2L, 7.79e-06]\n",
      "[\"NNP : ''\", 2L, 7.79e-06]\n",
      "['FW : VBN', 2L, 7.79e-06]\n",
      "['DT : RBR', 2L, 7.79e-06]\n",
      "['WP$ : JJ', 2L, 7.79e-06]\n",
      "['NNPS : WDT', 2L, 7.79e-06]\n",
      "['SYM : NNP', 2L, 7.79e-06]\n",
      "['$ : VBZ', 2L, 7.79e-06]\n",
      "['JJR : JJS', 2L, 7.79e-06]\n",
      "['$ : NONE', 2L, 7.79e-06]\n",
      "[': : TO', 2L, 7.79e-06]\n",
      "['RBS : TO', 2L, 7.79e-06]\n",
      "['RP : MD', 2L, 7.79e-06]\n",
      "['MD : VBZ', 2L, 7.79e-06]\n",
      "['RBR : PDT', 2L, 7.79e-06]\n",
      "['NNS : EX', 2L, 7.79e-06]\n",
      "['RBR : TO', 2L, 7.79e-06]\n",
      "['. : VB', 2L, 7.79e-06]\n",
      "['WRB : NNPS', 2L, 7.79e-06]\n",
      "['EX : NNS', 2L, 7.79e-06]\n",
      "['CD : RBR', 2L, 7.79e-06]\n",
      "['WP : :', 2L, 7.79e-06]\n",
      "['VBG : EX', 2L, 7.79e-06]\n",
      "['JJS : WP', 2L, 7.79e-06]\n",
      "['PRP : WDT', 2L, 7.79e-06]\n",
      "['WRB : NONE', 2L, 7.79e-06]\n",
      "['PRP : :', 2L, 7.79e-06]\n",
      "['VBD : WDT', 2L, 7.79e-06]\n",
      "['VBD : EX', 2L, 7.79e-06]\n",
      "['RP : NONE', 2L, 7.79e-06]\n",
      "['RBR : PRP$', 2L, 7.79e-06]\n",
      "['EX : CC', 2L, 7.79e-06]\n",
      "['WP : JJS', 2L, 7.79e-06]\n",
      "['CC : RBR', 2L, 7.79e-06]\n",
      "['RBR : VBZ', 2L, 7.79e-06]\n",
      "['NNS : RBR', 2L, 7.79e-06]\n",
      "['VBP : MD', 2L, 7.79e-06]\n",
      "['VBN : RP', 2L, 7.79e-06]\n",
      "['TO : RBS', 2L, 7.79e-06]\n",
      "['TO : RBR', 2L, 7.79e-06]\n",
      "['EX : PRP$', 1L, 3.9e-06]\n",
      "[': : VBG', 1L, 3.9e-06]\n",
      "['( : VB', 1L, 3.9e-06]\n",
      "['VBD : PDT', 1L, 3.9e-06]\n",
      "['VBP : PDT', 1L, 3.9e-06]\n",
      "['WP$ : RB', 1L, 3.9e-06]\n",
      "['VBD : FW', 1L, 3.9e-06]\n",
      "['. : .', 1L, 3.9e-06]\n",
      "['MD : JJR', 1L, 3.9e-06]\n",
      "['WDT : EX', 1L, 3.9e-06]\n",
      "['. : WRB', 1L, 3.9e-06]\n",
      "['VBZ : EX', 1L, 3.9e-06]\n",
      "['$ : PRP', 1L, 3.9e-06]\n",
      "['JJR : :', 1L, 3.9e-06]\n",
      "['SYM : JJ', 1L, 3.9e-06]\n",
      "['PRP$ : JJS', 1L, 3.9e-06]\n",
      "['( : NNS', 1L, 3.9e-06]\n",
      "['VBN : POS', 1L, 3.9e-06]\n",
      "['MD : RBS', 1L, 3.9e-06]\n",
      "['FW : RB', 1L, 3.9e-06]\n",
      "[\"NNS : ''\", 1L, 3.9e-06]\n",
      "[\"NONE : ''\", 1L, 3.9e-06]\n",
      "[\"VBG : ''\", 1L, 3.9e-06]\n",
      "['PRP$ : (', 1L, 3.9e-06]\n",
      "['RBS : NNPS', 1L, 3.9e-06]\n",
      "['RBR : PRP', 1L, 3.9e-06]\n",
      "['NNS : )', 1L, 3.9e-06]\n",
      "[\"JJ : ''\", 1L, 3.9e-06]\n",
      "['DT : PDT', 1L, 3.9e-06]\n",
      "['$ : CC', 1L, 3.9e-06]\n",
      "['\", : EX\"', 1L, 3.9e-06]\n",
      "['VBG : WP$', 1L, 3.9e-06]\n",
      "['WRB : VBN', 1L, 3.9e-06]\n",
      "['VB : RBR', 1L, 3.9e-06]\n",
      "['TO : FW', 1L, 3.9e-06]\n",
      "['\", : $\"', 1L, 3.9e-06]\n",
      "['JJR : NNPS', 1L, 3.9e-06]\n",
      "['VBN : PDT', 1L, 3.9e-06]\n",
      "['VBP : FW', 1L, 3.9e-06]\n",
      "['POS : FW', 1L, 3.9e-06]\n",
      "['EX : VBD', 1L, 3.9e-06]\n",
      "['CD : RP', 1L, 3.9e-06]\n",
      "['EX : VBZ', 1L, 3.9e-06]\n",
      "['VBZ : $', 1L, 3.9e-06]\n",
      "[') : (', 1L, 3.9e-06]\n",
      "['VBD : )', 1L, 3.9e-06]\n",
      "['RP : VBG', 1L, 3.9e-06]\n",
      "['RP : VBP', 1L, 3.9e-06]\n",
      "['POS : VB', 1L, 3.9e-06]\n",
      "['NNPS : FW', 1L, 3.9e-06]\n",
      "['POS : WRB', 1L, 3.9e-06]\n",
      "['IN : SYM', 1L, 3.9e-06]\n",
      "['WRB : JJS', 1L, 3.9e-06]\n",
      "['NNPS : (', 1L, 3.9e-06]\n",
      "['VBN : (', 1L, 3.9e-06]\n",
      "[': : NNPS', 1L, 3.9e-06]\n",
      "['FW : JJ', 1L, 3.9e-06]\n",
      "['VBZ : (', 1L, 3.9e-06]\n",
      "['JJ : SYM', 1L, 3.9e-06]\n",
      "['WDT : PDT', 1L, 3.9e-06]\n",
      "['TO : VBP', 1L, 3.9e-06]\n",
      "['TO : (', 1L, 3.9e-06]\n",
      "[': : (', 1L, 3.9e-06]\n",
      "[') : RBR', 1L, 3.9e-06]\n",
      "['NNPS : PDT', 1L, 3.9e-06]\n",
      "['PDT : CD', 1L, 3.9e-06]\n",
      "[') : PRP', 1L, 3.9e-06]\n",
      "['WDT : NNPS', 1L, 3.9e-06]\n",
      "['MD : MD', 1L, 3.9e-06]\n",
      "['EX : VB', 1L, 3.9e-06]\n",
      "['WP$ : DT', 1L, 3.9e-06]\n",
      "['RP : :', 1L, 3.9e-06]\n",
      "['WDT : JJS', 1L, 3.9e-06]\n",
      "[\"CD : ''\", 1L, 3.9e-06]\n",
      "['FW : VBP', 1L, 3.9e-06]\n",
      "['RBR : VB', 1L, 3.9e-06]\n",
      "[') : NNPS', 1L, 3.9e-06]\n",
      "['VBG : (', 1L, 3.9e-06]\n",
      "['CD : WP$', 1L, 3.9e-06]\n",
      "['VB : )', 1L, 3.9e-06]\n",
      "['JJ : PDT', 1L, 3.9e-06]\n",
      "['FW : NNS', 1L, 3.9e-06]\n",
      "['RP : NNPS', 1L, 3.9e-06]\n",
      "['WP : JJR', 1L, 3.9e-06]\n",
      "['JJR : RBR', 1L, 3.9e-06]\n",
      "['$ : VBN', 1L, 3.9e-06]\n",
      "['POS : WP', 1L, 3.9e-06]\n",
      "['NNS : WP$', 1L, 3.9e-06]\n",
      "['. : RBS', 1L, 3.9e-06]\n",
      "['. : RBR', 1L, 3.9e-06]\n",
      "['VBN : :', 1L, 3.9e-06]\n",
      "[': : PDT', 1L, 3.9e-06]\n",
      "['VBN : EX', 1L, 3.9e-06]\n",
      "['RBR : VBP', 1L, 3.9e-06]\n",
      "['RBR : RB', 1L, 3.9e-06]\n",
      "['EX : JJR', 1L, 3.9e-06]\n",
      "['( : MD', 1L, 3.9e-06]\n",
      "['JJS : PDT', 1L, 3.9e-06]\n",
      "['VB : $', 1L, 3.9e-06]\n",
      "['( : JJ', 1L, 3.9e-06]\n",
      "['MD : EX', 1L, 3.9e-06]\n",
      "['VBZ : )', 1L, 3.9e-06]\n",
      "['NONE : RP', 1L, 3.9e-06]\n",
      "['NNS : $', 1L, 3.9e-06]\n",
      "['WP$ : VBP', 1L, 3.9e-06]\n",
      "[') : VBZ', 1L, 3.9e-06]\n",
      "['SYM : :', 1L, 3.9e-06]\n",
      "['WP : FW', 1L, 3.9e-06]\n",
      "['$ : JJS', 1L, 3.9e-06]\n",
      "['PRP$ : POS', 1L, 3.9e-06]\n",
      "['NONE : EX', 1L, 3.9e-06]\n",
      "['RP : RB', 1L, 3.9e-06]\n",
      "['POS : NNPS', 1L, 3.9e-06]\n",
      "[\"' : CC\", 1L, 3.9e-06]\n",
      "[\"' : VBN\", 1L, 3.9e-06]\n",
      "[\"' : VBG\", 1L, 3.9e-06]\n",
      "[\"' : NNP\", 1L, 3.9e-06]\n",
      "['MD : VBP', 1L, 3.9e-06]\n",
      "['MD : POS', 1L, 3.9e-06]\n",
      "['POS : WP$', 1L, 3.9e-06]\n",
      "['NNS : PDT', 1L, 3.9e-06]\n",
      "['JJR : MD', 1L, 3.9e-06]\n",
      "['POS : JJS', 1L, 3.9e-06]\n",
      "['RP : WDT', 1L, 3.9e-06]\n",
      "['VBZ : PDT', 1L, 3.9e-06]\n",
      "['WP : NNPS', 1L, 3.9e-06]\n",
      "['WDT : NONE', 1L, 3.9e-06]\n",
      "['MD : NNPS', 1L, 3.9e-06]\n",
      "['RB : :', 1L, 3.9e-06]\n",
      "['JJS : (', 1L, 3.9e-06]\n",
      "['JJS : )', 1L, 3.9e-06]\n",
      "['JJS : :', 1L, 3.9e-06]\n",
      "[') : JJS', 1L, 3.9e-06]\n",
      "[') : NNS', 1L, 3.9e-06]\n",
      "['PRP$ : RP', 1L, 3.9e-06]\n",
      "['VB : FW', 1L, 3.9e-06]\n",
      "['PDT : VBG', 1L, 3.9e-06]\n",
      "['JJR : WRB', 1L, 3.9e-06]\n",
      "['VBG : FW', 1L, 3.9e-06]\n",
      "['PRP$ : EX', 1L, 3.9e-06]\n",
      "sum: 29773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nGood ones:\\n'NNP : DT'\\n'NN : DT'\\n'NNP : CD'\\nand pretty much all non-mention solos\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This is to see the difference between pos tags of mentions and non-mentions.\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "posDict = {'mentions':{}, 'non-mentions':{}}\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikisim/posdata.txt', 'r') as posFile:\n",
    "    content = posFile.readlines()\n",
    "    for line in content[1:]:\n",
    "        theLine = line.split('\\t')\n",
    "        if theLine[0] <> '':\n",
    "            posDict['mentions'][theLine[0]] = [long(theLine[1]),float(theLine[2].replace('\\\\n',''))]\n",
    "        posDict['non-mentions'][theLine[3]] = [long(theLine[4]),float(theLine[5].replace('\\\\n',''))]\n",
    "        \n",
    "diffs = []\n",
    "rawDiffs = []\n",
    "mSolos =[]\n",
    "for mKey in posDict['mentions'].keys():\n",
    "    if mKey in posDict['non-mentions']:\n",
    "        # find abs diff between \n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][1] - posDict['non-mentions'][mKey][1])]\n",
    "        diffs.append(dif)\n",
    "        dif = [mKey, abs(posDict['mentions'][mKey][0] - posDict['non-mentions'][mKey][0])]\n",
    "        rawDiffs.append(dif)\n",
    "    else:\n",
    "        # only for mentions\n",
    "        solo = [mKey, posDict['mentions'][mKey][1]]\n",
    "        mSolos.append(solo)\n",
    "nSolos =[]\n",
    "for mKey in posDict['non-mentions'].keys():\n",
    "    if mKey not in posDict['mentions']:\n",
    "        # only for non-mentions\n",
    "        solo = [mKey, posDict['non-mentions'][mKey][0], posDict['non-mentions'][mKey][1]]\n",
    "        nSolos.append(solo)\n",
    "        \n",
    "diffs = sorted(diffs, key = itemgetter(1), reverse = True)\n",
    "rawDiffs = sorted(rawDiffs, key = itemgetter(1), reverse = True)\n",
    "mSolos = sorted(mSolos, key = itemgetter(1), reverse = True)\n",
    "nSolos = sorted(nSolos, key = itemgetter(1), reverse = True)\n",
    "\n",
    "show = 10\n",
    "\n",
    "print 'Diffs:'\n",
    "for dif in diffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nRaw Diffs:'\n",
    "for dif in rawDiffs[:show]:\n",
    "    print dif\n",
    "    \n",
    "print '\\nMention Solos:'\n",
    "for solo in mSolos[:show]:\n",
    "    print solo\n",
    "    \n",
    "asum = 0\n",
    "print '\\nNon-Mention Solos:'\n",
    "for solo in nSolos:\n",
    "    print solo\n",
    "    asum += solo[1]\n",
    "    \n",
    "print 'sum: ' + str(asum)\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-filter-out-nonmentions.txt', 'w') as resultFile:\n",
    "    # all that appear only for non mentions\n",
    "    for solo in nSolos:\n",
    "        resultFile.write(str(solo[0]) + '\\n')\n",
    "    # some well hand picked ones\n",
    "    resultFile.write(\"NNP : DT\\nNN : DT\\nNNP : CD\")\n",
    "\n",
    "\"\"\"\n",
    "Good ones:\n",
    "'NNP : DT'\n",
    "'NN : DT'\n",
    "'NNP : CD'\n",
    "and pretty much all non-mention solos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "from calcsim import *\n",
    "sys.path.append('../')\n",
    "from wsd.wsd import *\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps that start at same letter from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            score = mentionProb(textData[i][2])\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "    \n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "     \n",
    "def mentionExtract(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    splitText = [] # the text now in split form\n",
    "    mentions = [] # mentions before remove inadequate ones\n",
    "    \n",
    "    textData = [] # [[begin,end,word,anchorProb],...]\n",
    "    \n",
    "    i = 0 # for wordIndex\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb, i])\n",
    "        i += 1\n",
    "        \n",
    "        # also fill split text\n",
    "        splitText.append(text[item[1]:item[3]])\n",
    "    \n",
    "    # get rid of overlaps\n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "        \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [5][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.001 # for getting rid of unlikelies\n",
    "    \n",
    "    # put in only good mentions\n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh # if popular enough, and either some type of noun or JJ or CD\n",
    "                and (item[5][1][0:2] == 'NN' or item[5][1] == 'JJ' or item[5][1] == 'CD')):\n",
    "            mentions.append([item[4], item[0], item[1]]) # wIndex, start, end\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    if asList == True:\n",
    "        words = (text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    else:\n",
    "        words = \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return words\n",
    "\n",
    "def getMentionSentence(text, mention, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            if asList == True:\n",
    "                sentence = (s.replace(text[mention[1]:mention[2]],\"\")).split(\" \")\n",
    "            else:\n",
    "                sentence = s.replace(text[mention[1]:mention[2]],\"\")\n",
    "            \n",
    "            return sentence\n",
    "        \n",
    "    # in case it missed\n",
    "    if asList == True:\n",
    "        return []\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestRelevancy1Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    #for doc in r.json()['response']['docs']:\n",
    "        #print '[' + id2title(doc['id']) + '] -> ' + str(doc['score'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestRelevancy2Match(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+context.encode('utf-8')+')',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        scoreDict[str(doc['entityid'])] += 1\n",
    "    \n",
    "    # get the index that has the best score\n",
    "    bestScore = 0\n",
    "    bestIndex = 0\n",
    "    curIndex = 0\n",
    "    for cand in candidates:\n",
    "        if scoreDict[str(cand[0])] > bestScore:\n",
    "            bestScore = scoreDict[str(cand[0])]\n",
    "            bestIndex = curIndex\n",
    "        curIndex += 1\n",
    "            \n",
    "    return bestIndex\n",
    "\n",
    "def bestWord2VecMatch(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the candidate with the best similarity to the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best similarity score with the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    bestIndex = 0\n",
    "    bestScore = 0\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        #print '[' + id2title(cand[0]) + ']' + ' -> ' + str(score)\n",
    "        # update score and index\n",
    "        if score > bestScore: \n",
    "            bestIndex = i\n",
    "            bestScore = score\n",
    "            \n",
    "        i += 1 # next index\n",
    "            \n",
    "    return bestIndex\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyRelevancy(textData, candidates, oText, useSentence = False, window = 7, method2 = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + context\n",
    "            if method2 == False:\n",
    "                bestIndex = bestRelevancy1Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            else:\n",
    "                bestIndex = bestRelevancy2Match(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest similarity to the context.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window, asList = True)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention, asList = True)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + \" \".join(context)\n",
    "            bestIndex = bestWord2VecMatch(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyCoherence(textData, candidates, ws = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest coherence according to rvs pagerank method.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ws: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCands = [] # the top candidate from each candidate list\n",
    "    candsScores = coherence_scores_driver(candidates, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    i = -1 # track what mention we are on\n",
    "    for cScores in candsScores:\n",
    "        i += 1\n",
    "        \n",
    "        if len(cScores) == 0:\n",
    "            continue # nothing to do with this one\n",
    "            \n",
    "        bestScore = sorted(cScores, reverse = True)[0]\n",
    "        curIndex = 0\n",
    "        for score in cScores:\n",
    "            if score == bestScore:\n",
    "                topCands.append([textData['mentions'][i][1], textData['mentions'][i][2], candidates[i][curIndex][0]])\n",
    "                break\n",
    "            curIndex += 1\n",
    "            \n",
    "    return topCands\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        textData = mentionExtract(text) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'relevancy1':\n",
    "        wikified = wikifyRelevancy(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    elif method == 'relevancy2':\n",
    "        wikified = wikifyRelevancy(textData, candidates, oText, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        wikified = wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end, text],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in line['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(line['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mentions.append([curStart, curStart + len(line['text'][curWord]), line['text'][curWord]])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def destroyInclusiveOverlaps(text, textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        text: The source text.\n",
    "        textData: [[_, start, _, end, _, alts, pop],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        endiestEnd = textData[i][1]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] < endiestEnd:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "                # update endiness if needed\n",
    "                if textData[j][1] > endiestEnd:\n",
    "                    endiestEnd = textData[j][1]\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "                    \n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    textData = []\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        \n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb])\n",
    "    \n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "    \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [4][1] is index of type of word\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.001\n",
    "    \n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh\n",
    "                and (item[4][1][0:2] == 'NN' or item[4][1] == 'JJ')):\n",
    "            mentions.append([item[0], item[1], item[2]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line)\n",
    "        solrMentions = getSolrMentions(\" \".join(line['text']))\n",
    "        \n",
    "        \"\"\"solrMentions0 = tagme.mentions(\" \".join(line['text']))\n",
    "        solrMentions = []\n",
    "        for item in solrMentions0.mentions:\n",
    "            solrMentions.append([item.begin, item.end, item.mention])\"\"\"\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "methods = ['popular', 'relevancy1', 'relevancy2', 'word2vec', 'coherence']\n",
    "\n",
    "#if 'word2vec' in methods: # run in different cell\n",
    "    #word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/word2vecfiles/word2vec.enwiki-20160305-replace_surface.1.0.500.5.5.15.5.5')\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # original split string with mentions given\n",
    "            #resultS = wikifyEval(copy.deepcopy(line), True, maxC = 50, method = mthd)\n",
    "            resultS = []\n",
    "            # unsplit string to be manually split and mentions found\n",
    "            resultM = wikifyEval(\" \".join(line['text']), False, maxC = 50, method = mthd)\n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            #precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "            precS = 0\n",
    "            precM = precision(trueEntities, resultM) # precision of manual split\n",
    "            #recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "            recS = 0\n",
    "            recM = recall(trueEntities, resultM) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            #print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            print str(precM) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS) + '\\n'\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines\n",
    "                                              }\n",
    "        \n",
    "clear_output()\n",
    "print performances\n",
    "for dataset in datasets:\n",
    "    print dataset['name']\n",
    "    for mthd in methods:\n",
    "        print (mthd + ':'\n",
    "               + '\\n    S Prec:' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "               + '\\n    S Rec:' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "               + '\\n    M Prec:' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "               + '\\n    M Rec:' + str(performances[dataset['name']][mthd]['M Rec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of TagMe wikification method.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "\n",
    "    print str(datetime.now()) + '\\n'\n",
    "\n",
    "    # reset counters\n",
    "    totalPrecM = 0\n",
    "    totalRecM = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        antns = tagme.annotate(\" \".join(line['text']))\n",
    "        resultM = []\n",
    "        for an in antns.get_annotations(0.005):\n",
    "            resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "        trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "\n",
    "        ## get statistical results from true entities and results\n",
    "        precM = precision(trueEntities, resultM)\n",
    "        recM = recall(trueEntities, resultM)\n",
    "\n",
    "        #clear_output() # delete this after\n",
    "        print str(precM) + ' ' + str(recM) + '\\n'\n",
    "        #print str(precS) + ' ' + str(recS)\n",
    "\n",
    "        # track results\n",
    "        totalPrecM += precM\n",
    "        totalRecM += recM\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "    performances[dataset['name']] = {'Prec':totalPrecM/totalLines,\n",
    "                                          'Rec':totalRecM/totalLines}\n",
    "            \n",
    "clear_output()\n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Try to compare the difference between mentions where the entity is the most popular and when it is not\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "maxC = 1\n",
    "corCandPosDict = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctPositions = []\n",
    "incorrectPositions = []\n",
    "correctRelevancies = []\n",
    "incorrectRelevancies = []\n",
    "for i in range(0, maxC):\n",
    "    corCandPosDict[str(i)] = 0\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "jdata = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    totalLines = 0\n",
    "    \n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        totalMentions += len(line['mentions'])\n",
    "        \n",
    "        oLine = copy.deepcopy(line)\n",
    "        \n",
    "        oMentions = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True)\n",
    "        \n",
    "        # get in right format\n",
    "        line['mentions'] = mentionStartsAndEnds(line) # put mentions in right form\n",
    "        oText = \" \".join(line['text'])\n",
    "            \n",
    "        candss = generateCandidates(line, maxC)\n",
    "        \n",
    "        copyCands = []\n",
    "        i = 0\n",
    "        for cands in candss:\n",
    "            if len(cands) > 0 and cands[0][0] != oMentions[i][2]:\n",
    "                copyCands.append(i)\n",
    "            i += 1\n",
    "            \n",
    "        # put in file\n",
    "        if len(copyCands) > 0:\n",
    "            jdata.append({'text':oLine['text'], 'mentions':[\n",
    "                oLine['mentions'][i] for i in copyCands]})\n",
    "        \n",
    "        totalLines += 1\n",
    "        \n",
    "        #print jdata\n",
    "        \n",
    "clear_output()\n",
    "with open('/users/cs/amaral/wsd-datasets/nopop.json', 'w') as f:\n",
    "    json.dump(jdata, f)\n",
    "    \n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/users/cs/amaral/wsd-datasets/nopop.json', 'w') as f:\n",
    "    for item in jdata:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posRels = []\n",
    "for i in range(0, len(correctPositions)):\n",
    "    posRels.append([correctPositions[i], relevancies[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##### https://stackoverflow.com/questions/470690/how-to-automatically-generate-n-distinct-colors\n",
    "##### Uri Cohen\n",
    "import colorsys\n",
    "def _get_colors(num_colors):\n",
    "    colors=[]\n",
    "    for i in np.arange(0., 360., 360. / num_colors):\n",
    "        hue = i/360.\n",
    "        lightness = (50 + np.random.rand() * 10)/100.\n",
    "        saturation = (90 + np.random.rand() * 10)/100.\n",
    "        colors.append(colorsys.hls_to_rgb(hue, lightness, saturation))\n",
    "    return colors\n",
    "#####\n",
    "#####\n",
    "\n",
    "maxC = 50\n",
    "df = pickle.load(open('/users/cs/amaral/wikisim/storage/correct-pos-frequencies.pkl', 'rb'))\n",
    "\n",
    "# https://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html\n",
    "f, ax = plt.subplots(1, figsize =(20,10))\n",
    "bar_width = 1\n",
    "bar_l = [i for i in range(len(df['freq_lvl']))]\n",
    "tick_pos = [i+(bar_width/2) for i in bar_l]\n",
    "\n",
    "# get the total at each level\n",
    "totals = []\n",
    "totalIndex = -1\n",
    "for i in range(0,len(df['freq_lvl'])):\n",
    "    totals.append(0)\n",
    "    totalIndex += 1\n",
    "    for j in range(0,maxC):\n",
    "        totals[totalIndex] += df['rank: ' + str(j)][i]\n",
    "        \n",
    "# get the percentage of each rank at each level\n",
    "data_rel = []\n",
    "drIndex = -1\n",
    "for i in range(0,maxC): # for each rank\n",
    "    data_rel.append([])\n",
    "    drIndex += 1\n",
    "    for j in range(0,len(df['freq_lvl'])): # for each level\n",
    "        data_rel[drIndex].append((df['rank: ' + str(i)][j] / totals[j]) * 100)\n",
    "        \n",
    "colors = _get_colors(maxC)\n",
    "\n",
    "# get numbers for stacking bars\n",
    "stackNums = []\n",
    "stackNums.append([0]*len(df['freq_lvl'])) # last all zero\n",
    "for i in range(1,maxC):\n",
    "    stackNums.append(copy.deepcopy(stackNums[i - 1]))\n",
    "    for j in range(0,len(df['freq_lvl'])):\n",
    "        # add all of current column\n",
    "        stackNums[i][j] += (df['rank: '+str(maxC - i)][j] / totals[j]) * 100\n",
    "\n",
    "for i in range(0, maxC):\n",
    "    ax.bar(bar_l,\n",
    "          data_rel[i],\n",
    "          bottom=stackNums[maxC - i - 1],\n",
    "          label='Rank: ' + str(i),\n",
    "          alpha=1,\n",
    "          color=colors[i],\n",
    "          width=bar_width,\n",
    "          edgecolor='white')\n",
    "    \n",
    "plt.xticks(tick_pos, df['freq_lvl'])\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xlabel('Relevancy Threshold')\n",
    "plt.xlim([min(tick_pos)-bar_width, max(tick_pos)])\n",
    "plt.ylim(-5, 105)\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='The red data')\n",
    "\n",
    "\n",
    "plt.legend(handles=[mpatches.Patch(color=colors[i], label='Rank: '+str(i)) for i in range(0,10)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "raw_data = {'freq_lvl': [i for i in range(300,-1,-10)]} # all the freq levels\n",
    "\n",
    "for i in range(0, maxC):\n",
    "    raw_data['rank: ' + str(i)] = []\n",
    "\n",
    "# get the frequency of each rank at each freq level\n",
    "for lvl in raw_data['freq_lvl']:\n",
    "    for i in range(0, maxC):\n",
    "        raw_data['rank: ' + str(i)].append(len([1 for posRel in posRels \n",
    "                                       if posRel[0] == i and posRel[1] > lvl]))\n",
    "        \n",
    "df = pd.DataFrame(raw_data, columns = ['first_name']\n",
    "                  .extend(['rank: ' + str(i) for i in range(0, maxC)]))\n",
    "with open('/users/cs/amaral/wikisim/storage/correct-pos-frequencies.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.suptitle('Relevancy Score for Candidates at given rank', fontsize=32)\n",
    "plt.xlabel('Candidate Popularity Rank (zero based)', fontsize=18)\n",
    "plt.ylabel('Relevancy Score (from containing sentence)', fontsize=18)\n",
    "plt.plot(incorrectPositions, incorrectRelevancies, 'ro')\n",
    "plt.plot(correctPositions, correctRelevancies, 'bo')\n",
    "plt.legend(handles=[mpatches.Patch(color='red', label='Incorrect'), mpatches.Patch(color='blue', label='Correct')])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This is for seeing how often correct entity shows up in candidates\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def generateCandidates1(textData, maxC, oText):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    #print textData['text']\n",
    "    for mention in textData['mentions']:\n",
    "        \n",
    "        # get all concepts for the anchor\n",
    "        concepts = anchor2concept(textData['text'][mention[0]])\n",
    "        \n",
    "        # get the ids as string for solr query\n",
    "        strIds = ['id:' +  str(strId[0]) for strId in concepts]\n",
    "        \n",
    "        context = getMentionSentence(oText, mention)\n",
    "        context = escapeStringSolr(context)\n",
    "        mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "        \n",
    "        # gets the relevancy scores of all of the given potential concepts\n",
    "        addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "        params={'fl':'id score', 'indent':'on', 'start': '0', 'rows': str(maxC),\n",
    "                'fq':\" \".join(strIds),\n",
    "                'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "                'wt':'json'}\n",
    "        r = requests.get(addr, params = params)\n",
    "        \n",
    "        solrRes = []\n",
    "        try:\n",
    "            if not ('response' not in r.json()\n",
    "                   or 'docs' not in r.json()['response']\n",
    "                   or len(r.json()['response']['docs']) == 0):\n",
    "                for doc in r.json()['response']['docs'][:maxC]:\n",
    "                    freq = 0\n",
    "                    for concept in concepts:\n",
    "                        # find concept frequency\n",
    "                        if concept[0] == int(doc['id']):\n",
    "                            freq = concept[1]\n",
    "                    solrRes.append([long(doc['id']), freq, doc['score']])\n",
    "        except:\n",
    "            solrRes = []\n",
    "                \n",
    "        # sort by frequency\n",
    "        solrRes = sorted(solrRes, key = itemgetter(1), reverse = True)\n",
    "        \n",
    "        #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "        #for res in solrRes:\n",
    "        #    print '[' + id2title(res[0]) + '] -> freq: ' + str(res[1]) + ', rel: ' + str(res[2])\n",
    "        \n",
    "        candidates.append(solrRes) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates2(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates3(textData, maxC):\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        anchors = anchor2concept(textData['text'][mention[0]])\n",
    "        entities = []\n",
    "        \n",
    "        for anchor in anchors:\n",
    "            wanchors = id2anchor(anchor[0]) # get all anchors of the id in this anchor\n",
    "            totalFreq = 0\n",
    "            for wanchor in wanchors:\n",
    "                totalFreq += wanchor[1]\n",
    "            \n",
    "            entities.append([anchor[0], totalFreq])\n",
    "        \n",
    "        results = sorted(entities, key = itemgetter(1), reverse = True)\n",
    "        \n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def generateCandidates5(textData, maxC):\n",
    "    \n",
    "    candidates = []\n",
    "    #print textData['text']\n",
    "    for mention in textData['mentions']:\n",
    "        \n",
    "        # get all concepts for the anchor\n",
    "        concepts = anchor2concept(textData['text'][mention[0]])\n",
    "        \n",
    "        # get the ids as string for solr query\n",
    "        strIds = ['id:' +  str(strId[0]) for strId in concepts]\n",
    "        \n",
    "        context = []\n",
    "        \n",
    "        for mention2 in textData['mentions']:\n",
    "            if mention2 <> mention:\n",
    "                context += escapeStringSolr(textData['text'][mention2[0]])\n",
    "        context = \" \".join(context)\n",
    "        mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "        \n",
    "        # gets the relevancy scores of all of the given potential concepts\n",
    "        addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "        params={'fl':'id score', 'indent':'on', 'start': '0', 'rows': str(maxC),\n",
    "                'fq':\" \".join(strIds),\n",
    "                'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "                'wt':'json'}\n",
    "        r = requests.get(addr, params = params)\n",
    "        \n",
    "        solrRes = []\n",
    "        try:\n",
    "            if not ('response' not in r.json()\n",
    "                   or 'docs' not in r.json()['response']\n",
    "                   or len(r.json()['response']['docs']) == 0):\n",
    "                for doc in r.json()['response']['docs'][:maxC]:\n",
    "                    freq = 0\n",
    "                    for concept in concepts:\n",
    "                        # find concept frequency\n",
    "                        if concept[0] == int(doc['id']):\n",
    "                            freq = concept[1]\n",
    "                    solrRes.append([long(doc['id']), freq, doc['score']])\n",
    "        except:\n",
    "            solrRes = []\n",
    "        \n",
    "        #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "        #for res in solrRes:\n",
    "        #    print '[' + id2title(res[0]) + '] -> freq: ' + str(res[1]) + ', rel: ' + str(res[2])\n",
    "        \n",
    "        candidates.append(solrRes) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "maxC = 20\n",
    "correctCands1 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands2 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands3 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands4 = {} # dictionary containing the amount of correct entities found at each index\n",
    "correctCands5 = {} # dictionary containing the amount of correct entities found at each index\n",
    "for i in range(-1, maxC):\n",
    "    correctCands1[str(i)] = 0\n",
    "    correctCands2[str(i)] = 0\n",
    "    correctCands3[str(i)] = 0\n",
    "    correctCands4[str(i)] = 0\n",
    "    correctCands5[str(i)] = 0\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    totalLines = 0\n",
    "    \n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        totalMentions += len(line['mentions'])\n",
    "        \n",
    "        oMentions = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True)\n",
    "        \n",
    "        # get in right format\n",
    "        line['mentions'] = mentionStartsAndEnds(line) # put mentions in right form\n",
    "        oText = \" \".join(line['text'])\n",
    "            \n",
    "        cands1 = generateCandidates1(line, maxC, oText)\n",
    "        cands2 = generateCandidates2(line, maxC)\n",
    "        cands3 = generateCandidates3(line, maxC)\n",
    "        cands4 = []\n",
    "        for cands in cands1:\n",
    "            cands4.append(sorted(cands, key = itemgetter(2), reverse = True))\n",
    "        cands5 = generateCandidates5(line, maxC)\n",
    "        \n",
    "        i = 0\n",
    "        for cand in cands1:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands1[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands1['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands2:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands2[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands2['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands3:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands3[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands3['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands4:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands4[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands4['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "        i = 0\n",
    "        for cand in cands5:\n",
    "            corCand = False\n",
    "            for j in range(0, len(cand)):\n",
    "                if cand[j][0] == oMentions[i][2]:\n",
    "                    correctCands5[str(j)] += 1\n",
    "                    corCand = True\n",
    "                    break\n",
    "            if corCand == False:\n",
    "                correctCands5['-1'] += 1\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "        totalLines += 1\n",
    "        \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candsPopRel = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPopRel.append(correctCands1[str(i)])\n",
    "    \n",
    "candsPop = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPop.append(correctCands2[str(i)])\n",
    "    \n",
    "candsPopPop = []\n",
    "for i in range(-1,maxC):\n",
    "    candsPopPop.append(correctCands3[str(i)])\n",
    "    \n",
    "candsRelSentence = []\n",
    "for i in range(-1,maxC):\n",
    "    candsRelSentence.append(correctCands4[str(i)])\n",
    "    \n",
    "candsRelMentions = []\n",
    "for i in range(-1,maxC):\n",
    "    candsRelMentions.append(correctCands5[str(i)])\n",
    "    \n",
    "x = range(-1, maxC)\n",
    "\n",
    "print 'Total Mentions: ' + str(totalMentions)\n",
    "\n",
    "\n",
    "plt.bar(x, candsPopRel, 0.5, color='red')\n",
    "plt.xlabel('Most Popular of most Relevant Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands1[str(-1)])\n",
    "print str(candsPopRel) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsPop, 0.5, color='orange')\n",
    "plt.xlabel('Popularity v1 Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands2[str(-1)])\n",
    "print str(candsPop) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsPopPop, 0.5, color='green')\n",
    "plt.xlabel('Popularity v2 Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands3[str(-1)])\n",
    "print str(candsPopPop) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsRelSentence, 0.5, color='blue')\n",
    "plt.xlabel('Sentence Relevancy Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands4[str(-1)])\n",
    "print str(candsRelSentence) + '\\n\\n'\n",
    "\n",
    "\n",
    "plt.bar(x, candsRelMentions, 0.5, color='purple')\n",
    "plt.xlabel('Mentions Relevancy Rank')\n",
    "plt.ylabel('Correct Entities')\n",
    "plt.title('Correct Entities at given Rank')\n",
    "plt.show()\n",
    "print 'Misplaced: ' + str(correctCands5[str(-1)])\n",
    "print str(candsRelMentions) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concepts = anchor2concept('Tiger')\n",
    "concepts = sorted(concepts, key = itemgetter(1), reverse = True)[:50]\n",
    "for concept in concepts:\n",
    "    print 'id:' + str(concept[0]) + ' ' + id2title(concept[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRP$': 1, 'VBG': 26, 'VBD': 17, 'VBN': 60, 'VBP': 7, 'WDT': 1, 'JJ': 1084, 'VBZ': 17, 'DT': 2, '$': 1, 'NN': 1946, 'FW': 2, 'POS': 1, '.': 2, 'PRP': 2, 'RB': 15, 'NNS': 569, 'NNP': 22726, 'VB': 31, 'CC': 1, 'CD': 347, 'IN': 4, 'MD': 2, 'NNPS': 212, 'JJS': 11, 'JJR': 2, 'SYM': 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test some of the POS stuff\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "totalMentions = 0\n",
    "\n",
    "posDict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    print str(datetime.now()) + '\\n'\n",
    "    \n",
    "    totalLines = 0\n",
    "    for line in dataLines: \n",
    "        print str(totalLines + 1)\n",
    "        totalMentions += len(line['mentions'])\n",
    "        pos = nltk.pos_tag(line['text'])\n",
    "        for mnt in line['mentions']:\n",
    "            if str(pos[mnt[0]][1]) not in posDict:\n",
    "                posDict[str(pos[mnt[0]][1])] = 1\n",
    "            else:\n",
    "                posDict[str(pos[mnt[0]][1])] += 1\n",
    "                \n",
    "        totalLines += 1\n",
    "        \n",
    "clear_output()\n",
    "print posDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
