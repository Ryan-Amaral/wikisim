{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def mentionStartsAndEnds(phraseData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phraseData object and appends it's mentions with the start and end\n",
    "        index of each mention in the original string.\n",
    "    Args:\n",
    "        phraseData: [['words','split','like','this'],[[wordId,entityId,frequency,start,end],...]]\n",
    "    Return:\n",
    "        The same phraseData but with each mention containing the start and end of that\n",
    "        mention in the source text\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in phraseData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(phraseData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mention.append(0) # frequency placeholder\n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(phraseData['text'][curWord])) # end of the mention\n",
    "\n",
    "    return phraseData\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase)\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitText = []\n",
    "    mentions = []\n",
    "    \n",
    "    i = 0\n",
    "    for datum in textData:\n",
    "        splitText.append(phrase[datum[1]:datum[3]])\n",
    "        mentions.append([i, '0', 0, datum[1], datum[3]])\n",
    "        i += 1\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(phrase, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in phrase.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in phrase['mentions']:\n",
    "        results = sorted(anchor2concept(phrase['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    # find all correct\n",
    "    for entity1 in mySet:\n",
    "        for entity2 in truthSet:\n",
    "            if entity1[1] == title2id(entity2[1]):\n",
    "                numCorrect += 1\n",
    "                break\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingSentences(phrase, axis):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding sentences.\n",
    "    Return:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones: [[w3,w4,w5],[w0,w1,w2,w6,w7,w8]]\n",
    "    \"\"\"\n",
    "    \n",
    "    frstSentenceStart = 0\n",
    "    # end of first sentence is just start of middle sentence\n",
    "    mdlSentenceStart = 0\n",
    "    mdlSentenceEnd = 0\n",
    "    # start of last sentence is just end of middle sentence\n",
    "    lstSentenceEnd = 0\n",
    "    \n",
    "    # get start index of middle sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(axis,-1,-1):\n",
    "        if phrase[i][-1] == '.':\n",
    "            mdlSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of middle sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(axis, len(phrase)):\n",
    "        if phrase[i][-1] == '.':\n",
    "            mdlSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            mdlSentenceEnd = len(phrase)\n",
    "            \n",
    "    # get start index of first sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(mdlSentenceStart - 2, -1, -1):\n",
    "        if phrase[i][-1] == '.':\n",
    "            frstSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of last sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(mdlSentenceEnd + 1, len(phrase)):\n",
    "        if phrase[i][-1] == '.':\n",
    "            lstSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            lstSentenceEnd = len(phrase)\n",
    "            \n",
    "    sentences = [phrase[mdlSentenceStart:axis]+phrase[axis+1:mdlSentenceEnd],\n",
    "                phrase[frstSentenceStart:mdlSentenceStart]+phrase[mdlSentenceEnd:lstSentenceEnd]]\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "def getSurroundingWords(phrase, axis, branchSize):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding words.\n",
    "        branchSize: The amount of words to the left and right to get.\n",
    "    Return:\n",
    "        The words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = axis - branchSize\n",
    "    imax = axis + branchSize\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(phrase):\n",
    "        imax = len(phrase)\n",
    "        \n",
    "    # return surrounding part of word minus the axis word\n",
    "    return (phrase[imin:axis] + phrase[axis+1:imax])\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestMultiContextMatch(mention, context, contextSurround, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words in the sentence of the target.\n",
    "        contextSurround: The words in the sentences that surround the target.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put texts in right format\n",
    "    text = (\" \".join(context)).encode('utf-8')\n",
    "    textSurround = (\" \".join(contextSurround)).encode('utf-8')\n",
    "    text = escapeStringSolr(text)\n",
    "    textSurround = escapeStringSolr(textSurround)\n",
    "    mention = escapeStringSolr(mention.encode('utf-8'))\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    if len(contextSurround) > 0:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text.decode('string_escape')+') text:('+textSurround.decode('string_escape')+')^0.4 title:('+mention.decode('string_escape')+')^0.7',\n",
    "                'wt':'json'}\n",
    "    else:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text.decode('string_escape')+') title:('+mention.decode('string_escape')+')^0.7',\n",
    "                'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestContextMatch(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that suround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    text = (\" \".join(context)).encode('utf-8')\n",
    "    text = escapeStringSolr(text)\n",
    "    mention = escapeStringSolr(mention.encode('utf-8'))\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+text.decode('string_escape')+') title:(' + mention.decode('string_escape') + ')^0.6',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "    \n",
    "def wikifyPopular(phrase, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[0], candidates[i][0][0], candidates[i][0][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "# the orginal version, with just surrounding words.\n",
    "def wikifyContexty(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding contextBranchSize words.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            context = getSurroundingWords(phrase['text'], mention[0], ctxBrchSz)\n",
    "            bestIndex = bestContextMatch(phrase['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], mention[2], mention[3]])\n",
    "        else:\n",
    "            topCandidates.append([mention[0], 0, -1, -1]) # a bad mention\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "# new version with surrounding sentences\n",
    "def wikifyContext(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding sentences and its own\n",
    "        serving as context.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            contexts = getSurroundingSentences(phrase['text'], mention[0])\n",
    "            bestIndex = bestMultiContextMatch(phrase['text'][mention[0]], contexts[0], contexts[1], candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], candidates[i][bestIndex][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(phrase, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(mentionsGiven):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "    else:\n",
    "        phrase = mentionStartsAndEnds(phrase)\n",
    "    \n",
    "        \n",
    "    wikified = [phrase['text']] # second index with proposed entities filled later\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        phrase['mentions'] = [item for item in phrase['mentions']\n",
    "                    if  len(phrase['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(phrase, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(phrase, candidates))\n",
    "    elif method == 'context':\n",
    "        wikified.append(wikifyContext(phrase, candidates, ctxBrchSz = len(phrase['text'])))\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if item[2] >= MIN_FREQUENCY]\n",
    "    \n",
    "    \"\"\"# remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\"\"\"\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kore\n",
      "context\n",
      "1\n",
      "true[[0, u'David_Beckham', 0, 0, 5], [2, u'Victoria_Beckham', 0, 10, 18]]\n",
      "manual[[0, 8618L, 31L, 0, 5], [1, 29945L, 1L, 6, 9], [2, 136747L, 137L, 10, 18], [3, 2551704L, 4L, 19, 24], [4, 19332192L, 1L, 25, 39], [5, 632153L, 15L, 40, 48], [6, 25117753L, 3L, 51, 56], [7, 2499779L, 4L, 59, 63], [8, 29945L, 1L, 66, 69], [9, 9807977L, 2L, 70, 76], [10, 494657L, 83L, 77, 82]]\n",
      "split[[0, 8618L, 31L, 0, 5], [2, 136747L, 137L, 10, 18]]\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 11\n",
      "0.5 0.0909090909091 0.5 0.5\n",
      "\n",
      "2\n",
      "true[[0, u'David_Beckham', 0, 0, 5], [2, u'Victoria_Beckham', 0, 10, 18]]\n",
      "manual[[0, 8618L, 31L, 0, 5], [1, 18152L, 19L, 6, 9], [2, 34379855L, 45L, 10, 18], [3, 428162L, 3L, 19, 24], [4, 6380293L, 1L, 25, 30], [5, 6851L, 1L, 31, 33], [6, 801376L, 1L, 34, 39], [7, 34352219L, 2L, 40, 48]]\n",
      "split[[0, 8618L, 31L, 0, 5], [2, 34379855L, 45L, 10, 18]]\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 8\n",
      "0.5 0.125 0.5 0.5\n",
      "\n",
      "3\n",
      "true[[0, u'Tiger_Woods', 0, 0, 5], [11, u'Elin_Nordegren', 0, 54, 58]]\n",
      "manual[[0, 2193794L, 6L, 0, 5], [1, 21833329L, 2L, 6, 14], [2, 37799182L, 2L, 15, 17], [3, 11090L, 1L, 18, 27], [4, 323927L, 1L, 28, 32], [5, 225073L, 9L, 33, 35], [6, 1429712L, 1L, 36, 39], [7, 40147L, 63L, 40, 48], [8, 201586L, 1L, 49, 53], [9, 952833L, 2L, 54, 58]]\n",
      "split[[0, 2193794L, 6L, 0, 5], [11, 952833L, 2L, 54, 58]]\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 10\n",
      "0.5 0.1 0.5 0.5\n",
      "\n",
      "4\n",
      "true[[0, u'Tiger_Woods', 0, 0, 5], [3, u'U.S._Open_(golf)', 0, 15, 22]]\n",
      "manual[[0, 30075L, 132L, 0, 5], [1, 4896030L, 3L, 6, 10], [2, 21383L, 1L, 11, 14], [3, 23741693L, 15L, 15, 22]]\n",
      "split[[0, 30075L, 132L, 0, 5], [3, 23741693L, 15L, 15, 22]]\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 4\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "5\n",
      "true[[0, u'Madonna_(entertainer)', 0, 0, 7]]\n",
      "manual[[0, 38297L, 6L, 0, 7], [1, 2188611L, 1L, 8, 14], [2, 51584L, 3L, 15, 18], [3, 8092L, 1L, 19, 22], [4, 385946L, 1L, 23, 31], [5, 1692404L, 1L, 32, 36], [6, 6518699L, 5L, 37, 43]]\n",
      "split[[0, 38297L, 6L, 0, 7]]\n",
      "correct: 0\n",
      "found: 1\n",
      "correct: 0\n",
      "found: 7\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "6\n",
      "true[[4, u'Madonna_(entertainer)', 0, 18, 25], [10, u'First_Lady_of_Argentina', 0, 49, 59]]\n",
      "manual[[0, 14749L, 12L, 0, 2], [1, 26247363L, 1L, 3, 7], [2, 3744292L, 2L, 8, 15], [3, 38297L, 6L, 18, 25], [4, 16845426L, 1L, 26, 32], [5, 171446L, 1L, 33, 36], [6, 170191L, 103L, 37, 41], [7, 37799182L, 1L, 42, 48], [8, 10847L, 396L, 49, 59]]\n",
      "split[[4, 38297L, 6L, 18, 25], [10, 10847L, 396L, 49, 59]]\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 9\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "7\n",
      "true[[0, u'Angelina_Jolie', 0, 0, 8], [4, u'Jon_Voight', 0, 22, 25], [9, u'Brad_Pitt', 0, 44, 48]]\n",
      "manual[[0, 5792809L, 2L, 0, 8], [1, 302324L, 1L, 11, 21], [2, 1191972L, 1L, 22, 25], [3, 22316162L, 2L, 28, 31], [4, 898632L, 1L, 32, 35], [5, 348347L, 4L, 36, 43], [6, 44849L, 1L, 44, 48], [7, 490101L, 1L, 49, 54], [8, 2188611L, 1L, 55, 61], [9, 42979383L, 1L, 62, 70], [10, 2958004L, 1L, 71, 73], [11, 6058079L, 1L, 74, 88]]\n",
      "split[[0, 5792809L, 2L, 0, 8], [4, 1191972L, 1L, 22, 25], [9, 44849L, 1L, 44, 48]]\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 12\n",
      "0.666666666667 0.166666666667 0.666666666667 0.666666666667\n",
      "\n",
      "8\n",
      "true[[0, u'Heidi_Klum', 0, 0, 5], [4, u'Seal_(musician)', 0, 22, 26], [7, u'Las_Vegas,_Nevada', 0, 35, 40]]\n",
      "manual[[0, 5765329L, 4L, 0, 5], [1, 8092L, 1L, 6, 9], [2, 85406L, 1L, 10, 21], [3, 38134L, 389L, 22, 26], [4, 28685L, 1L, 27, 34], [5, 47737L, 23L, 35, 40]]\n",
      "split[[0, 5765329L, 4L, 0, 5], [4, 38134L, 389L, 22, 26], [7, 47737L, 23L, 35, 40]]\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 6\n",
      "0.666666666667 0.333333333333 0.666666666667 0.666666666667\n",
      "\n",
      "9\n",
      "true[[0, u'Paris_Hilton', 0, 0, 5], [2, u'Kim_Kardashian', 0, 10, 13]]\n",
      "manual[[0, 11217925L, 14L, 0, 5], [1, 1280983L, 3L, 6, 9], [2, 659032L, 4L, 10, 13], [3, 5630L, 2L, 14, 17], [4, 14912L, 1L, 18, 22], [5, 4695282L, 1L, 23, 30], [6, 25733011L, 3L, 31, 39], [7, 1157398L, 4L, 40, 43], [8, 27546L, 2L, 44, 51], [9, 70912L, 6L, 52, 57], [10, 989858L, 1L, 58, 73]]\n",
      "split[[0, 11217925L, 14L, 0, 5], [2, 659032L, 4L, 10, 13]]\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 11\n",
      "0.5 0.0909090909091 0.5 0.5\n",
      "\n",
      "10\n",
      "true[[0, u'Justin_Bieber', 0, 0, 6], [2, u'Lady_gaga', 0, 9, 16], [5, u'Kate_Perry', 0, 23, 27], [14, u'MTV', 0, 70, 73], [16, u'Twitter', 0, 78, 85]]\n",
      "manual[[0, 135774L, 4L, 0, 6], [1, 167805L, 1L, 9, 16], [2, 17867L, 1L, 19, 22], [3, 942010L, 3L, 23, 27], [4, 5630L, 2L, 28, 31], [5, 342063L, 2L, 38, 41], [6, 3830968L, 1L, 42, 54], [7, 186932L, 2L, 55, 61], [8, 957080L, 1L, 62, 64], [9, 14912L, 1L, 65, 69], [10, 4377954L, 16L, 70, 73], [11, 17867L, 1L, 74, 77], [12, 9988187L, 8155L, 78, 85]]\n",
      "split[[0, 135774L, 4L, 0, 6], [2, 167805L, 1L, 9, 16], [5, 942010L, 3L, 23, 27], [14, 4377954L, 16L, 70, 73], [16, 23225658L, 1L, 78, 85]]\n",
      "correct: 0\n",
      "found: 5\n",
      "correct: 1\n",
      "found: 13\n",
      "0.0 0.0769230769231 0.0 0.2\n",
      "\n",
      "11\n",
      "true[[0, u'Bob_Dylan', 0, 0, 5], [2, u'Hurricane_(song)', 0, 16, 25], [7, u'Rubin_Carter', 0, 50, 56], [12, u'Desire_(Bob_Dylan_album)', 0, 74, 80]]\n",
      "manual[[0, 4637590L, 89L, 0, 5], [1, 1538038L, 1L, 6, 15], [2, 1750070L, 24L, 16, 25], [3, 18143904L, 3L, 26, 31], [4, 10396793L, 2L, 32, 35], [5, 4874L, 82L, 36, 41], [6, 379330L, 1L, 42, 49], [7, 15992L, 165L, 50, 56], [8, 201586L, 1L, 59, 63], [9, 312489L, 1L, 64, 67], [10, 2505234L, 2L, 68, 73], [11, 835983L, 39L, 74, 80]]\n",
      "split[[0, 4637590L, 89L, 0, 5], [2, 1750070L, 24L, 16, 25], [7, 15992L, 165L, 50, 56], [12, 835983L, 39L, 74, 80]]\n",
      "correct: 3\n",
      "found: 4\n",
      "correct: 3\n",
      "found: 12\n",
      "0.75 0.25 0.75 0.75\n",
      "\n",
      "12\n",
      "true[[0, u'Desire_(Bob_Dylan_album)', 0, 0, 6], [5, u'Emmylou_Harris', 0, 28, 34], [9, u'Joey_(Bob_Dylan_song)', 0, 47, 51]]\n",
      "manual[[0, 835983L, 39L, 0, 6], [1, 204420L, 1L, 7, 15], [2, 4442545L, 10L, 16, 17], [3, 19499622L, 1L, 18, 27], [4, 15264705L, 9L, 28, 34], [5, 43055328L, 6L, 35, 37], [6, 3054581L, 1L, 38, 46], [7, 23555357L, 6L, 47, 51]]\n",
      "split[[0, 835983L, 39L, 0, 6], [5, 15264705L, 9L, 28, 34], [9, 23555357L, 6L, 47, 51]]\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 8\n",
      "0.666666666667 0.25 0.666666666667 0.666666666667\n",
      "\n",
      "13\n",
      "true[[13, u'Eric_Clapton', 0, 73, 80], [15, u'Jeff_Beck', 0, 83, 87], [18, u'Jimmy_Page', 0, 94, 98]]\n",
      "manual[[0, 1769563L, 10L, 0, 5], [1, 149661L, 1L, 6, 12], [2, 663041L, 3L, 13, 21], [3, 12620L, 34L, 22, 32], [4, 234788L, 24L, 33, 40], [5, 801376L, 1L, 41, 46], [6, 47426323L, 1L, 47, 53], [7, 682629L, 1L, 54, 56], [8, 12517846L, 1L, 57, 65], [9, 159489L, 2L, 66, 70], [10, 10049L, 17L, 73, 80], [11, 105389L, 2L, 83, 87], [12, 1883448L, 2L, 90, 93], [13, 102096L, 5L, 94, 98]]\n",
      "split[[13, 10049L, 17L, 73, 80], [15, 105389L, 2L, 83, 87], [18, 102096L, 5L, 94, 98]]\n",
      "correct: 3\n",
      "found: 3\n",
      "correct: 3\n",
      "found: 14\n",
      "1.0 0.214285714286 1.0 1.0\n",
      "\n",
      "14\n",
      "true[[0, u'Paul_Allen', 0, 0, 5], [3, u'EMP_Museum', 0, 18, 21], [5, u'Seattle', 0, 25, 32], [11, u'Jimi_Hendrix', 0, 68, 75], [13, u'Bob_Dylan', 0, 80, 85]]\n",
      "manual[[0, 144196L, 4L, 0, 5], [1, 40189L, 5L, 6, 13], [2, 342063L, 2L, 14, 17], [3, 102346L, 4L, 18, 21], [4, 14775L, 93L, 22, 24], [5, 101490L, 4L, 25, 32], [6, 4656421L, 1L, 35, 40], [7, 83491L, 1L, 41, 49], [8, 3447769L, 1L, 50, 61], [9, 18143904L, 3L, 62, 67], [10, 2396428L, 2L, 68, 75], [11, 8092L, 1L, 76, 79], [12, 1202347L, 1L, 80, 85], [13, 1408926L, 1L, 88, 91], [14, 16161L, 1L, 92, 96], [15, 18143904L, 3L, 97, 102], [16, 13652176L, 1L, 103, 110], [17, 179757L, 5L, 111, 133]]\n",
      "split[[0, 144196L, 4L, 0, 5], [3, 102346L, 4L, 18, 21], [5, 101490L, 4L, 25, 32], [11, 2396428L, 2L, 68, 75], [13, 1202347L, 1L, 80, 85]]\n",
      "correct: 1\n",
      "found: 5\n",
      "correct: 1\n",
      "found: 18\n",
      "0.2 0.0555555555556 0.2 0.2\n",
      "\n",
      "15\n",
      "true[[13, u'Frank_Sinatra', 0, 82, 89], [15, u'Bob_Dylan', 0, 92, 97], [17, u'Billy_Joel', 0, 100, 104], [20, u'Carlos_Santana', 0, 111, 118], [22, u'Columbia_Records', 0, 123, 131], [26, u'Sony_Music_Entertainment', 0, 147, 151]]\n",
      "manual[[0, 24754903L, 2L, 0, 7], [1, 15104305L, 16L, 8, 17], [2, 5042916L, 1L, 18, 22], [3, 13440359L, 1L, 23, 25], [4, 710325L, 1L, 26, 34], [5, 147778L, 2L, 47, 56], [6, 13440359L, 1L, 57, 59], [7, 28187L, 2L, 60, 65], [8, 34579L, 1L, 66, 72], [9, 567140L, 10L, 77, 81], [10, 11181L, 64L, 82, 89], [11, 4637590L, 89L, 92, 97], [12, 2620298L, 2L, 100, 104], [13, 8092L, 1L, 107, 110], [14, 3058493L, 3L, 111, 118], [15, 165108L, 1142L, 123, 131], [16, 5630L, 1L, 132, 135], [17, 53509L, 1L, 144, 146], [18, 165108L, 2L, 147, 151], [19, 19753121L, 2L, 152, 164]]\n",
      "split[[13, 11181L, 64L, 82, 89], [15, 4637590L, 89L, 92, 97], [17, 2620298L, 2L, 100, 104], [20, 3058493L, 3L, 111, 118], [22, 165108L, 1142L, 123, 131], [26, 165108L, 2L, 147, 151]]\n",
      "correct: 4\n",
      "found: 6\n",
      "correct: 4\n",
      "found: 20\n",
      "0.666666666667 0.2 0.666666666667 0.666666666667\n",
      "\n",
      "16\n",
      "true[[7, u'Johnny_Cash', 0, 46, 50], [14, u'American_Recordings_(album)', 0, 86, 105], [24, u'Rick_Rubin', 0, 146, 151]]\n",
      "manual[[0, 35905920L, 8L, 0, 5], [1, 1369907L, 1L, 6, 18], [2, 26197L, 1L, 19, 24], [3, 229060L, 1L, 27, 32], [4, 5247L, 3453L, 33, 40], [5, 26808L, 4134L, 41, 45], [6, 11983070L, 3L, 46, 50], [7, 4221444L, 29L, 51, 55], [8, 5407L, 1L, 56, 57], [9, 14315293L, 5L, 58, 67], [10, 518843L, 1L, 68, 76], [11, 217537L, 1L, 77, 81], [12, 3807L, 1L, 82, 85], [13, 2281002L, 22L, 86, 105], [14, 4843981L, 2L, 108, 116], [15, 26964606L, 1L, 117, 119], [16, 186419L, 1L, 120, 128], [17, 217537L, 1L, 129, 133], [18, 2717447L, 1L, 134, 137], [19, 1224851L, 2L, 138, 142], [20, 2814062L, 1L, 143, 145], [21, 399397L, 1L, 146, 151]]\n",
      "split[[7, 11983070L, 3L, 46, 50], [14, 2281002L, 22L, 86, 105], [24, 399397L, 1L, 146, 151]]\n",
      "correct: 3\n",
      "found: 3\n",
      "correct: 3\n",
      "found: 22\n",
      "1.0 0.136363636364 1.0 1.0\n",
      "\n",
      "17\n",
      "true[[4, u'Josh_Homme', 0, 20, 25], [6, u'Dave_Grohl', 0, 28, 33], [9, u'John_Paul_Jones_(musician)', 0, 40, 45]]\n",
      "manual[[0, 1569843L, 1L, 0, 9], [1, 36729062L, 2L, 10, 16], [2, 345807L, 1L, 17, 19], [3, 31447770L, 2L, 20, 25], [4, 8099L, 2L, 28, 33], [5, 29945L, 1L, 36, 39], [6, 1227628L, 3L, 40, 45], [7, 13446L, 1L, 46, 49], [8, 258949L, 1L, 50, 58], [9, 5630L, 6L, 59, 64], [10, 38493L, 9L, 65, 70], [11, 6852L, 539L, 71, 79], [12, 889926L, 1L, 82, 85], [13, 3805890L, 1L, 86, 94], [14, 6202821L, 1L, 95, 112]]\n",
      "split[[4, 31447770L, 2L, 20, 25], [6, 8099L, 2L, 28, 33], [9, 1227628L, 3L, 40, 45]]\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "found: 15\n",
      "0.333333333333 0.0666666666667 0.333333333333 0.333333333333\n",
      "\n",
      "18\n",
      "true[[0, u'Steve_Jobs', 0, 0, 4], [2, u'Joan_Baez', 0, 9, 13], [14, u'Stanford_University', 0, 65, 73]]\n",
      "manual[[0, 7412236L, 3L, 0, 4], [1, 40800679L, 6L, 5, 8], [2, 50960L, 2L, 9, 13], [3, 19016242L, 1L, 14, 19], [4, 3416065L, 1L, 20, 22], [5, 269040L, 1L, 23, 37], [6, 40800679L, 6L, 40, 43], [7, 7785570L, 9L, 44, 47], [8, 1538038L, 1L, 48, 57], [9, 216153L, 1L, 58, 60], [10, 12787L, 1L, 61, 64], [11, 26977L, 1302L, 65, 73], [12, 52369L, 1L, 74, 82]]\n",
      "split[[0, 7412236L, 3L, 0, 4], [2, 50960L, 2L, 9, 13], [14, 26977L, 1302L, 65, 73]]\n",
      "correct: 3\n",
      "found: 3\n",
      "correct: 3\n",
      "found: 13\n",
      "1.0 0.230769230769 1.0 1.0\n",
      "\n",
      "19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa8a6eb271b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mresultS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikifyEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmthd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# unsplit string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mresultM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikifyEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmthd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m#resultM = [[],[]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-319e7368020b>\u001b[0m in \u001b[0;36mwikifyEval\u001b[0;34m(phrase, mentionsGiven, maxC, method, strict)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mwikified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikifyPopular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'context'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mwikified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikifyContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxBrchSz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;31m# get rid of very unpopular mentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-319e7368020b>\u001b[0m in \u001b[0;36mwikifyContext\u001b[0;34m(phrase, candidates, ctxBrchSz)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSurroundingSentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mbestIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestMultiContextMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m             \u001b[0mtopCandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# move to list of candidates for next mention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-319e7368020b>\u001b[0m in \u001b[0;36mbestMultiContextMatch\u001b[0;34m(mention, context, contextSurround, candidates)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'response'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# default to most popular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_')])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['context','popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name']\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "                \n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'].encode('utf-8').strip(), False, method = mthd)\n",
    "            else:\n",
    "                trueEntities = line['mentions'] # the ground truth\n",
    "                \n",
    "                # original split string\n",
    "                resultS = wikifyEval(line, True, method = mthd)\n",
    "                # unsplit string\n",
    "                resultM = wikifyEval((\" \".join(line['text'])).encode('utf-8').strip(), False, method = mthd)\n",
    "                \n",
    "            #resultM = [[],[]]\n",
    "            \n",
    "            print 'true' + str(trueEntities)\n",
    "            print 'manual' + str(resultM[1])\n",
    "            print 'split' + str(resultS[1])\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'Pre-Split Precision':totalPrecS/totalLines, \n",
    "                                               'Manual Split Precision':totalPrecM/totalLines,\n",
    "                                              'Pre-Split Recall':totalRecS/totalLines, \n",
    "                                               'Manual Split Recall':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, method='popular')\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -In wikifyContext make the current sentence worth 1 and each surrounding sentence worth 0.5.\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = sorted(anchor2concept(\"David Edgar\"), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "\n",
    "for tmpp in tmp:\n",
    "    print 'id: ' + str(tmpp[0]) + ', title: ' + id2title(tmpp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = splitWords('I walked down to the park and found a duck studying quantum mechanical systems')\n",
    "print split\n",
    "cands = generateCandidates(split, 2)\n",
    "\n",
    "print cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(33509L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \" \".join([\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"])\n",
    "print text\n",
    "\n",
    "text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "text = text.replace('+', r'\\+')\n",
    "text = text.replace(\"-\", \"\\-\")\n",
    "text = text.replace(\"&&\", \"\\&&\")\n",
    "text = text.replace(\"||\", \"\\||\")\n",
    "text = text.replace(\"!\", \"\\!\")\n",
    "text = text.replace(\"(\", \"\\(\")\n",
    "text = text.replace(\")\", \"\\)\")\n",
    "text = text.replace(\"{\", \"\\{\")\n",
    "text = text.replace(\"}\", \"\\}\")\n",
    "text = text.replace(\"[\", \"\\[\")\n",
    "text = text.replace(\"]\", \"\\]\")\n",
    "text = text.replace(\"^\", \"\\^\")\n",
    "text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "text = text.replace(\"~\", \"\\~\")\n",
    "text = text.replace(\"*\", \"\\*\")\n",
    "text = text.replace(\"?\", \"\\?\")\n",
    "text = text.replace(\":\", \"\\:\")\n",
    "\n",
    "text = text.decode('string_escape')\n",
    "\n",
    "print text + '\\n\\n'\n",
    "\n",
    "addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "params={'fl':'title id score', 'fq':'id:8551 id:8618', 'indent':'on', 'q':'\\text:('+text.decode('string_escape')+')', 'wt':'json'}\n",
    "r = requests.get(addr, params = params)\n",
    "textData = r.json()\n",
    "\n",
    "print textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phraseData = {\"text\": [\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"], \"mentions\": [[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]}\n",
    "print str(phraseData) + '\\n'\n",
    "phraseData = mentionStartsAndEnds(phraseData)\n",
    "print phraseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
