{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "correct: 2\n",
      "found: 8\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.25 1.0\n",
      "\n",
      "2\n",
      "correct: 2\n",
      "found: 4\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.5 1.0\n",
      "\n",
      "3\n",
      "correct: 2\n",
      "found: 6\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.333333333333 1.0\n",
      "\n",
      "4\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.666666666667 1.0\n",
      "\n",
      "5\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "actual: 1\n",
      "0.333333333333 1.0\n",
      "\n",
      "6\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.666666666667 1.0\n",
      "\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d339c8c7c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mtrueMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTextMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m#solrMentions = getSolrMentions(\" \".join(line['text']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0msolrMentions0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0msolrMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/tagme/__init__.pyc\u001b[0m in \u001b[0;36mmentions\u001b[0;34m(text, gcube_token, lang, api)\u001b[0m\n\u001b[1;32m    213\u001b[0m     payload = [(\"text\", text.encode(\"utf-8\")),\n\u001b[1;32m    214\u001b[0m                (\"lang\", lang.encode(\"utf-8\"))]\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_issue_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcube_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMentionsResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_response\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjson_response\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/tagme/__init__.pyc\u001b[0m in \u001b[0;36m_issue_request\u001b[0;34m(api, payload, gcube_token)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gcube-token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcube_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calling %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tagme returned status code %d message:\\n%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    421\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    592\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/OpenSSL/SSL.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, bufsiz, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsiz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsiz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end, text],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in line['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(line['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mentions.append([curStart, curStart + len(line['text'][curWord]), line['text'][curWord]])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def destroyOverlaps(text, textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        text: The source text.\n",
    "        textData: [[_, start, _, end, _, alts, pop],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        endiestEnd = textData[i][3]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][1] < endiestEnd:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "                # update endiness if needed\n",
    "                if textData[j][3] > endiestEnd:\n",
    "                    endiestEnd = textData[j][3]\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(text[textData[i][1]:textData[i][3]])\n",
    "            totalAppearances = get_solr_count(text[textData[i][1]:textData[i][3]].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "                    \n",
    "\n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(text[item[1]:item[3]])\n",
    "        \n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [6][1] is index of type of word\n",
    "        \n",
    "    textData = destroyOverlaps(text, textData)\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.001\n",
    "    \n",
    "    for item in textData:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        \n",
    "        \"\"\"print 'item: ' + text[item[1]:item[3]]\n",
    "        if totalAppearances == 0:\n",
    "            print 'prob: ' + text[item[1]:item[3]] + ' --> 0'\n",
    "        else:\n",
    "            print 'prob: ' + text[item[1]:item[3]] + ' --> ' + str(totalMentions/totalAppearances)\"\"\"\n",
    "        \n",
    "        if (totalAppearances > 0 and\n",
    "                (totalMentions/totalAppearances) >= mentionPThrsh\n",
    "                and (item[6][1][0:2] == 'NN' or item[6][1] == 'JJ')) or True:\n",
    "            mentions.append([item[1], item[3], text[item[1]:item[3]]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if trueMentions[trueIndex] == otherMentions[otherIndex]:\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line)\n",
    "        solrMentions = getSolrMentions(\" \".join(line['text']))\n",
    "        \n",
    "        \"\"\"solrMentions0 = tagme.mentions(\" \".join(line['text']))\n",
    "        solrMentions = []\n",
    "        for item in solrMentions0.mentions:\n",
    "            solrMentions.append([item.begin, item.end, item.mention])\"\"\"\n",
    "        \n",
    "        ## get statistical results from true mentions and solr mentions\n",
    "        \n",
    "        aNumber = len(solrMentions)/len(trueMentions)\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970\n",
      "1041\n",
      "0.931796349664\n"
     ]
    }
   ],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionStartsAndEnds(phraseData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phraseData object and appends it's mentions with the start and end\n",
    "        index of each mention in the original string.\n",
    "    Args:\n",
    "        phraseData: [['words','split','like','this'],[[wordId,entityId,frequency,start,end],...]]\n",
    "    Return:\n",
    "        The same phraseData but with each mention containing the start and end of that\n",
    "        mention in the source text\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in phraseData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(phraseData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mention.append(0) # frequency placeholder\n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(phraseData['text'][curWord])) # end of the mention\n",
    "\n",
    "    return phraseData\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase.encode('utf-8'))\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitText = []\n",
    "    mentions = []\n",
    "    \n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(phrase[item[1]:item[3]])\n",
    "        \n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [6][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.005\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    i = 0\n",
    "    for item in textData:    \n",
    "        totalMentions = get_mention_count(phrase[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(phrase[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if (totalAppearances > 0 \n",
    "                and (totalMentions/totalAppearances) >= mentionPThrsh \n",
    "                and phrase[item[1]:item[3]] not in stopWords\n",
    "                and (item[6][1] == 'NNP' or item[6][1] == 'NNPS' or item[6][1] == 'NN')):\n",
    "            mentions.append([i, '0', 0, item[1], item[3]])\n",
    "            \n",
    "        splitText.append(phrase[item[1]:item[3]])\n",
    "        i += 1\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(phrase, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in phrase.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in phrase['mentions']:\n",
    "        results = sorted(anchor2concept(phrase['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][3] < truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][4] > truthSet[truthIndex][3]:\n",
    "                # overlap with mine behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][3] == truthSet[truthIndex][3]:\n",
    "            # same mention (same start atleast)\n",
    "            if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][3] > truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][3] < truthSet[truthIndex][4]:\n",
    "                # overlap with truth behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][3] < truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][4] > truthSet[truthIndex][3]:\n",
    "                # overlap with mine behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][3] == truthSet[truthIndex][3]:\n",
    "            # same mention (same start atleast)\n",
    "            if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][3] > truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][3] < truthSet[truthIndex][4]:\n",
    "                # overlap with truth behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSentenceOfMention():\n",
    "    pass\n",
    "    \n",
    "def getSurroundingSentences(phrase, axis):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding sentences.\n",
    "    Return:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones: [[w3,w4,w5],[w0,w1,w2,w6,w7,w8]]\n",
    "    \"\"\"\n",
    "    \n",
    "    frstSentenceStart = 0\n",
    "    # end of first sentence is just start of middle sentence\n",
    "    mdlSentenceStart = 0\n",
    "    mdlSentenceEnd = 0\n",
    "    # start of last sentence is just end of middle sentence\n",
    "    lstSentenceEnd = 0\n",
    "    \n",
    "    # get start index of middle sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(axis,-1,-1):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            mdlSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of middle sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(axis, len(phrase)):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            mdlSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            mdlSentenceEnd = len(phrase)\n",
    "            \n",
    "    # get start index of first sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(mdlSentenceStart - 2, -1, -1):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            frstSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of last sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(mdlSentenceEnd + 1, len(phrase)):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            lstSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            lstSentenceEnd = len(phrase)\n",
    "            \n",
    "    sentences = [phrase[mdlSentenceStart:axis]+phrase[axis+1:mdlSentenceEnd],\n",
    "                phrase[frstSentenceStart:mdlSentenceStart]+phrase[mdlSentenceEnd:lstSentenceEnd]]\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "def getSurroundingWords(phrase, axis, branchSize):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding words.\n",
    "        branchSize: The amount of words to the left and right to get.\n",
    "    Return:\n",
    "        The words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = axis - branchSize\n",
    "    imax = axis + branchSize\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(phrase):\n",
    "        imax = len(phrase)\n",
    "        \n",
    "    # return surrounding part of word minus the axis word\n",
    "    return (phrase[imin:axis] + phrase[axis+1:imax])\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestMultiContextMatch(mention, context, contextSurround, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words in the sentence of the target.\n",
    "        contextSurround: The words in the sentences that surround the target.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put texts in right format\n",
    "    text = \" \".join(context)\n",
    "    textSurround = \" \".join(contextSurround)\n",
    "    text = escapeStringSolr(text)\n",
    "    textSurround = escapeStringSolr(textSurround)\n",
    "    mention = escapeStringSolr(mention)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    if len(contextSurround) > 0:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text.decode('string_escape')+')^1 text:('+textSurround.decode('string_escape')+')^0 title:('+mention.decode('string_escape')+')^1.35',\n",
    "                'wt':'json'}\n",
    "    else:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text+') title:('+mention+')^1.35',\n",
    "                'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestContextMatch(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that suround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    text = (\" \".join(context)).encode('utf-8')\n",
    "    text = escapeStringSolr(text)\n",
    "    mention = escapeStringSolr(mention.encode('utf-8'))\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+text.decode('string_escape')+') title:(' + mention.decode('string_escape') + ')^0.6',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "    \n",
    "def wikifyPopular(phrase, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[0], candidates[i][0][0], candidates[i][0][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "# the orginal version, with just surrounding words.\n",
    "def wikifyContexty(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding contextBranchSize words.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            context = getSurroundingWords(phrase['text'], mention[0], ctxBrchSz)\n",
    "            bestIndex = bestContextMatch(phrase['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], mention[2], mention[3]])\n",
    "        else:\n",
    "            topCandidates.append([mention[0], 0, -1, -1]) # a bad mention\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "# new version with surrounding sentences\n",
    "def wikifyContext(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding sentences and its own\n",
    "        serving as context.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            contexts = getSurroundingSentences(phrase['text'], mention[0])\n",
    "            bestIndex = bestMultiContextMatch(phrase['text'][mention[0]], contexts[0], contexts[1], candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], candidates[i][bestIndex][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(phrase, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(mentionsGiven):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "    else:\n",
    "        phrase = mentionStartsAndEnds(phrase)\n",
    "    \n",
    "        \n",
    "    wikified = [phrase['text']] # second index with proposed entities filled later\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        phrase['mentions'] = [item for item in phrase['mentions']\n",
    "                    if  len(phrase['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(phrase, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(phrase, candidates))\n",
    "    elif method == 'context':\n",
    "        wikified.append(wikifyContext(phrase, candidates, ctxBrchSz = len(phrase['text'])))\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if item[2] >= MIN_FREQUENCY]\n",
    "    \n",
    "    \"\"\"# remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\"\"\"\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_'), 0, item['from'], item['to']])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def wikilineLine(inLine):\n",
    "    \"\"\"\n",
    "    Puts the inLine in the right format if it came from wikipedia.\n",
    "    \"\"\"\n",
    "    newLine = {'text':[], 'mentions':[]}\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['context','popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "\n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'], False, method = mthd, maxC = 7)\n",
    "            else:\n",
    "                trueEntities = mentionStartsAndEnds(line)['mentions'] # the ground truth\n",
    "                \n",
    "                # original split string\n",
    "                resultS = wikifyEval(line, True, method = mthd, maxC = 7)\n",
    "                # unsplit string\n",
    "                resultM = wikifyEval(\" \".join(line['text']), False, method = mthd, maxC = 7)\n",
    "                \n",
    "            #resultM = [[],[]]\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'Pre-Split Precision':totalPrecS/totalLines, \n",
    "                                               'Manual Split Precision':totalPrecM/totalLines,\n",
    "                                              'Pre-Split Recall':totalRecS/totalLines, \n",
    "                                               'Manual Split Recall':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, method='popular')\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -In wikifyContext make the current sentence worth 1 and each surrounding sentence worth 0.5.\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = sorted(anchor2concept(\"David Edgar\"), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "\n",
    "for tmpp in tmp:\n",
    "    print 'id: ' + str(tmpp[0]) + ', title: ' + id2title(tmpp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase = {u'text': [u'Voller', u'presidential', u'preferences', u'How', u'will', u'American', u'voters', u'compensate', u'in', u'the', u'next', u'search', u'for', u'a', u'president?', u'WASHINGTON', u'-', u'Now', u'that', u'the', u'38th', u'president', u'has', u'been', u'laid', u'to', u'rest,', u'the', u'capital', u'can', u'take', u'up', u'the', u'main', u'business', u'of', u'2007:', u'trying', u'to', u'figure', u'out', u'who', u'will', u'be', u'the', u'44th.', u'What', u'type', u'of', u'leader', u'does', u'the', u'country', u'want?', u'Here', u'is', u'my', u'sense', u'of', u'it,', u'based', u'on', u'talking', u'to', u'politicians,', u'strategists', u'and', u'voters', u'here', u'and', u'around', u'the', u'nation.', u'No', u'ideologues,', u'please', u'There', u'was', u'a', u'time', u'when', u'President George W. Bush', u\"'s\", u'ideological', u'certitude', u'was', u'politically', u'appealing', u'and', u'perhaps', u'functionally', u'necessary.', u'That', u'time', u'has', u'long', u'since', u'passed.', u'The', u'country', u'is', u'tired,', u'even', u'fearful,', u'of', u'leaders', u'with', u'fervent', u'beliefs', u'that', u'seem', u'impervious', u'to', u'new', u'(or', u'even', u'old)', u'facts.', u'Voters', u'see', u'the', u'war', u'in', u'Iraq', u'as', u'an', u'\"idea,\"', u'not', u'a', u'solution', u'-', u'and', u'Americans', u'do', u'not', u'like', u'ideas', u'that', u'do', u'not', u'work.', u'Voters', u'likely', u'will', u'view', u'Bush', u\"'s\", u'\"surge\"', u'of', u'troops', u'into', u'Iraq', u'as', u'new', u'evidence', u'of', u'failure,', u'and', u'the', u'dangers', u'of', u'a', u'leader', u'who', u'depends', u'on', u'preconceived', u'ideas.', u'Serious', u'student', u'Presidential', u'elections', u'are', u'a', u'never-ending', u'series', u'of', u'mid-course', u'corrections.', u'Voters', u'look', u'to', u'compensate', u'for', u'the', u'leadership', u'weaknesses', u'of', u'the', u'incumbent.', u'An', u'example', u'comes', u'from', u'the', u'life', u'and', u'career', u'of', u'Gerald Ford', u'.', u'In', u'1976,', u'voters', u'wanted', u'a', u'pure', u'antidote', u'to', u'Richard Nixon', u\"'s\", u'paranoid', u'megalomania.', u'Once', u'Ford', u'pardoned', u'Nixon', u',', u'he', u'could', u'not', u'be', u'that', u'candidate.', u'Instead,', u'Americans', u'chose', u'Jimmy Carter', u',', u'a', u'peanut', u'farmer', u'who', u'had', u'never', u'worked', u'in', u'Washington', u',', u'and', u'who', u'promised', u'never', u'to', u'lie', u'to', u'the', u'American people', u'.', u'The', u'counterpoint', u'thinking', u'continues.', u'Voters', u'in', u'2008', u'are', u'going', u'to', u'want', u'someone', u'who', u'prides', u'himself', u'(or', u'herself)', u'on', u'spending', u'time', u'in', u'the', u'library', u'-', u'who', u'has', u'a', u'hands-on', u'curiosity', u'about', u'the', u'details.', u'Washington', u'experience', u'not', u'necessary', u'Voters', u'these', u'days', u'not', u'only', u'do', u'not', u'value', u'Washington', u'experience', u'-', u'or', u'any', u'office-holding', u'experience', u'-', u'it', u'can', u'make', u'them', u'suspicious.', u'That', u'is', u'what', u'strategists', u'and', u'polltakers', u'for', u'Sen.', u'Evan Bayh', u'found', u'when', u'they', u'studied', u'whether', u'he', u'should', u'run', u'for', u'president.', u'They', u'found', u'that', u'his', u'remarkably', u'deep', u'resume', u'-', u'the', u'son', u'of', u'a', u'senator,', u'he', u'was', u'the', u'\"boy', u'governor\"', u'of', u'Indiana', u'before', u'going', u'to', u'the', u'Senate', u'-', u'was', u'as', u'handicap.', u'Americans', u'always', u'are', u'dubious', u'about', u'the', u'capital,', u'but', u'that', u'sentiment', u'seems', u'particularly', u'strong.', u'Bayh', u'decided', u'not', u'to', u'run.', u'\"`', u'Washington', u\"'\", u\"doesn't\", u'make', u'the', u'case,\"', u'said', u'Dan Pfeiffer', u',', u'who', u'worked', u'for', u'Bayh', u'.', u'No', u'more', u'boomer', u'obsessions', u'Not', u'all', u'elections', u'are', u'about', u'change,', u'but', u'2008', u'will', u'be.', u'Americans', u'are', u'moderately', u'upbeat', u'about', u'the', u\"country's\", u'prospects,', u'but', u'deeply', u'worried', u'about', u'the', u'world', u'-', u'and', u'they', u'have', u'come', u'to', u'realize', u'that', u'they', u\"can't\", u'separate', u'one', u'from', u'the', u'other.', u'One', u'thing', u'for', u'sure,', u'says', u'Pfeiffer', u',', u'voters', u'are', u'tired', u'of', u'arguing', u'about', u'the', u'culture', u'of', u'the', u'1960s', u'and', u'other', u'Boomer', u'issues.', u'\"There', u'is', u'a', u'sense', u'that', u'the', u'2004', u'election', u'was', u'too', u'much', u'about', u'who', u'did', u'or', u'did', u'not', u'do', u'what', u'in', u'Vietnam', u',\"', u'said', u'Pfeiffer', u',', u'referring', u'to', u'the', u'Bush campaign', u'against', u'Sen.', u'John Kerry', u'.', u'In', u'2000,', u'Bush', u'won', u'in', u'part', u'by', u'selling', u'himself', u'as', u'a', u'\"grown', u'up\"', u'Boomer', u'answer', u'to', u'Bill Clinton', u'.', u'\"Voters', u'are', u'tired', u'of', u'that', u'era', u'and', u'its', u'concerns,\"', u'said', u'Pfeiffer', u'said.', u'\"They', u'want', u'to', u'move', u'on.\"', u'Know', u'the', u'middle', u'class', u'Bushes', u'have', u'a', u'congenital', u'family', u'problem', u'with', u'this,', u'and', u'it', u'leaves', u'an', u'opening', u'for', u'someone', u'-', u'of', u'either', u'party', u'-', u'who', u'can', u'prove', u'that', u'he', u'or', u'she', u'really', u'understands', u'the', u'strains', u'of', u'middle', u'class', u'life.', u\"It's\", u'not', u'just', u'about', u'money,', u'but', u'about', u'cultural', u'assaults', u'and', u'the', u'lack', u'of', u'time', u'for', u'family', u'in', u'an', u'era', u'when', u'both', u'parents', u'or', u'partners', u'need', u'to', u'work.', u'In', u'his', u'forthcoming', u'book,', u'Positively', u'American,', u'Sen.', u'Charles Schumer', u'of', u'New York', u'imagines', u'the', u'hard', u'life', u'of', u'a', u'fictitious', u'middle', u'class', u'family', u'-', u'and', u'offers', u'a', u'series', u'of', u'governmental', u'proposals', u'to', u'address', u'them.', u'A', u'shrewd', u'student', u'of', u'the', u'American', u'mood,', u'Schumer', u'is', u'aiming', u'in', u'the', u'right', u'direction.', u'The', u'next', u'president', u'will', u'need', u'to', u'show', u'that', u'he', u'or', u'she', u'understands', u'that', u'family.'], u'mentions': [[15, u'Washington,_D.C.', 0, 106, 116], [81, u'George_W._Bush', 0, 459, 483], [123, u'Iraq', 0, 743, 747], [145, u'George_W._Bush', 0, 853, 857], [151, u'Iraq', 0, 884, 888], [199, u'Gerald_Ford', 0, 1191, 1202], [209, u'Richard_Nixon', 0, 1247, 1260], [214, u'Gerald_Ford', 0, 1291, 1295], [216, u'Richard_Nixon', 0, 1305, 1310], [227, u'Jimmy_Carter', 0, 1370, 1382], [237, u'Washington,_D.C.', 0, 1425, 1435], [247, u'Demographics_of_the_United_States', 0, 1475, 1490], [281, u'Washington,_D.c.', 0, 1685, 1695], [293, u'Washington,_D.c.', 0, 1761, 1771], [314, u'Evan_Bayh', 0, 1898, 1907], [344, u'Indiana', 0, 2065, 2072], [349, u'United_States_Senate', 0, 2093, 2099], [367, u'Evan_Bayh', 0, 2213, 2217], [373, u'Washington,_D.C.', 0, 2241, 2251], [380, u'Dan_Pfeiffer', 0, 2283, 2295], [385, u'Evan_Bayh', 0, 2313, 2317], [435, u'Dan_Pfeiffer', 0, 2600, 2608], [450, u'Boomer', 0, 2680, 2686], [472, u'Vietnam_War', 0, 2785, 2792], [475, u'Dan_Pfeiffer', 0, 2801, 2809], [480, u'George_W._Bush_presidential_campaign,_2004', 0, 2829, 2842], [483, u'John_Kerry', 0, 2856, 2866], [487, u'George_W._Bush', 0, 2878, 2882], [501, u'Bill_Clinton', 0, 2947, 2959], [513, u'Dan_Pfeiffer', 0, 3016, 3024], [593, u'Charles_Schumer', 0, 3459, 3474], [595, u'New_York', 0, 3478, 3486], [624, u'Charles_Schumer', 0, 3650, 3657]]}\n",
    "wikified = [phrase['text']]\n",
    "cands = generateCandidates(phrase, 7)\n",
    "wikified.append(wikifyContext(phrase, cands, ctxBrchSz = len(phrase['text'])))\n",
    "\n",
    "for mention in wikified[1]:\n",
    "    mention[1] = id2title(mention[1])\n",
    "    \n",
    "print (\" \".join(wikified[0])).encode('utf-8').strip()\n",
    "print wikified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(33509L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text = \" \".join([\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"])\n",
    "print text\n",
    "\n",
    "text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "text = text.replace('+', r'\\+')\n",
    "text = text.replace(\"-\", \"\\-\")\n",
    "text = text.replace(\"&&\", \"\\&&\")\n",
    "text = text.replace(\"||\", \"\\||\")\n",
    "text = text.replace(\"!\", \"\\!\")\n",
    "text = text.replace(\"(\", \"\\(\")\n",
    "text = text.replace(\")\", \"\\)\")\n",
    "text = text.replace(\"{\", \"\\{\")\n",
    "text = text.replace(\"}\", \"\\}\")\n",
    "text = text.replace(\"[\", \"\\[\")\n",
    "text = text.replace(\"]\", \"\\]\")\n",
    "text = text.replace(\"^\", \"\\^\")\n",
    "text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "text = text.replace(\"~\", \"\\~\")\n",
    "text = text.replace(\"*\", \"\\*\")\n",
    "text = text.replace(\"?\", \"\\?\")\n",
    "text = text.replace(\":\", \"\\:\")\n",
    "\n",
    "text = text.decode('string_escape')\n",
    "\n",
    "print text + '\\n\\n'\n",
    "\n",
    "addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "r = requests.post(addr, params=params, data=text)\n",
    "textData = r.json()['tags']\n",
    "\n",
    "print textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phraseData = {\"text\": [\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"], \"mentions\": [[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]}\n",
    "print str(phraseData) + '\\n'\n",
    "phraseData = mentionStartsAndEnds(phraseData)\n",
    "print phraseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in sorted(anchor2concept('Muller'), key=itemgetter(1), reverse = True):\n",
    "    print id2title(item[0]) + ' ----- ' + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "tomatoes_mentions = tagme.mentions(\"I definitely like ice cream better than tomatoes.\")\n",
    "\n",
    "for mention in tomatoes_mentions.mentions:\n",
    "    print mention.begin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
