{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end, text],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in line['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(line['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mentions.append([curStart, curStart + len(line['text'][curWord]), line['text'][curWord]])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary parts of the overlapping\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "                    \n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    textData = []\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        \n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb])\n",
    "    \n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [4][1] is index of type of word\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.001\n",
    "    \n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh\n",
    "                and (item[4][1][0:2] == 'NN' or item[4][1] == 'JJ')):\n",
    "            mentions.append([item[0], item[1], item[2]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line)\n",
    "        solrMentions = getSolrMentions(\" \".join(line['text']))\n",
    "        \n",
    "        print line['text']\n",
    "        print trueMentions\n",
    "        print str(solrMentions) + '\\n'\n",
    "        \n",
    "        \"\"\"solrMentions0 = tagme.mentions(\" \".join(line['text']))\n",
    "        solrMentions = []\n",
    "        for item in solrMentions0.mentions:\n",
    "            solrMentions.append([item.begin, item.end, item.mention])\"\"\"\n",
    "        \n",
    "        ## get statistical results from true mentions and solr mentions\n",
    "        \n",
    "        aNumber = len(solrMentions)/len(trueMentions)\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "import nltk\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words untill not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            totalMentions = get_mention_count(textData[i][2])\n",
    "            totalAppearances = get_solr_count(textData[i][2].replace(\".\", \"\"))\n",
    "            if totalAppearances == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = totalMentions/totalAppearances\n",
    "            \n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "     \n",
    "def mentionExtract(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    splitText = [] # the text now in split form\n",
    "    mentions = [] # mentions before remove inadequate ones\n",
    "    \n",
    "    textData = [] # [[begin,end,word,anchorProb],...]\n",
    "    \n",
    "    i = 0 # for wordIndex\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb, i])\n",
    "        i += 1\n",
    "        \n",
    "        # also fill split text\n",
    "        splitText.append(text[item[1]:item[3]])\n",
    "    \n",
    "    # get rid of overlaps\n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "        \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [5][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.001 # for getting rid of unlikelies\n",
    "    \n",
    "    # put in only good mentions\n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh # if popular enough, and either some type of noun or JJ\n",
    "                and (item[5][1][0:2] == 'NN' or item[5][1] == 'JJ')):\n",
    "            mentions.append([item[4], item[0], item[1]]) # wIndex, start, end\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase (most frequent).\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "\n",
    "def getMentionSentence(text, mention):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            return s.replace(text[mention[1]:mention[2]],\"\")\n",
    "        \n",
    "    return \"\" # in case it missed\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestContextMatch(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyContext(textData, candidates, oText, useSentence = False, window = 7):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention)\n",
    "            bestIndex = bestContextMatch(textData['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        textData = mentionExtract(text) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'context':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kore\n",
      "\n",
      "context\n",
      "2017-05-26 17:31:37.542674\n",
      "\n",
      "1\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 8\n",
      "0.5 0.125 0.5 0.5\n",
      "\n",
      "2\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 4\n",
      "0.5 0.25 0.5 0.5\n",
      "\n",
      "3\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5 0.5 0.5\n",
      "\n",
      "4\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "5\n",
      "correct: 0\n",
      "found: 1\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "6\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 4\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "7\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "found: 7\n",
      "0.333333333333 0.142857142857 0.333333333333 0.333333333333\n",
      "\n",
      "8\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 6\n",
      "0.666666666667 0.333333333333 0.666666666667 0.666666666667\n",
      "\n",
      "9\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 6\n",
      "0.5 0.166666666667 0.5 0.5\n",
      "\n",
      "10\n",
      "correct: 1\n",
      "found: 5\n",
      "correct: 1\n",
      "found: 7\n",
      "0.2 0.142857142857 0.2 0.2\n",
      "\n",
      "11\n",
      "correct: 3\n",
      "found: 4\n",
      "correct: 3\n",
      "found: 7\n",
      "0.75 0.428571428571 0.75 0.75\n",
      "\n",
      "12\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 5\n",
      "0.666666666667 0.4 0.666666666667 0.666666666667\n",
      "\n",
      "13\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 5\n",
      "0.666666666667 0.4 0.666666666667 0.666666666667\n",
      "\n",
      "14\n",
      "correct: 4\n",
      "found: 5\n",
      "correct: 4\n",
      "found: 9\n",
      "0.8 0.444444444444 0.8 0.8\n",
      "\n",
      "15\n",
      "correct: 4\n",
      "found: 6\n",
      "correct: 4\n",
      "found: 8\n",
      "0.666666666667 0.5 0.666666666667 0.666666666667\n",
      "\n",
      "16\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 1\n",
      "found: 8\n",
      "0.666666666667 0.125 0.666666666667 0.333333333333\n",
      "\n",
      "17\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "found: 5\n",
      "0.333333333333 0.2 0.333333333333 0.333333333333\n",
      "\n",
      "18\n",
      "correct: 3\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 3\n",
      "1.0 0.666666666667 1.0 0.666666666667\n",
      "\n",
      "19\n",
      "correct: 1\n",
      "found: 5\n",
      "correct: 2\n",
      "found: 9\n",
      "0.2 0.222222222222 0.2 0.4\n",
      "\n",
      "20\n",
      "correct: 2\n",
      "found: 4\n",
      "correct: 2\n",
      "found: 5\n",
      "0.5 0.4 0.5 0.5\n",
      "\n",
      "21\n",
      "correct: 0\n",
      "found: 3\n",
      "correct: 0\n",
      "found: 7\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "22\n",
      "correct: 3\n",
      "found: 4\n",
      "correct: 3\n",
      "found: 9\n",
      "0.75 0.333333333333 0.75 0.75\n",
      "\n",
      "23\n",
      "correct: 3\n",
      "found: 3\n",
      "correct: 3\n",
      "found: 12\n",
      "1.0 0.25 1.0 1.0\n",
      "\n",
      "24\n",
      "correct: 1\n",
      "found: 4\n",
      "correct: 1\n",
      "found: 6\n",
      "0.25 0.166666666667 0.25 0.25\n",
      "\n",
      "25\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 5\n",
      "0.5 0.2 0.5 0.5\n",
      "\n",
      "26\n",
      "correct: 0\n",
      "found: 3\n",
      "correct: 0\n",
      "found: 6\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "27\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 8\n",
      "0.5 0.125 0.5 0.5\n",
      "\n",
      "28\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "found: 3\n",
      "0.333333333333 0.333333333333 0.333333333333 0.333333333333\n",
      "\n",
      "29\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "found: 4\n",
      "0.333333333333 0.25 0.333333333333 0.333333333333\n",
      "\n",
      "30\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "found: 5\n",
      "1.0 0.4 1.0 1.0\n",
      "\n",
      "31\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "32\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 3\n",
      "0.5 0.333333333333 0.5 0.5\n",
      "\n",
      "33\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "34\n",
      "correct: 0\n",
      "found: 1\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "35\n",
      "correct: 0\n",
      "found: 3\n",
      "correct: 0\n",
      "found: 4\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "36\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 3\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "37\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 4\n",
      "0.5 0.25 0.333333333333 0.333333333333\n",
      "\n",
      "38\n",
      "correct: 0\n",
      "found: 3\n",
      "correct: 0\n",
      "found: 4\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "39\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "40\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 2\n",
      "0.5 0.5 0.5 0.5\n",
      "\n",
      "41\n",
      "correct: 3\n",
      "found: 4\n",
      "correct: 3\n",
      "found: 6\n",
      "0.75 0.5 0.75 0.75\n",
      "\n",
      "42\n",
      "correct: 1\n",
      "found: 4\n",
      "correct: 1\n",
      "found: 5\n",
      "0.25 0.2 0.25 0.25\n",
      "\n",
      "43\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 7\n",
      "0.666666666667 0.285714285714 0.666666666667 0.666666666667\n",
      "\n",
      "44\n",
      "correct: 4\n",
      "found: 4\n",
      "correct: 4\n",
      "found: 6\n",
      "1.0 0.666666666667 1.0 1.0\n",
      "\n",
      "45\n",
      "correct: 0\n",
      "found: 2\n",
      "correct: 0\n",
      "found: 2\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "46\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "found: 5\n",
      "1.0 0.4 1.0 1.0\n",
      "\n",
      "47\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "found: 2\n",
      "1.0 1.0 1.0 1.0\n",
      "\n",
      "48\n",
      "correct: 1\n",
      "found: 2\n",
      "correct: 1\n",
      "found: 3\n",
      "0.5 0.333333333333 0.5 0.5\n",
      "\n",
      "49\n",
      "correct: 2\n",
      "found: 3\n",
      "correct: 2\n",
      "found: 3\n",
      "0.666666666667 0.666666666667 0.666666666667 0.666666666667\n",
      "\n",
      "50\n",
      "correct: 0\n",
      "found: 1\n",
      "correct: 0\n",
      "found: 4\n",
      "0.0 0.0 0.0 0.0\n",
      "\n",
      "AQUAINT\n",
      "\n",
      "context\n",
      "2017-05-26 17:31:49.329917\n",
      "\n",
      "1\n",
      "correct: 10\n",
      "found: 10\n",
      "correct: 8\n",
      "found: 59\n",
      "1.0 0.135593220339 0.909090909091 0.727272727273\n",
      "\n",
      "2\n",
      "correct: 6\n",
      "found: 10\n",
      "correct: 7\n",
      "found: 67\n",
      "0.6 0.10447761194 0.5 0.583333333333\n",
      "\n",
      "3\n",
      "correct: 7\n",
      "found: 14\n",
      "correct: 6\n",
      "found: 75\n",
      "0.5 0.08 0.5 0.428571428571\n",
      "\n",
      "4\n",
      "correct: 9\n",
      "found: 15\n",
      "correct: 9\n",
      "found: 77\n",
      "0.6 0.116883116883 0.6 0.6\n",
      "\n",
      "5\n",
      "correct: 10\n",
      "found: 11\n",
      "correct: 10\n",
      "found: 73\n",
      "0.909090909091 0.13698630137 0.833333333333 0.833333333333\n",
      "\n",
      "6\n",
      "correct: 5\n",
      "found: 7\n",
      "correct: 5\n",
      "found: 84\n",
      "0.714285714286 0.0595238095238 0.555555555556 0.555555555556\n",
      "\n",
      "7\n",
      "correct: 12\n",
      "found: 15\n",
      "correct: 11\n",
      "found: 79\n",
      "0.8 0.139240506329 0.8 0.733333333333\n",
      "\n",
      "8\n",
      "correct: 4\n",
      "found: 5\n",
      "correct: 5\n",
      "found: 59\n",
      "0.8 0.0847457627119 0.666666666667 0.833333333333\n",
      "\n",
      "9\n",
      "correct: 12\n",
      "found: 16\n",
      "correct: 10\n",
      "found: 78\n",
      "0.75 0.128205128205 0.75 0.625\n",
      "\n",
      "10\n",
      "correct: 12\n",
      "found: 18\n",
      "correct: 12\n",
      "found: 75\n",
      "0.666666666667 0.16 0.666666666667 0.666666666667\n",
      "\n",
      "11\n",
      "correct: 14\n",
      "found: 18\n",
      "correct: 14\n",
      "found: 76\n",
      "0.777777777778 0.184210526316 0.777777777778 0.777777777778\n",
      "\n",
      "12\n",
      "correct: 11\n",
      "found: 14\n",
      "correct: 11\n",
      "found: 96\n",
      "0.785714285714 0.114583333333 0.733333333333 0.733333333333\n",
      "\n",
      "13\n",
      "correct: 20\n",
      "found: 22\n",
      "correct: 20\n",
      "found: 102\n",
      "0.909090909091 0.196078431373 0.909090909091 0.909090909091\n",
      "\n",
      "14\n",
      "correct: 7\n",
      "found: 12\n",
      "correct: 8\n",
      "found: 79\n",
      "0.583333333333 0.101265822785 0.538461538462 0.615384615385\n",
      "\n",
      "15\n",
      "correct: 12\n",
      "found: 15\n",
      "correct: 11\n",
      "found: 87\n",
      "0.8 0.126436781609 0.75 0.6875\n",
      "\n",
      "16\n",
      "correct: 7\n",
      "found: 8\n",
      "correct: 7\n",
      "found: 65\n",
      "0.875 0.107692307692 0.875 0.875\n",
      "\n",
      "17\n",
      "correct: 6\n",
      "found: 10\n",
      "correct: 6\n",
      "found: 82\n",
      "0.6 0.0731707317073 0.5 0.5\n",
      "\n",
      "18\n",
      "correct: 14\n",
      "found: 20\n",
      "correct: 14\n",
      "found: 78\n",
      "0.7 0.179487179487 0.7 0.7\n",
      "\n",
      "19\n",
      "correct: 4\n",
      "found: 13\n",
      "correct: 6\n",
      "found: 85\n",
      "0.307692307692 0.0705882352941 0.307692307692 0.461538461538\n",
      "\n",
      "20\n",
      "correct: 11\n",
      "found: 15\n",
      "correct: 10\n",
      "found: 90\n",
      "0.733333333333 0.111111111111 0.6875 0.625\n",
      "\n",
      "21\n",
      "correct: 21\n",
      "found: 31\n",
      "correct: 20\n",
      "found: 68\n",
      "0.677419354839 0.294117647059 0.617647058824 0.588235294118\n",
      "\n",
      "22\n",
      "correct: 9\n",
      "found: 16\n",
      "correct: 8\n",
      "found: 59\n",
      "0.5625 0.135593220339 0.5625 0.5\n",
      "\n",
      "23\n",
      "correct: 10\n",
      "found: 12\n",
      "correct: 10\n",
      "found: 77\n",
      "0.833333333333 0.12987012987 0.769230769231 0.769230769231\n",
      "\n",
      "24\n",
      "correct: 4\n",
      "found: 5\n",
      "correct: 5\n",
      "found: 40\n",
      "0.8 0.125 0.8 1.0\n",
      "\n",
      "25\n",
      "correct: 7\n",
      "found: 11\n",
      "correct: 6\n",
      "found: 94\n",
      "0.636363636364 0.063829787234 0.636363636364 0.545454545455\n",
      "\n",
      "26\n",
      "correct: 6\n",
      "found: 8\n",
      "correct: 7\n",
      "found: 79\n",
      "0.75 0.0886075949367 0.75 0.875\n",
      "\n",
      "27\n",
      "correct: 14\n",
      "found: 16\n",
      "correct: 11\n",
      "found: 73\n",
      "0.875 0.150684931507 0.823529411765 0.647058823529\n",
      "\n",
      "28\n",
      "correct: 7\n",
      "found: 14\n",
      "correct: 8\n",
      "found: 73\n",
      "0.5 0.109589041096 0.466666666667 0.533333333333\n",
      "\n",
      "29\n",
      "correct: 7\n",
      "found: 12\n",
      "correct: 6\n",
      "found: 51\n",
      "0.583333333333 0.117647058824 0.583333333333 0.5\n",
      "\n",
      "30\n",
      "correct: 11\n",
      "found: 14\n",
      "correct: 8\n",
      "found: 78\n",
      "0.785714285714 0.102564102564 0.785714285714 0.571428571429\n",
      "\n",
      "31\n",
      "correct: 5\n",
      "found: 8\n",
      "correct: 5\n",
      "found: 64\n",
      "0.625 0.078125 0.625 0.625\n",
      "\n",
      "32\n",
      "correct: 9\n",
      "found: 11\n",
      "correct: 8\n",
      "found: 68\n",
      "0.818181818182 0.117647058824 0.818181818182 0.727272727273\n",
      "\n",
      "33\n",
      "correct: 9\n",
      "found: 9\n",
      "correct: 8\n",
      "found: 71\n",
      "1.0 0.112676056338 1.0 0.888888888889\n",
      "\n",
      "34\n",
      "correct: 7\n",
      "found: 12\n",
      "correct: 7\n",
      "found: 74\n",
      "0.583333333333 0.0945945945946 0.583333333333 0.583333333333\n",
      "\n",
      "35\n",
      "correct: 7\n",
      "found: 10\n",
      "correct: 6\n",
      "found: 71\n",
      "0.7 0.0845070422535 0.636363636364 0.545454545455\n",
      "\n",
      "36\n",
      "correct: 10\n",
      "found: 11\n",
      "correct: 9\n",
      "found: 71\n",
      "0.909090909091 0.12676056338 0.909090909091 0.818181818182\n",
      "\n",
      "37\n",
      "correct: 8\n",
      "found: 14\n",
      "correct: 9\n",
      "found: 70\n",
      "0.571428571429 0.128571428571 0.533333333333 0.6\n",
      "\n",
      "38\n",
      "correct: 5\n",
      "found: 6\n",
      "correct: 5\n",
      "found: 71\n",
      "0.833333333333 0.0704225352113 0.714285714286 0.714285714286\n",
      "\n",
      "39\n",
      "correct: 12\n",
      "found: 18\n",
      "correct: 11\n",
      "found: 65\n",
      "0.666666666667 0.169230769231 0.666666666667 0.611111111111\n",
      "\n",
      "40\n",
      "correct: 7\n",
      "found: 14\n",
      "correct: 7\n",
      "found: 86\n",
      "0.5 0.0813953488372 0.4375 0.4375\n",
      "\n",
      "41\n",
      "correct: 13\n",
      "found: 19\n",
      "correct: 11\n",
      "found: 78\n",
      "0.684210526316 0.141025641026 0.684210526316 0.578947368421\n",
      "\n",
      "42\n",
      "correct: 12\n",
      "found: 15\n",
      "correct: 11\n",
      "found: 74\n",
      "0.8 0.148648648649 0.8 0.733333333333\n",
      "\n",
      "43\n",
      "correct: 12\n",
      "found: 14\n",
      "correct: 11\n",
      "found: 82\n",
      "0.857142857143 0.134146341463 0.857142857143 0.785714285714\n",
      "\n",
      "44\n",
      "correct: 12\n",
      "found: 13\n",
      "correct: 11\n",
      "found: 90\n",
      "0.923076923077 0.122222222222 0.8 0.733333333333\n",
      "\n",
      "45\n",
      "correct: 3\n",
      "found: 5\n",
      "correct: 3\n",
      "found: 67\n",
      "0.6 0.044776119403 0.5 0.5\n",
      "\n",
      "46\n",
      "correct: 5\n",
      "found: 10\n",
      "correct: 5\n",
      "found: 70\n",
      "0.5 0.0714285714286 0.454545454545 0.454545454545\n",
      "\n",
      "47\n",
      "correct: 7\n",
      "found: 7\n",
      "correct: 6\n",
      "found: 78\n",
      "1.0 0.0769230769231 0.777777777778 0.666666666667\n",
      "\n",
      "48\n",
      "correct: 14\n",
      "found: 23\n",
      "correct: 13\n",
      "found: 78\n",
      "0.608695652174 0.166666666667 0.608695652174 0.565217391304\n",
      "\n",
      "49\n",
      "correct: 7\n",
      "found: 10\n",
      "correct: 7\n",
      "found: 84\n",
      "0.7 0.0833333333333 0.636363636364 0.636363636364\n",
      "\n",
      "50\n",
      "correct: 6\n",
      "found: 17\n",
      "correct: 6\n",
      "found: 61\n",
      "0.352941176471 0.0983606557377 0.333333333333 0.333333333333\n",
      "\n",
      "MSNBC\n",
      "\n",
      "context\n",
      "2017-05-26 17:34:42.067107\n",
      "\n",
      "1\n",
      "correct: 43\n",
      "found: 54\n",
      "correct: 42\n",
      "found: 239\n",
      "0.796296296296 0.175732217573 0.754385964912 0.736842105263\n",
      "\n",
      "2\n",
      "correct: 15\n",
      "found: 24\n",
      "correct: 14\n",
      "found: 55\n",
      "0.625 0.254545454545 0.625 0.583333333333\n",
      "\n",
      "3\n",
      "correct: 32\n",
      "found: 41\n",
      "correct: 33\n",
      "found: 254\n",
      "0.780487804878 0.129921259843 0.711111111111 0.733333333333\n",
      "\n",
      "4\n",
      "correct: 16\n",
      "found: 36\n",
      "correct: 15\n",
      "found: 90\n",
      "0.444444444444 0.166666666667 0.444444444444 0.416666666667\n",
      "\n",
      "5\n",
      "correct: 33\n",
      "found: 92\n",
      "correct: 35\n",
      "found: 246\n",
      "0.358695652174 0.142276422764 0.351063829787 0.372340425532\n",
      "\n",
      "6\n",
      "correct: 11\n",
      "found: 12\n",
      "correct: 10\n",
      "found: 67\n",
      "0.916666666667 0.149253731343 0.916666666667 0.833333333333\n",
      "\n",
      "7\n",
      "correct: 50\n",
      "found: 67\n",
      "correct: 51\n",
      "found: 271\n",
      "0.746268656716 0.188191881919 0.704225352113 0.718309859155\n",
      "\n",
      "8\n",
      "correct: 11\n",
      "found: 15\n",
      "correct: 13\n",
      "found: 104\n",
      "0.733333333333 0.125 0.733333333333 0.866666666667\n",
      "\n",
      "9\n",
      "correct: 24\n",
      "found: 48\n",
      "correct: 23\n",
      "found: 267\n",
      "0.5 0.0861423220974 0.489795918367 0.469387755102\n",
      "\n",
      "10\n",
      "correct: 29\n",
      "found: 45\n",
      "correct: 26\n",
      "found: 224\n",
      "0.644444444444 0.116071428571 0.644444444444 0.577777777778\n",
      "\n",
      "11\n",
      "correct: 11\n",
      "found: 11\n",
      "correct: 11\n",
      "found: 72\n",
      "1.0 0.152777777778 0.916666666667 0.916666666667\n",
      "\n",
      "12\n",
      "correct: 22\n",
      "found: 33\n",
      "correct: 21\n",
      "found: 185\n",
      "0.666666666667 0.113513513514 0.647058823529 0.617647058824\n",
      "\n",
      "13\n",
      "correct: 17\n",
      "found: 33\n",
      "correct: 17\n",
      "found: 167\n",
      "0.515151515152 0.101796407186 0.515151515152 0.515151515152\n",
      "\n",
      "14\n",
      "correct: 22\n",
      "found: 36\n",
      "correct: 21\n",
      "found: 195\n",
      "0.611111111111 0.107692307692 0.594594594595 0.567567567568\n",
      "\n",
      "15\n",
      "correct: 6\n",
      "found: 8\n",
      "correct: 6\n",
      "found: 73\n",
      "0.75 0.0821917808219 0.666666666667 0.666666666667\n",
      "\n",
      "16\n",
      "correct: 19\n",
      "found: 23\n",
      "correct: 16\n",
      "found: 254\n",
      "0.826086956522 0.0629921259843 0.791666666667 0.666666666667\n",
      "\n",
      "17\n",
      "correct: 10\n",
      "found: 13\n",
      "correct: 10\n",
      "found: 113\n",
      "0.769230769231 0.0884955752212 0.625 0.625\n",
      "\n",
      "18\n",
      "correct: 7\n",
      "found: 8\n",
      "correct: 8\n",
      "found: 164\n",
      "0.875 0.0487804878049 0.7 0.8\n",
      "\n",
      "19\n",
      "correct: 13\n",
      "found: 23\n",
      "correct: 12\n",
      "found: 92\n",
      "0.565217391304 0.130434782609 0.565217391304 0.521739130435\n",
      "\n",
      "20\n",
      "correct: 10\n",
      "found: 11\n",
      "correct: 10\n",
      "found: 173\n",
      "0.909090909091 0.0578034682081 0.909090909091 0.909090909091\n",
      "\n",
      "{'kore': {'context': {'S Prec': 0.42900000000000005, 'M Prec': 0.25283333333333335, 'S Rec': 0.42566666666666675, 'M Rec': 0.4163333333333334}}, 'MSNBC': {'context': {'S Prec': 0.7016596309015312, 'M Prec': 0.12401398060706514, 'S Rec': 0.6652792149425157, 'M Rec': 0.6557093718615729}}, 'AQUAINT': {'context': {'S Prec': 0.7129750189556648, 'M Prec': 0.11758490211063682, 'S Rec': 0.674619576169482, 'M Rec': 0.6507849817432535}}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['context']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # original split string with mentions given\n",
    "            resultS = wikifyEval(copy.deepcopy(line), True, maxC = 7, method = mthd)\n",
    "            # unsplit string to be manually split and mentions found\n",
    "            resultM = wikifyEval(\" \".join(line['text']), False, maxC = 7, method = mthd)\n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "            precM = precision(trueEntities, resultM) # precision of manual split\n",
    "            recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "            recM = recall(trueEntities, resultM) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing performance of TagMe wikification method.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "\n",
    "    print str(datetime.now()) + '\\n'\n",
    "\n",
    "    # reset counters\n",
    "    totalPrecM = 0\n",
    "    totalRecM = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "        \n",
    "        antns = tagme.annotate(\" \".join(line['text']))\n",
    "        resultM = []\n",
    "        for an in antns.get_annotations(0.005):\n",
    "            resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "        trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "\n",
    "        ## get statistical results from true entities and results\n",
    "        precM = precision(trueEntities, resultM)\n",
    "        recM = recall(trueEntities, resultM)\n",
    "\n",
    "        #clear_output() # delete this after\n",
    "        print str(precM) + ' ' + str(recM) + '\\n'\n",
    "        #print str(precS) + ' ' + str(recS)\n",
    "\n",
    "        # track results\n",
    "        totalPrecM += precM\n",
    "        totalRecM += recM\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "    performances[dataset['name']] = {'Precision':totalPrecM/totalLines,\n",
    "                                          'Recall':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, method='popular')\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -In wikifyContext make the current sentence worth 1 and each surrounding sentence worth 0.5.\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = sorted(anchor2concept(\"David Edgar\"), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "\n",
    "for tmpp in tmp:\n",
    "    print 'id: ' + str(tmpp[0]) + ', title: ' + id2title(tmpp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase = {u'text': [u'Voller', u'presidential', u'preferences', u'How', u'will', u'American', u'voters', u'compensate', u'in', u'the', u'next', u'search', u'for', u'a', u'president?', u'WASHINGTON', u'-', u'Now', u'that', u'the', u'38th', u'president', u'has', u'been', u'laid', u'to', u'rest,', u'the', u'capital', u'can', u'take', u'up', u'the', u'main', u'business', u'of', u'2007:', u'trying', u'to', u'figure', u'out', u'who', u'will', u'be', u'the', u'44th.', u'What', u'type', u'of', u'leader', u'does', u'the', u'country', u'want?', u'Here', u'is', u'my', u'sense', u'of', u'it,', u'based', u'on', u'talking', u'to', u'politicians,', u'strategists', u'and', u'voters', u'here', u'and', u'around', u'the', u'nation.', u'No', u'ideologues,', u'please', u'There', u'was', u'a', u'time', u'when', u'President George W. Bush', u\"'s\", u'ideological', u'certitude', u'was', u'politically', u'appealing', u'and', u'perhaps', u'functionally', u'necessary.', u'That', u'time', u'has', u'long', u'since', u'passed.', u'The', u'country', u'is', u'tired,', u'even', u'fearful,', u'of', u'leaders', u'with', u'fervent', u'beliefs', u'that', u'seem', u'impervious', u'to', u'new', u'(or', u'even', u'old)', u'facts.', u'Voters', u'see', u'the', u'war', u'in', u'Iraq', u'as', u'an', u'\"idea,\"', u'not', u'a', u'solution', u'-', u'and', u'Americans', u'do', u'not', u'like', u'ideas', u'that', u'do', u'not', u'work.', u'Voters', u'likely', u'will', u'view', u'Bush', u\"'s\", u'\"surge\"', u'of', u'troops', u'into', u'Iraq', u'as', u'new', u'evidence', u'of', u'failure,', u'and', u'the', u'dangers', u'of', u'a', u'leader', u'who', u'depends', u'on', u'preconceived', u'ideas.', u'Serious', u'student', u'Presidential', u'elections', u'are', u'a', u'never-ending', u'series', u'of', u'mid-course', u'corrections.', u'Voters', u'look', u'to', u'compensate', u'for', u'the', u'leadership', u'weaknesses', u'of', u'the', u'incumbent.', u'An', u'example', u'comes', u'from', u'the', u'life', u'and', u'career', u'of', u'Gerald Ford', u'.', u'In', u'1976,', u'voters', u'wanted', u'a', u'pure', u'antidote', u'to', u'Richard Nixon', u\"'s\", u'paranoid', u'megalomania.', u'Once', u'Ford', u'pardoned', u'Nixon', u',', u'he', u'could', u'not', u'be', u'that', u'candidate.', u'Instead,', u'Americans', u'chose', u'Jimmy Carter', u',', u'a', u'peanut', u'farmer', u'who', u'had', u'never', u'worked', u'in', u'Washington', u',', u'and', u'who', u'promised', u'never', u'to', u'lie', u'to', u'the', u'American people', u'.', u'The', u'counterpoint', u'thinking', u'continues.', u'Voters', u'in', u'2008', u'are', u'going', u'to', u'want', u'someone', u'who', u'prides', u'himself', u'(or', u'herself)', u'on', u'spending', u'time', u'in', u'the', u'library', u'-', u'who', u'has', u'a', u'hands-on', u'curiosity', u'about', u'the', u'details.', u'Washington', u'experience', u'not', u'necessary', u'Voters', u'these', u'days', u'not', u'only', u'do', u'not', u'value', u'Washington', u'experience', u'-', u'or', u'any', u'office-holding', u'experience', u'-', u'it', u'can', u'make', u'them', u'suspicious.', u'That', u'is', u'what', u'strategists', u'and', u'polltakers', u'for', u'Sen.', u'Evan Bayh', u'found', u'when', u'they', u'studied', u'whether', u'he', u'should', u'run', u'for', u'president.', u'They', u'found', u'that', u'his', u'remarkably', u'deep', u'resume', u'-', u'the', u'son', u'of', u'a', u'senator,', u'he', u'was', u'the', u'\"boy', u'governor\"', u'of', u'Indiana', u'before', u'going', u'to', u'the', u'Senate', u'-', u'was', u'as', u'handicap.', u'Americans', u'always', u'are', u'dubious', u'about', u'the', u'capital,', u'but', u'that', u'sentiment', u'seems', u'particularly', u'strong.', u'Bayh', u'decided', u'not', u'to', u'run.', u'\"`', u'Washington', u\"'\", u\"doesn't\", u'make', u'the', u'case,\"', u'said', u'Dan Pfeiffer', u',', u'who', u'worked', u'for', u'Bayh', u'.', u'No', u'more', u'boomer', u'obsessions', u'Not', u'all', u'elections', u'are', u'about', u'change,', u'but', u'2008', u'will', u'be.', u'Americans', u'are', u'moderately', u'upbeat', u'about', u'the', u\"country's\", u'prospects,', u'but', u'deeply', u'worried', u'about', u'the', u'world', u'-', u'and', u'they', u'have', u'come', u'to', u'realize', u'that', u'they', u\"can't\", u'separate', u'one', u'from', u'the', u'other.', u'One', u'thing', u'for', u'sure,', u'says', u'Pfeiffer', u',', u'voters', u'are', u'tired', u'of', u'arguing', u'about', u'the', u'culture', u'of', u'the', u'1960s', u'and', u'other', u'Boomer', u'issues.', u'\"There', u'is', u'a', u'sense', u'that', u'the', u'2004', u'election', u'was', u'too', u'much', u'about', u'who', u'did', u'or', u'did', u'not', u'do', u'what', u'in', u'Vietnam', u',\"', u'said', u'Pfeiffer', u',', u'referring', u'to', u'the', u'Bush campaign', u'against', u'Sen.', u'John Kerry', u'.', u'In', u'2000,', u'Bush', u'won', u'in', u'part', u'by', u'selling', u'himself', u'as', u'a', u'\"grown', u'up\"', u'Boomer', u'answer', u'to', u'Bill Clinton', u'.', u'\"Voters', u'are', u'tired', u'of', u'that', u'era', u'and', u'its', u'concerns,\"', u'said', u'Pfeiffer', u'said.', u'\"They', u'want', u'to', u'move', u'on.\"', u'Know', u'the', u'middle', u'class', u'Bushes', u'have', u'a', u'congenital', u'family', u'problem', u'with', u'this,', u'and', u'it', u'leaves', u'an', u'opening', u'for', u'someone', u'-', u'of', u'either', u'party', u'-', u'who', u'can', u'prove', u'that', u'he', u'or', u'she', u'really', u'understands', u'the', u'strains', u'of', u'middle', u'class', u'life.', u\"It's\", u'not', u'just', u'about', u'money,', u'but', u'about', u'cultural', u'assaults', u'and', u'the', u'lack', u'of', u'time', u'for', u'family', u'in', u'an', u'era', u'when', u'both', u'parents', u'or', u'partners', u'need', u'to', u'work.', u'In', u'his', u'forthcoming', u'book,', u'Positively', u'American,', u'Sen.', u'Charles Schumer', u'of', u'New York', u'imagines', u'the', u'hard', u'life', u'of', u'a', u'fictitious', u'middle', u'class', u'family', u'-', u'and', u'offers', u'a', u'series', u'of', u'governmental', u'proposals', u'to', u'address', u'them.', u'A', u'shrewd', u'student', u'of', u'the', u'American', u'mood,', u'Schumer', u'is', u'aiming', u'in', u'the', u'right', u'direction.', u'The', u'next', u'president', u'will', u'need', u'to', u'show', u'that', u'he', u'or', u'she', u'understands', u'that', u'family.'], u'mentions': [[15, u'Washington,_D.C.', 0, 106, 116], [81, u'George_W._Bush', 0, 459, 483], [123, u'Iraq', 0, 743, 747], [145, u'George_W._Bush', 0, 853, 857], [151, u'Iraq', 0, 884, 888], [199, u'Gerald_Ford', 0, 1191, 1202], [209, u'Richard_Nixon', 0, 1247, 1260], [214, u'Gerald_Ford', 0, 1291, 1295], [216, u'Richard_Nixon', 0, 1305, 1310], [227, u'Jimmy_Carter', 0, 1370, 1382], [237, u'Washington,_D.C.', 0, 1425, 1435], [247, u'Demographics_of_the_United_States', 0, 1475, 1490], [281, u'Washington,_D.c.', 0, 1685, 1695], [293, u'Washington,_D.c.', 0, 1761, 1771], [314, u'Evan_Bayh', 0, 1898, 1907], [344, u'Indiana', 0, 2065, 2072], [349, u'United_States_Senate', 0, 2093, 2099], [367, u'Evan_Bayh', 0, 2213, 2217], [373, u'Washington,_D.C.', 0, 2241, 2251], [380, u'Dan_Pfeiffer', 0, 2283, 2295], [385, u'Evan_Bayh', 0, 2313, 2317], [435, u'Dan_Pfeiffer', 0, 2600, 2608], [450, u'Boomer', 0, 2680, 2686], [472, u'Vietnam_War', 0, 2785, 2792], [475, u'Dan_Pfeiffer', 0, 2801, 2809], [480, u'George_W._Bush_presidential_campaign,_2004', 0, 2829, 2842], [483, u'John_Kerry', 0, 2856, 2866], [487, u'George_W._Bush', 0, 2878, 2882], [501, u'Bill_Clinton', 0, 2947, 2959], [513, u'Dan_Pfeiffer', 0, 3016, 3024], [593, u'Charles_Schumer', 0, 3459, 3474], [595, u'New_York', 0, 3478, 3486], [624, u'Charles_Schumer', 0, 3650, 3657]]}\n",
    "wikified = [phrase['text']]\n",
    "cands = generateCandidates(phrase, 7)\n",
    "wikified.append(wikifyContext(phrase, cands, ctxBrchSz = len(phrase['text'])))\n",
    "\n",
    "for mention in wikified[1]:\n",
    "    mention[1] = id2title(mention[1])\n",
    "    \n",
    "print (\" \".join(wikified[0])).encode('utf-8').strip()\n",
    "print wikified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(33509L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text = \" \".join([\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"])\n",
    "print text\n",
    "\n",
    "text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "text = text.replace('+', r'\\+')\n",
    "text = text.replace(\"-\", \"\\-\")\n",
    "text = text.replace(\"&&\", \"\\&&\")\n",
    "text = text.replace(\"||\", \"\\||\")\n",
    "text = text.replace(\"!\", \"\\!\")\n",
    "text = text.replace(\"(\", \"\\(\")\n",
    "text = text.replace(\")\", \"\\)\")\n",
    "text = text.replace(\"{\", \"\\{\")\n",
    "text = text.replace(\"}\", \"\\}\")\n",
    "text = text.replace(\"[\", \"\\[\")\n",
    "text = text.replace(\"]\", \"\\]\")\n",
    "text = text.replace(\"^\", \"\\^\")\n",
    "text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "text = text.replace(\"~\", \"\\~\")\n",
    "text = text.replace(\"*\", \"\\*\")\n",
    "text = text.replace(\"?\", \"\\?\")\n",
    "text = text.replace(\":\", \"\\:\")\n",
    "\n",
    "text = text.decode('string_escape')\n",
    "\n",
    "print text + '\\n\\n'\n",
    "\n",
    "addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "r = requests.post(addr, params=params, data=text)\n",
    "textData = r.json()['tags']\n",
    "\n",
    "print textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phraseData = {\"text\": [\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"], \"mentions\": [[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]}\n",
    "print str(phraseData) + '\\n'\n",
    "phraseData = mentionStartsAndEnds(phraseData)\n",
    "print phraseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in sorted(anchor2concept('Muller'), key=itemgetter(1), reverse = True):\n",
    "    print id2title(item[0]) + ' ----- ' + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tagme\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "\n",
    "antns = tagme.annotate(\"I definitely like ice cream better than tomatoes.\")\n",
    "\n",
    "for an in antns.get_annotations(0.1):\n",
    "    print an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
