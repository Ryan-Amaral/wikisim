{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "anchor2concept('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(48324759L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "David and Victoria named their children Brooklyn , Romeo , Cruz , and Harper Seven .\n",
      "[('David', 'NNP'), ('and', 'CC'), ('Victoria', 'NNP'), ('named', 'VBD'), ('their children', 'JJ'), ('Brooklyn', 'NNP'), ('Romeo', 'NNP'), ('Cruz', 'NNP'), ('and', 'CC'), ('Harper', 'NNP'), ('Seven', 'NNP')]\n",
      "correct: 2\n",
      "found: 5\n",
      "correct: 2\n",
      "actual: 2\n",
      "0.4 1.0\n",
      "\n",
      "2\n",
      "David and Victoria added spice to their marriage .\n",
      "[('David', 'NNP'), ('and', 'CC'), ('Victoria', 'NNP'), ('added', 'VBD'), ('spice', 'NN'), ('to', 'TO'), ('their', 'PRP$'), ('marriage', 'NN')]\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "actual: 2\n",
      "1.0 1.0\n",
      "\n",
      "3\n",
      "Tiger was lost in the woods when he got divorced from Elin .\n",
      "[('Tiger', 'NN'), ('was lost', 'NN'), ('in', 'IN'), ('the woods', 'NNS'), ('when', 'WRB'), ('he', 'PRP'), ('got', 'VBD'), ('divorced', 'VBN'), ('from', 'IN'), ('Elin', 'NNP')]\n",
      "correct: 1\n",
      "found: 1\n",
      "correct: 1\n",
      "actual: 2\n",
      "1.0 0.5\n",
      "\n",
      "4\n",
      "Tiger lost the US Open .\n",
      "[('Tiger', 'NN'), ('lost', 'VBD'), ('the', 'DT'), ('US Open', 'NNP')]\n",
      "correct: 1\n",
      "found: 1\n",
      "correct: 1\n",
      "actual: 2\n",
      "1.0 0.5\n",
      "\n",
      "5\n",
      "Madonna played Eva and was seen with Carlos .\n",
      "[('Madonna', 'NNP'), ('played', 'VBD'), ('Eva', 'NNP'), ('and', 'CC'), ('was seen', 'JJ'), ('with', 'IN'), ('Carlos', 'NNP')]\n",
      "correct: 1\n",
      "found: 3\n",
      "correct: 1\n",
      "actual: 1\n",
      "0.333333333333 1.0\n",
      "\n",
      "6\n",
      "In this musical , Madonna played the role of the First Lady .\n",
      "[('In', 'IN'), ('this', 'DT'), ('musical', 'JJ'), ('Madonna', 'NNP'), ('played', 'VBD'), ('the', 'DT'), ('role', 'NN'), ('of the', 'NN'), ('First Lady', 'NNP')]\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "actual: 2\n",
      "1.0 1.0\n",
      "\n",
      "7\n",
      "Angelina , her father Jon , and her partner Brad never played together in the same movie .\n",
      "[('Angelina', 'NNP'), ('her father', 'PRP'), ('Jon', 'NNP'), ('and', 'CC'), ('her', 'PRP$'), ('partner', 'NN'), ('Brad', 'NNP'), ('never', 'RB'), ('played', 'VBD'), ('together', 'RB'), ('in', 'IN'), ('the same movie', 'NN')]\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "actual: 3\n",
      "1.0 0.666666666667\n",
      "\n",
      "8\n",
      "Heidi and her husband Seal live in Vegas .\n",
      "[('Heidi', 'NNP'), ('and', 'CC'), ('her husband', 'VB'), ('Seal', 'NNP'), ('live in', 'NN'), ('Vegas', 'NNP')]\n",
      "correct: 3\n",
      "found: 3\n",
      "correct: 3\n",
      "actual: 3\n",
      "1.0 1.0\n",
      "\n",
      "9\n",
      "Paris and Kim are both wealthy It Girls who had sex tapes on the Internet .\n",
      "[('Paris', 'NNP'), ('and', 'CC'), ('Kim', 'NNP'), ('are', 'VBP'), ('both', 'DT'), ('wealthy', 'JJ'), ('It Girls', 'NNP'), ('who', 'WP'), ('had sex', 'VBP'), ('tapes', 'NNS'), ('on the Internet', 'NN')]\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "actual: 2\n",
      "1.0 1.0\n",
      "\n",
      "10\n",
      "Justin , Stefani , and Kate are among the most popular people on both MTV and Twitter .\n",
      "[('Justin', 'NNP'), ('Stefani', 'NNP'), ('and', 'CC'), ('Kate', 'NNP'), ('are', 'VBP'), ('the', 'DT'), ('most popular', 'JJ'), ('people', 'NNS'), ('on', 'IN'), ('both', 'DT'), ('MTV', 'NNP'), ('and', 'CC'), ('Twitter', 'NNP')]\n",
      "correct: 5\n",
      "found: 5\n",
      "correct: 5\n",
      "actual: 5\n",
      "1.0 1.0\n",
      "\n",
      "11\n",
      "Dylan performed Hurricane about the black fighter Carter , from his album Desire .\n",
      "[('Dylan', 'NNP'), ('performed', 'VBD'), ('Hurricane', 'NNP'), ('about', 'IN'), ('the', 'DT'), ('black', 'JJ'), ('fighter', 'NN'), ('Carter', 'NN'), ('from', 'IN'), ('his', 'PRP$'), ('album', 'NN'), ('Desire', 'NN')]\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "actual: 4\n",
      "1.0 0.5\n",
      "\n",
      "12\n",
      "Desire contains a duet with Harris in the song Joey .\n",
      "[('Desire', 'NN'), ('contains', 'VBZ'), ('a', 'DT'), ('duet with', 'NN'), ('Harris', 'NNP'), ('in', 'IN'), ('the song', 'JJ'), ('Joey', 'NNP')]\n",
      "correct: 2\n",
      "found: 2\n",
      "correct: 2\n",
      "actual: 3\n",
      "1.0 0.666666666667\n",
      "\n",
      "13\n",
      "Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page .\n",
      "[('Three', 'CD'), ('of the', 'JJ'), ('greatest', 'JJS'), ('guitarists', 'NNS'), ('started', 'VBD'), ('their', 'PRP$'), ('career', 'NN'), ('in', 'IN'), ('a single', 'JJ'), ('band', 'NN'), ('Clapton', 'NNP'), ('Beck', 'NNP'), ('and', 'CC'), ('Page', 'NNP')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3a7abcd76f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0msolrMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSolrMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opening_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0msolrMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSolrMentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m## get statistical results from true mentions and solr mentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3a7abcd76f2b>\u001b[0m in \u001b[0;36mgetSolrMentions\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtotalMentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mention_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtotalAppearances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_solr_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         if (totalAppearances > 0 and\n\u001b[1;32m    114\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mtotalMentions\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotalAppearances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmentionPThrsh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3a7abcd76f2b>\u001b[0m in \u001b[0;36mget_solr_count\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mqstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://localhost:8983/solr/enwiki20160305/select'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'indent'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rows'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'response'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    421\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    592\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from wikipedia import *\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "\"\"\"\n",
    "Testing the Solr splitting\n",
    "\"\"\"\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def getTextMentions(line, isWiki):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the mentions in an evaluable format, includes the mentions'\n",
    "        start and end.\n",
    "    Args:\n",
    "        line: The json data that has info that needs to be converted.\n",
    "        isWiki: Whether the inputted line is from the wiki 5000 dataset, which needs alternate\n",
    "            handling.\n",
    "    Return:\n",
    "        The mentions in the form [[start, end],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mentions = []\n",
    "    \n",
    "    if isWiki:\n",
    "        for item in json.loads(line['opening_annotation']):\n",
    "            mentions.append([item['from'], item['to']])\n",
    "    else:\n",
    "        curWord = 0 \n",
    "        curStart = 0\n",
    "        for mention in line['mentions']:\n",
    "            while curWord < mention[0]:\n",
    "                curStart += len(line['text'][curWord]) + 1\n",
    "                curWord += 1\n",
    "            mentions.append([curStart, curStart + len(line['text'][curWord])])\n",
    "        \n",
    "    return mentions\n",
    "\n",
    "def getSolrMentions(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A method to split the text and try to extract mentions using Solr.\n",
    "    Args:\n",
    "        text: The text to find mentions in.\n",
    "    Return:\n",
    "        The mentions as found from our method using Solr.\n",
    "    \"\"\"\n",
    "    \n",
    "    print unidecode(text.decode('utf-8'))\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text)\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(text[item[1]:item[3]])\n",
    "        \n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [6][1] is index of type of word\n",
    "\n",
    "    print postrs\n",
    "    \n",
    "    mentions = []\n",
    "    mentionPThrsh = 0.005\n",
    "    \n",
    "    for item in textData:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if (totalAppearances > 0 and\n",
    "                (totalMentions/totalAppearances) >= mentionPThrsh\n",
    "                and (item[6][1] == 'NNP' or item[6][1] == 'NNPS')):\n",
    "            mentions.append([item[1], item[3]])\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def precision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if trueMentions[trueIndex] == otherMentions[otherIndex]:\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherSet against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if trueMentions[trueIndex] == otherMentions[otherIndex]:\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        doAppend = True\n",
    "        theLine = json.loads(line.decode('utf-8').strip())\n",
    "        \n",
    "        if dataset['name'] == 'wiki5000':\n",
    "            textName = 'opening_text'\n",
    "        else:\n",
    "            textName = 'text'\n",
    "        \n",
    "        #for i in \" \".join(theLine[textName]):\n",
    "        #    if ord(i) >= 128:\n",
    "        #        doAppend = False\n",
    "        #        break\n",
    "                \n",
    "        if doAppend:\n",
    "            dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "\n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = getTextMentions(line, dataset['name'] == 'wiki5000')\n",
    "        if dataset['name'] == 'wiki5000':  \n",
    "            solrMentions = getSolrMentions(unidecode(line['opening_text']))\n",
    "        else:\n",
    "            solrMentions = getSolrMentions(unidecode(\" \".join(line['text'])))\n",
    "\n",
    "        ## get statistical results from true mentions and solr mentions\n",
    "\n",
    "        prec = precision(trueMentions, solrMentions)\n",
    "        rec = recall(trueMentions, solrMentions)\n",
    "        print str(prec) + ' ' + str(rec) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                           'Recall':totalRec/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Dalhousie University\"\n",
    "\n",
    "print get_mention_count(text)\n",
    "print get_solr_count(text)\n",
    "\n",
    "print get_mention_count(text)/get_solr_count(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```curl -X POST \\\n",
    "  'http://localhost:8983/solr/geonames/tag?overlaps=NO_SUB&tagsLimit=5000&fl=id,name,countrycode&wt=json&indent=on' \\\n",
    "  -H 'Content-Type:text/plain' -d 'Hello New York City'```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wikification for evaluation purposes\n",
    "\"\"\"\n",
    "\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "from __future__ import division\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionStartsAndEnds(phraseData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phraseData object and appends it's mentions with the start and end\n",
    "        index of each mention in the original string.\n",
    "    Args:\n",
    "        phraseData: [['words','split','like','this'],[[wordId,entityId,frequency,start,end],...]]\n",
    "    Return:\n",
    "        The same phraseData but with each mention containing the start and end of that\n",
    "        mention in the source text\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in phraseData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(phraseData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "        mention.append(0) # frequency placeholder\n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(phraseData['text'][curWord])) # end of the mention\n",
    "\n",
    "    return phraseData\n",
    "     \n",
    "def splitWords(phrase):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a phrase and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions.\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=phrase)\n",
    "    textData = r.json()['tags']\n",
    "    \n",
    "    splitText = []\n",
    "    mentions = []\n",
    "    \n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(phrase[item[1]:item[3]])\n",
    "        \n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [6][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.005\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    i = 0\n",
    "    for item in textData:    \n",
    "        totalMentions = get_mention_count(phrase[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(phrase[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if (totalAppearances > 0 \n",
    "                and (totalMentions/totalAppearances) >= mentionPThrsh \n",
    "                and phrase[item[1]:item[3]] not in stopWords\n",
    "                and (item[6][1] == 'NNP' or item[6][1] == 'NNPS' or item[6][1] == 'NN')):\n",
    "            mentions.append([i, '0', 0, item[1], item[3]])\n",
    "            \n",
    "        splitText.append(phrase[item[1]:item[3]])\n",
    "        i += 1\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(phrase, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in phrase.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for mention in phrase['mentions']:\n",
    "        results = sorted(anchor2concept(phrase['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][3] < truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][4] > truthSet[truthIndex][3]:\n",
    "                # overlap with mine behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][3] == truthSet[truthIndex][3]:\n",
    "            # same mention (same start atleast)\n",
    "            if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][3] > truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][3] < truthSet[truthIndex][4]:\n",
    "                # overlap with truth behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are.\n",
    "        mySet: My code's output for what it thinks the right entities are.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][3] < truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][4] > truthSet[truthIndex][3]:\n",
    "                # overlap with mine behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][3] == truthSet[truthIndex][3]:\n",
    "            # same mention (same start atleast)\n",
    "            if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][3] > truthSet[truthIndex][3]:\n",
    "            if mySet[myIndex][3] < truthSet[truthIndex][4]:\n",
    "                # overlap with truth behind\n",
    "                if title2id(truthSet[truthIndex][1]) == mySet[myIndex][1]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][4] < mySet[myIndex][4]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSentenceOfMention():\n",
    "    pass\n",
    "    \n",
    "def getSurroundingSentences(phrase, axis):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding sentences.\n",
    "    Return:\n",
    "        Returns the words as a list that belong to the sentence of this axis, and the surrounding\n",
    "        ones: [[w3,w4,w5],[w0,w1,w2,w6,w7,w8]]\n",
    "    \"\"\"\n",
    "    \n",
    "    frstSentenceStart = 0\n",
    "    # end of first sentence is just start of middle sentence\n",
    "    mdlSentenceStart = 0\n",
    "    mdlSentenceEnd = 0\n",
    "    # start of last sentence is just end of middle sentence\n",
    "    lstSentenceEnd = 0\n",
    "    \n",
    "    # get start index of middle sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(axis,-1,-1):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            mdlSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of middle sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(axis, len(phrase)):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            mdlSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            mdlSentenceEnd = len(phrase)\n",
    "            \n",
    "    # get start index of first sentence\n",
    "    # look back untill period or absolute start\n",
    "    for i in range(mdlSentenceStart - 2, -1, -1):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            frstSentenceStart = i + 1\n",
    "            break\n",
    "            \n",
    "    # get end index of last sentence\n",
    "    # look forward untill next period or end\n",
    "    for i in range(mdlSentenceEnd + 1, len(phrase)):\n",
    "        if phrase[i][-1] == '.' or phrase[i][-1] == '?' or phrase[i][-1] == '!':\n",
    "            lstSentenceEnd = i + 1\n",
    "            break\n",
    "        elif i == len(phrase)-1:\n",
    "            lstSentenceEnd = len(phrase)\n",
    "            \n",
    "    sentences = [phrase[mdlSentenceStart:axis]+phrase[axis+1:mdlSentenceEnd],\n",
    "                phrase[frstSentenceStart:mdlSentenceStart]+phrase[mdlSentenceEnd:lstSentenceEnd]]\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "def getSurroundingWords(phrase, axis, branchSize):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        phrase: A list of words.\n",
    "        axis: The index of the word that is the center of where to get surrounding words.\n",
    "        branchSize: The amount of words to the left and right to get.\n",
    "    Return:\n",
    "        The words as a list that surround the given axis. Expanding out branchSize elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = axis - branchSize\n",
    "    imax = axis + branchSize\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(phrase):\n",
    "        imax = len(phrase)\n",
    "        \n",
    "    # return surrounding part of word minus the axis word\n",
    "    return (phrase[imin:axis] + phrase[axis+1:imax])\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestMultiContextMatch(mention, context, contextSurround, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words in the sentence of the target.\n",
    "        contextSurround: The words in the sentences that surround the target.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put texts in right format\n",
    "    text = \" \".join(context)\n",
    "    textSurround = \" \".join(contextSurround)\n",
    "    text = escapeStringSolr(text)\n",
    "    textSurround = escapeStringSolr(textSurround)\n",
    "    mention = escapeStringSolr(mention)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    if len(contextSurround) > 0:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text.decode('string_escape')+')^1 text:('+textSurround.decode('string_escape')+')^0 title:('+mention.decode('string_escape')+')^1.35',\n",
    "                'wt':'json'}\n",
    "    else:\n",
    "        params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "                'q':'text:('+text.decode('string_escape')+') title:('+mention.decode('string_escape')+')^1.35',\n",
    "                'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestContextMatch(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that suround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    text = (\" \".join(context)).encode('utf-8')\n",
    "    text = escapeStringSolr(text)\n",
    "    mention = escapeStringSolr(mention.encode('utf-8'))\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+text.decode('string_escape')+') title:(' + mention.decode('string_escape') + ')^0.6',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "    \n",
    "def wikifyPopular(phrase, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[0], candidates[i][0][0], candidates[i][0][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "# the orginal version, with just surrounding words.\n",
    "def wikifyContexty(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding contextBranchSize words.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            context = getSurroundingWords(phrase['text'], mention[0], ctxBrchSz)\n",
    "            bestIndex = bestContextMatch(phrase['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], mention[2], mention[3]])\n",
    "        else:\n",
    "            topCandidates.append([mention[0], 0, -1, -1]) # a bad mention\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "# new version with surrounding sentences\n",
    "def wikifyContext(phrase, candidates, ctxBrchSz = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding sentences and its own\n",
    "        serving as context.\n",
    "    Args:\n",
    "        phrase: A phrase in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ctxBrchSz: How many words on both sides of a mention to search.\n",
    "    Return:\n",
    "        The word index, entity id, and entity frequency of each winning candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in phrase['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            # get the \n",
    "            contexts = getSurroundingSentences(phrase['text'], mention[0])\n",
    "            bestIndex = bestMultiContextMatch(phrase['text'][mention[0]], contexts[0], contexts[1], candidates[i])\n",
    "            topCandidates.append([mention[0], candidates[i][bestIndex][0], candidates[i][bestIndex][1], mention[3], mention[4]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(phrase, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the phrase string, and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        phrase: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        The original split text and the anchors along with their best matched concept from wikipedia.\n",
    "        Of the form: [[w1,w2,...], [[wid,entityId],...]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # words are not in pre-split form\n",
    "    if not(mentionsGiven):\n",
    "        phrase = splitWords(phrase) # modify phrase into split form\n",
    "    else:\n",
    "        phrase = mentionStartsAndEnds(phrase)\n",
    "    \n",
    "        \n",
    "    wikified = [phrase['text']] # second index with proposed entities filled later\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        phrase['mentions'] = [item for item in phrase['mentions']\n",
    "                    if  len(phrase['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(phrase, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified.append(wikifyPopular(phrase, candidates))\n",
    "    elif method == 'context':\n",
    "        wikified.append(wikifyContext(phrase, candidates, ctxBrchSz = len(phrase['text'])))\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified[1] = [item for item in wikified[1]\n",
    "                    if item[2] >= MIN_FREQUENCY]\n",
    "    \n",
    "    \"\"\"# remove duplicates\n",
    "    idsHad = [] # a list of entities to check for duplicates\n",
    "    newWikified1 = [] # to replace old wikified[1]\n",
    "    for item in wikified[1]:\n",
    "        if item[1] not in idsHad:\n",
    "            newWikified1.append(item)\n",
    "            idsHad.append(item[1])\n",
    "    wikified[1] = newWikified1\"\"\"\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "def getWiki5000Entities(annotationData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A helper method to get the entities of wiki5000 into the right form.\n",
    "    Args:\n",
    "        annotationData: The json data that has info that needs to be converted.\n",
    "    Return:\n",
    "        The entities in the usual format of [[something, entity],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    for item in json.loads(annotationData):\n",
    "        entities.append([None, item['url'].replace(' ', '_'), 0, item['from'], item['to']])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def wikilineLine(inLine):\n",
    "    \"\"\"\n",
    "    Puts the inLine in the right format if it came from wikipedia.\n",
    "    \"\"\"\n",
    "    newLine = {'text':[], 'mentions':[]}\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "methods = ['context','popular']\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        doAppend = True\n",
    "        theLine = json.loads(line.decode('utf-8').strip())\n",
    "        \n",
    "        if dataset['name'] == 'wiki5000':\n",
    "            textName = 'opening_text'\n",
    "        else:\n",
    "            textName = 'text'\n",
    "        \n",
    "        for i in \"\".join(theLine[textName]):\n",
    "            if ord(i) >= 128:\n",
    "                doAppend = False\n",
    "                break\n",
    "                \n",
    "        if doAppend:\n",
    "            dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            \n",
    "            print str(totalLines + 1)\n",
    "            \n",
    "            # different structure for wiki\n",
    "            if dataset['name'] == 'wiki5000':\n",
    "                # for unification of format for statistical testing\n",
    "                trueEntities = getWiki5000Entities(line['opening_annotation'])\n",
    "\n",
    "                resultS = None # no pre-split text\n",
    "                resultM = wikifyEval(line['opening_text'].encode('utf-8').strip(), False, method = mthd, maxC = 7)\n",
    "            else:\n",
    "                trueEntities = mentionStartsAndEnds(line)['mentions'] # the ground truth\n",
    "                \n",
    "                # original split string\n",
    "                resultS = wikifyEval(line, True, method = mthd, maxC = 7)\n",
    "                # unsplit string\n",
    "                resultM = wikifyEval((\" \".join(line['text'])).encode('utf-8').strip(), False, method = mthd, maxC = 7)\n",
    "                \n",
    "            #resultM = [[],[]]\n",
    "                \n",
    "            ## get statistical results from true entities and results S and M\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                precS = precision(trueEntities, resultS[1]) # precision of pre-split\n",
    "            else:\n",
    "                precS = 0\n",
    "                \n",
    "            precM = precision(trueEntities, resultM[1]) # precision of manual split\n",
    "            \n",
    "            # wiki5000 exception\n",
    "            if resultS <> None:\n",
    "                recS = recall(trueEntities, resultS[1]) # recall of pre-split\n",
    "            else:\n",
    "                recS = 0\n",
    "                \n",
    "            recM = recall(trueEntities, resultM[1]) # recall of manual split\n",
    "            \n",
    "            #clear_output() # delete this after\n",
    "            print str(precS) + ' ' + str(precM) + ' ' + str(recS) + ' ' + str(recM) + '\\n'\n",
    "            #print str(precS) + ' ' + str(recS)\n",
    "            \n",
    "            # track results\n",
    "            totalPrecS += precS\n",
    "            totalPrecM += precM\n",
    "            totalRecS += recS\n",
    "            totalRecM += recM\n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'Pre-Split Precision':totalPrecS/totalLines, \n",
    "                                               'Manual Split Precision':totalPrecM/totalLines,\n",
    "                                              'Pre-Split Recall':totalRecS/totalLines, \n",
    "                                               'Manual Split Recall':totalRecM/totalLines}\n",
    "            \n",
    "print performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test individual text on wikification.\n",
    "\"\"\"\n",
    "\n",
    "data = json.loads(\"\"\"{\"text\": [\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"], \"mentions\": [[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]}\n",
    "\"\"\".decode('utf-8').strip())\n",
    "\n",
    "print str(data) + '\\n'\n",
    "\n",
    "print \" \".join(data['text']).encode('utf-8').strip()\n",
    "\n",
    "#results = wikifyEval(data['text'], True, 'popular', True)\n",
    "results = wikifyEval(\" \".join(data['text']).encode('utf-8').strip(), False, method='popular')\n",
    "print results[0]\n",
    "for result in results[1]:\n",
    "    print id2title(result[1])\n",
    "\n",
    "prec = precision(data['mentions'], results[1])\n",
    "rec = recall(data['mentions'], results[1])\n",
    "\n",
    "print '\\nprecision: ' + str(prec) + ', rec: ' + str(rec) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wikify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-21a4dde0fa36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mention'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-->'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wikiTitle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wikify' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is for testing if the wikification works.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "phrase = 'Three of the greatest guitarists started their career in a single band : Clapton , Beck , and Page'\n",
    "print phrase + \"\\n\"\n",
    "\n",
    "anchors = wikify(phrase, False)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "\n",
    "anchors = wikify(phrase, True)\n",
    "for anchor in anchors:\n",
    "    print anchor['mention'] + '-->' + anchor['wikiTitle']\n",
    "    \n",
    "print\n",
    "    \n",
    "newText = \"\"\n",
    "\n",
    "anchors = sorted(anchors, key=itemgetter('start')) # make sure anchors are sorted\n",
    "anchorIndex = 0 # keep track of current anchor added\n",
    "i = 0 \n",
    "while i < len(phrase):\n",
    "    if anchorIndex < len(anchors) and i == anchors[anchorIndex]['start']:\n",
    "        anchor = anchors[anchorIndex]\n",
    "        newText += (\"<a href=\\\"https://en.wikipedia.org/wiki/\" + anchor['wikiTitle']\n",
    "                   + \"\\\" target=\\\"_blank\\\">\" + anchor['mention'] + \"</a>\")\n",
    "        i = anchors[anchorIndex]['end']\n",
    "        anchorIndex += 1\n",
    "    else:\n",
    "        newText += phrase[i]\n",
    "        i += 1\n",
    "    \n",
    "display(HTML(newText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideas:\n",
    "    -In wikifyContext make the current sentence worth 1 and each surrounding sentence worth 0.5.\n",
    "    -anchor frequency adjuster\n",
    "    -use similarity with other anchors\n",
    "\n",
    "Sample Querries:\n",
    "    'I walked down to the park and found a duck and a pebble'\n",
    "    'I walked into an electronic store and bought a pebble'\n",
    "    'I walked down to the park and found a duck studying quantum mechanics'\n",
    "    'I walked down to the park and found a duck studying quantum mechanical systems'\n",
    "    'I met David in Spain'\n",
    "    'An entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = sorted(anchor2concept(\"David Edgar\"), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "\n",
    "for tmpp in tmp:\n",
    "    print 'id: ' + str(tmpp[0]) + ', title: ' + id2title(tmpp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voller presidential preferences How will American voters compensate in the next search for a president? WASHINGTON - Now that the 38th president has been laid to rest, the capital can take up the main business of 2007: trying to figure out who will be the 44th. What type of leader does the country want? Here is my sense of it, based on talking to politicians, strategists and voters here and around the nation. No ideologues, please There was a time when President George W. Bush 's ideological certitude was politically appealing and perhaps functionally necessary. That time has long since passed. The country is tired, even fearful, of leaders with fervent beliefs that seem impervious to new (or even old) facts. Voters see the war in Iraq as an \"idea,\" not a solution - and Americans do not like ideas that do not work. Voters likely will view Bush 's \"surge\" of troops into Iraq as new evidence of failure, and the dangers of a leader who depends on preconceived ideas. Serious student Presidential elections are a never-ending series of mid-course corrections. Voters look to compensate for the leadership weaknesses of the incumbent. An example comes from the life and career of Gerald Ford . In 1976, voters wanted a pure antidote to Richard Nixon 's paranoid megalomania. Once Ford pardoned Nixon , he could not be that candidate. Instead, Americans chose Jimmy Carter , a peanut farmer who had never worked in Washington , and who promised never to lie to the American people . The counterpoint thinking continues. Voters in 2008 are going to want someone who prides himself (or herself) on spending time in the library - who has a hands-on curiosity about the details. Washington experience not necessary Voters these days not only do not value Washington experience - or any office-holding experience - it can make them suspicious. That is what strategists and polltakers for Sen. Evan Bayh found when they studied whether he should run for president. They found that his remarkably deep resume - the son of a senator, he was the \"boy governor\" of Indiana before going to the Senate - was as handicap. Americans always are dubious about the capital, but that sentiment seems particularly strong. Bayh decided not to run. \"` Washington ' doesn't make the case,\" said Dan Pfeiffer , who worked for Bayh . No more boomer obsessions Not all elections are about change, but 2008 will be. Americans are moderately upbeat about the country's prospects, but deeply worried about the world - and they have come to realize that they can't separate one from the other. One thing for sure, says Pfeiffer , voters are tired of arguing about the culture of the 1960s and other Boomer issues. \"There is a sense that the 2004 election was too much about who did or did not do what in Vietnam ,\" said Pfeiffer , referring to the Bush campaign against Sen. John Kerry . In 2000, Bush won in part by selling himself as a \"grown up\" Boomer answer to Bill Clinton . \"Voters are tired of that era and its concerns,\" said Pfeiffer said. \"They want to move on.\" Know the middle class Bushes have a congenital family problem with this, and it leaves an opening for someone - of either party - who can prove that he or she really understands the strains of middle class life. It's not just about money, but about cultural assaults and the lack of time for family in an era when both parents or partners need to work. In his forthcoming book, Positively American, Sen. Charles Schumer of New York imagines the hard life of a fictitious middle class family - and offers a series of governmental proposals to address them. A shrewd student of the American mood, Schumer is aiming in the right direction. The next president will need to show that he or she understands that family.\n",
      "[[u'Voller', u'presidential', u'preferences', u'How', u'will', u'American', u'voters', u'compensate', u'in', u'the', u'next', u'search', u'for', u'a', u'president?', u'WASHINGTON', u'-', u'Now', u'that', u'the', u'38th', u'president', u'has', u'been', u'laid', u'to', u'rest,', u'the', u'capital', u'can', u'take', u'up', u'the', u'main', u'business', u'of', u'2007:', u'trying', u'to', u'figure', u'out', u'who', u'will', u'be', u'the', u'44th.', u'What', u'type', u'of', u'leader', u'does', u'the', u'country', u'want?', u'Here', u'is', u'my', u'sense', u'of', u'it,', u'based', u'on', u'talking', u'to', u'politicians,', u'strategists', u'and', u'voters', u'here', u'and', u'around', u'the', u'nation.', u'No', u'ideologues,', u'please', u'There', u'was', u'a', u'time', u'when', u'President George W. Bush', u\"'s\", u'ideological', u'certitude', u'was', u'politically', u'appealing', u'and', u'perhaps', u'functionally', u'necessary.', u'That', u'time', u'has', u'long', u'since', u'passed.', u'The', u'country', u'is', u'tired,', u'even', u'fearful,', u'of', u'leaders', u'with', u'fervent', u'beliefs', u'that', u'seem', u'impervious', u'to', u'new', u'(or', u'even', u'old)', u'facts.', u'Voters', u'see', u'the', u'war', u'in', u'Iraq', u'as', u'an', u'\"idea,\"', u'not', u'a', u'solution', u'-', u'and', u'Americans', u'do', u'not', u'like', u'ideas', u'that', u'do', u'not', u'work.', u'Voters', u'likely', u'will', u'view', u'Bush', u\"'s\", u'\"surge\"', u'of', u'troops', u'into', u'Iraq', u'as', u'new', u'evidence', u'of', u'failure,', u'and', u'the', u'dangers', u'of', u'a', u'leader', u'who', u'depends', u'on', u'preconceived', u'ideas.', u'Serious', u'student', u'Presidential', u'elections', u'are', u'a', u'never-ending', u'series', u'of', u'mid-course', u'corrections.', u'Voters', u'look', u'to', u'compensate', u'for', u'the', u'leadership', u'weaknesses', u'of', u'the', u'incumbent.', u'An', u'example', u'comes', u'from', u'the', u'life', u'and', u'career', u'of', u'Gerald Ford', u'.', u'In', u'1976,', u'voters', u'wanted', u'a', u'pure', u'antidote', u'to', u'Richard Nixon', u\"'s\", u'paranoid', u'megalomania.', u'Once', u'Ford', u'pardoned', u'Nixon', u',', u'he', u'could', u'not', u'be', u'that', u'candidate.', u'Instead,', u'Americans', u'chose', u'Jimmy Carter', u',', u'a', u'peanut', u'farmer', u'who', u'had', u'never', u'worked', u'in', u'Washington', u',', u'and', u'who', u'promised', u'never', u'to', u'lie', u'to', u'the', u'American people', u'.', u'The', u'counterpoint', u'thinking', u'continues.', u'Voters', u'in', u'2008', u'are', u'going', u'to', u'want', u'someone', u'who', u'prides', u'himself', u'(or', u'herself)', u'on', u'spending', u'time', u'in', u'the', u'library', u'-', u'who', u'has', u'a', u'hands-on', u'curiosity', u'about', u'the', u'details.', u'Washington', u'experience', u'not', u'necessary', u'Voters', u'these', u'days', u'not', u'only', u'do', u'not', u'value', u'Washington', u'experience', u'-', u'or', u'any', u'office-holding', u'experience', u'-', u'it', u'can', u'make', u'them', u'suspicious.', u'That', u'is', u'what', u'strategists', u'and', u'polltakers', u'for', u'Sen.', u'Evan Bayh', u'found', u'when', u'they', u'studied', u'whether', u'he', u'should', u'run', u'for', u'president.', u'They', u'found', u'that', u'his', u'remarkably', u'deep', u'resume', u'-', u'the', u'son', u'of', u'a', u'senator,', u'he', u'was', u'the', u'\"boy', u'governor\"', u'of', u'Indiana', u'before', u'going', u'to', u'the', u'Senate', u'-', u'was', u'as', u'handicap.', u'Americans', u'always', u'are', u'dubious', u'about', u'the', u'capital,', u'but', u'that', u'sentiment', u'seems', u'particularly', u'strong.', u'Bayh', u'decided', u'not', u'to', u'run.', u'\"`', u'Washington', u\"'\", u\"doesn't\", u'make', u'the', u'case,\"', u'said', u'Dan Pfeiffer', u',', u'who', u'worked', u'for', u'Bayh', u'.', u'No', u'more', u'boomer', u'obsessions', u'Not', u'all', u'elections', u'are', u'about', u'change,', u'but', u'2008', u'will', u'be.', u'Americans', u'are', u'moderately', u'upbeat', u'about', u'the', u\"country's\", u'prospects,', u'but', u'deeply', u'worried', u'about', u'the', u'world', u'-', u'and', u'they', u'have', u'come', u'to', u'realize', u'that', u'they', u\"can't\", u'separate', u'one', u'from', u'the', u'other.', u'One', u'thing', u'for', u'sure,', u'says', u'Pfeiffer', u',', u'voters', u'are', u'tired', u'of', u'arguing', u'about', u'the', u'culture', u'of', u'the', u'1960s', u'and', u'other', u'Boomer', u'issues.', u'\"There', u'is', u'a', u'sense', u'that', u'the', u'2004', u'election', u'was', u'too', u'much', u'about', u'who', u'did', u'or', u'did', u'not', u'do', u'what', u'in', u'Vietnam', u',\"', u'said', u'Pfeiffer', u',', u'referring', u'to', u'the', u'Bush campaign', u'against', u'Sen.', u'John Kerry', u'.', u'In', u'2000,', u'Bush', u'won', u'in', u'part', u'by', u'selling', u'himself', u'as', u'a', u'\"grown', u'up\"', u'Boomer', u'answer', u'to', u'Bill Clinton', u'.', u'\"Voters', u'are', u'tired', u'of', u'that', u'era', u'and', u'its', u'concerns,\"', u'said', u'Pfeiffer', u'said.', u'\"They', u'want', u'to', u'move', u'on.\"', u'Know', u'the', u'middle', u'class', u'Bushes', u'have', u'a', u'congenital', u'family', u'problem', u'with', u'this,', u'and', u'it', u'leaves', u'an', u'opening', u'for', u'someone', u'-', u'of', u'either', u'party', u'-', u'who', u'can', u'prove', u'that', u'he', u'or', u'she', u'really', u'understands', u'the', u'strains', u'of', u'middle', u'class', u'life.', u\"It's\", u'not', u'just', u'about', u'money,', u'but', u'about', u'cultural', u'assaults', u'and', u'the', u'lack', u'of', u'time', u'for', u'family', u'in', u'an', u'era', u'when', u'both', u'parents', u'or', u'partners', u'need', u'to', u'work.', u'In', u'his', u'forthcoming', u'book,', u'Positively', u'American,', u'Sen.', u'Charles Schumer', u'of', u'New York', u'imagines', u'the', u'hard', u'life', u'of', u'a', u'fictitious', u'middle', u'class', u'family', u'-', u'and', u'offers', u'a', u'series', u'of', u'governmental', u'proposals', u'to', u'address', u'them.', u'A', u'shrewd', u'student', u'of', u'the', u'American', u'mood,', u'Schumer', u'is', u'aiming', u'in', u'the', u'right', u'direction.', u'The', u'next', u'president', u'will', u'need', u'to', u'show', u'that', u'he', u'or', u'she', u'understands', u'that', u'family.'], [[15, 'George_Washington', 1L, 106, 116], [81, 'George_W._Bush', 94L, 459, 483], [123, 'Iraq', 13690L, 743, 747], [145, 'George_W._Bush', 413L, 853, 857], [151, 'Iraq_War', 432L, 884, 888], [199, 'Gerald_Ford', 1496L, 1191, 1202], [209, 'Richard_Nixon', 4080L, 1247, 1260], [214, 'Gerald_Ford', 100L, 1291, 1295], [216, 'Richard_Nixon', 371L, 1305, 1310], [227, 'Jimmy_Carter', 3370L, 1370, 1382], [237, 'George_Washington', 199L, 1425, 1435], [247, 'Americans', 12L, 1475, 1490], [281, 'George_Washington', 199L, 1685, 1695], [293, 'George_Washington', 199L, 1761, 1771], [314, 'Evan_Bayh', 135L, 1898, 1907], [344, 'Indiana', 11742L, 2065, 2072], [349, 'Roman_Senate', 330L, 2093, 2099], [367, 'Birch_Evans_Bayh', 1L, 2213, 2217], [373, 'George_Washington', 199L, 2241, 2251], [380, 'Daniel_Pfeiffer', 6L, 2283, 2295], [385, 'Birch_Evans_Bayh', 1L, 2313, 2317], [435, 'Pfeiffer_Falcons', 1L, 2600, 2608], [450, 'Boomer', 3L, 2680, 2686], [472, 'Vietnam', 14574L, 2785, 2792], [475, 'Pfeiffer', 2L, 2801, 2809], [480, 'Bush_campaign', 1L, 2829, 2842], [483, 'John_Kerry', 2523L, 2856, 2866], [487, 'Presidency_of_George_W._Bush', 9L, 2878, 2882], [501, 'Bill_Clinton', 6979L, 2947, 2959], [513, 'Pfeiffer', 2L, 3016, 3024], [593, 'Chuck_Schumer', 225L, 3459, 3474], [595, 'New_York', 46410L, 3478, 3486], [624, 'Schumer', 2L, 3650, 3657]]]\n"
     ]
    }
   ],
   "source": [
    "phrase = {u'text': [u'Voller', u'presidential', u'preferences', u'How', u'will', u'American', u'voters', u'compensate', u'in', u'the', u'next', u'search', u'for', u'a', u'president?', u'WASHINGTON', u'-', u'Now', u'that', u'the', u'38th', u'president', u'has', u'been', u'laid', u'to', u'rest,', u'the', u'capital', u'can', u'take', u'up', u'the', u'main', u'business', u'of', u'2007:', u'trying', u'to', u'figure', u'out', u'who', u'will', u'be', u'the', u'44th.', u'What', u'type', u'of', u'leader', u'does', u'the', u'country', u'want?', u'Here', u'is', u'my', u'sense', u'of', u'it,', u'based', u'on', u'talking', u'to', u'politicians,', u'strategists', u'and', u'voters', u'here', u'and', u'around', u'the', u'nation.', u'No', u'ideologues,', u'please', u'There', u'was', u'a', u'time', u'when', u'President George W. Bush', u\"'s\", u'ideological', u'certitude', u'was', u'politically', u'appealing', u'and', u'perhaps', u'functionally', u'necessary.', u'That', u'time', u'has', u'long', u'since', u'passed.', u'The', u'country', u'is', u'tired,', u'even', u'fearful,', u'of', u'leaders', u'with', u'fervent', u'beliefs', u'that', u'seem', u'impervious', u'to', u'new', u'(or', u'even', u'old)', u'facts.', u'Voters', u'see', u'the', u'war', u'in', u'Iraq', u'as', u'an', u'\"idea,\"', u'not', u'a', u'solution', u'-', u'and', u'Americans', u'do', u'not', u'like', u'ideas', u'that', u'do', u'not', u'work.', u'Voters', u'likely', u'will', u'view', u'Bush', u\"'s\", u'\"surge\"', u'of', u'troops', u'into', u'Iraq', u'as', u'new', u'evidence', u'of', u'failure,', u'and', u'the', u'dangers', u'of', u'a', u'leader', u'who', u'depends', u'on', u'preconceived', u'ideas.', u'Serious', u'student', u'Presidential', u'elections', u'are', u'a', u'never-ending', u'series', u'of', u'mid-course', u'corrections.', u'Voters', u'look', u'to', u'compensate', u'for', u'the', u'leadership', u'weaknesses', u'of', u'the', u'incumbent.', u'An', u'example', u'comes', u'from', u'the', u'life', u'and', u'career', u'of', u'Gerald Ford', u'.', u'In', u'1976,', u'voters', u'wanted', u'a', u'pure', u'antidote', u'to', u'Richard Nixon', u\"'s\", u'paranoid', u'megalomania.', u'Once', u'Ford', u'pardoned', u'Nixon', u',', u'he', u'could', u'not', u'be', u'that', u'candidate.', u'Instead,', u'Americans', u'chose', u'Jimmy Carter', u',', u'a', u'peanut', u'farmer', u'who', u'had', u'never', u'worked', u'in', u'Washington', u',', u'and', u'who', u'promised', u'never', u'to', u'lie', u'to', u'the', u'American people', u'.', u'The', u'counterpoint', u'thinking', u'continues.', u'Voters', u'in', u'2008', u'are', u'going', u'to', u'want', u'someone', u'who', u'prides', u'himself', u'(or', u'herself)', u'on', u'spending', u'time', u'in', u'the', u'library', u'-', u'who', u'has', u'a', u'hands-on', u'curiosity', u'about', u'the', u'details.', u'Washington', u'experience', u'not', u'necessary', u'Voters', u'these', u'days', u'not', u'only', u'do', u'not', u'value', u'Washington', u'experience', u'-', u'or', u'any', u'office-holding', u'experience', u'-', u'it', u'can', u'make', u'them', u'suspicious.', u'That', u'is', u'what', u'strategists', u'and', u'polltakers', u'for', u'Sen.', u'Evan Bayh', u'found', u'when', u'they', u'studied', u'whether', u'he', u'should', u'run', u'for', u'president.', u'They', u'found', u'that', u'his', u'remarkably', u'deep', u'resume', u'-', u'the', u'son', u'of', u'a', u'senator,', u'he', u'was', u'the', u'\"boy', u'governor\"', u'of', u'Indiana', u'before', u'going', u'to', u'the', u'Senate', u'-', u'was', u'as', u'handicap.', u'Americans', u'always', u'are', u'dubious', u'about', u'the', u'capital,', u'but', u'that', u'sentiment', u'seems', u'particularly', u'strong.', u'Bayh', u'decided', u'not', u'to', u'run.', u'\"`', u'Washington', u\"'\", u\"doesn't\", u'make', u'the', u'case,\"', u'said', u'Dan Pfeiffer', u',', u'who', u'worked', u'for', u'Bayh', u'.', u'No', u'more', u'boomer', u'obsessions', u'Not', u'all', u'elections', u'are', u'about', u'change,', u'but', u'2008', u'will', u'be.', u'Americans', u'are', u'moderately', u'upbeat', u'about', u'the', u\"country's\", u'prospects,', u'but', u'deeply', u'worried', u'about', u'the', u'world', u'-', u'and', u'they', u'have', u'come', u'to', u'realize', u'that', u'they', u\"can't\", u'separate', u'one', u'from', u'the', u'other.', u'One', u'thing', u'for', u'sure,', u'says', u'Pfeiffer', u',', u'voters', u'are', u'tired', u'of', u'arguing', u'about', u'the', u'culture', u'of', u'the', u'1960s', u'and', u'other', u'Boomer', u'issues.', u'\"There', u'is', u'a', u'sense', u'that', u'the', u'2004', u'election', u'was', u'too', u'much', u'about', u'who', u'did', u'or', u'did', u'not', u'do', u'what', u'in', u'Vietnam', u',\"', u'said', u'Pfeiffer', u',', u'referring', u'to', u'the', u'Bush campaign', u'against', u'Sen.', u'John Kerry', u'.', u'In', u'2000,', u'Bush', u'won', u'in', u'part', u'by', u'selling', u'himself', u'as', u'a', u'\"grown', u'up\"', u'Boomer', u'answer', u'to', u'Bill Clinton', u'.', u'\"Voters', u'are', u'tired', u'of', u'that', u'era', u'and', u'its', u'concerns,\"', u'said', u'Pfeiffer', u'said.', u'\"They', u'want', u'to', u'move', u'on.\"', u'Know', u'the', u'middle', u'class', u'Bushes', u'have', u'a', u'congenital', u'family', u'problem', u'with', u'this,', u'and', u'it', u'leaves', u'an', u'opening', u'for', u'someone', u'-', u'of', u'either', u'party', u'-', u'who', u'can', u'prove', u'that', u'he', u'or', u'she', u'really', u'understands', u'the', u'strains', u'of', u'middle', u'class', u'life.', u\"It's\", u'not', u'just', u'about', u'money,', u'but', u'about', u'cultural', u'assaults', u'and', u'the', u'lack', u'of', u'time', u'for', u'family', u'in', u'an', u'era', u'when', u'both', u'parents', u'or', u'partners', u'need', u'to', u'work.', u'In', u'his', u'forthcoming', u'book,', u'Positively', u'American,', u'Sen.', u'Charles Schumer', u'of', u'New York', u'imagines', u'the', u'hard', u'life', u'of', u'a', u'fictitious', u'middle', u'class', u'family', u'-', u'and', u'offers', u'a', u'series', u'of', u'governmental', u'proposals', u'to', u'address', u'them.', u'A', u'shrewd', u'student', u'of', u'the', u'American', u'mood,', u'Schumer', u'is', u'aiming', u'in', u'the', u'right', u'direction.', u'The', u'next', u'president', u'will', u'need', u'to', u'show', u'that', u'he', u'or', u'she', u'understands', u'that', u'family.'], u'mentions': [[15, u'Washington,_D.C.', 0, 106, 116], [81, u'George_W._Bush', 0, 459, 483], [123, u'Iraq', 0, 743, 747], [145, u'George_W._Bush', 0, 853, 857], [151, u'Iraq', 0, 884, 888], [199, u'Gerald_Ford', 0, 1191, 1202], [209, u'Richard_Nixon', 0, 1247, 1260], [214, u'Gerald_Ford', 0, 1291, 1295], [216, u'Richard_Nixon', 0, 1305, 1310], [227, u'Jimmy_Carter', 0, 1370, 1382], [237, u'Washington,_D.C.', 0, 1425, 1435], [247, u'Demographics_of_the_United_States', 0, 1475, 1490], [281, u'Washington,_D.c.', 0, 1685, 1695], [293, u'Washington,_D.c.', 0, 1761, 1771], [314, u'Evan_Bayh', 0, 1898, 1907], [344, u'Indiana', 0, 2065, 2072], [349, u'United_States_Senate', 0, 2093, 2099], [367, u'Evan_Bayh', 0, 2213, 2217], [373, u'Washington,_D.C.', 0, 2241, 2251], [380, u'Dan_Pfeiffer', 0, 2283, 2295], [385, u'Evan_Bayh', 0, 2313, 2317], [435, u'Dan_Pfeiffer', 0, 2600, 2608], [450, u'Boomer', 0, 2680, 2686], [472, u'Vietnam_War', 0, 2785, 2792], [475, u'Dan_Pfeiffer', 0, 2801, 2809], [480, u'George_W._Bush_presidential_campaign,_2004', 0, 2829, 2842], [483, u'John_Kerry', 0, 2856, 2866], [487, u'George_W._Bush', 0, 2878, 2882], [501, u'Bill_Clinton', 0, 2947, 2959], [513, u'Dan_Pfeiffer', 0, 3016, 3024], [593, u'Charles_Schumer', 0, 3459, 3474], [595, u'New_York', 0, 3478, 3486], [624, u'Charles_Schumer', 0, 3650, 3657]]}\n",
    "wikified = [phrase['text']]\n",
    "cands = generateCandidates(phrase, 7)\n",
    "wikified.append(wikifyContext(phrase, cands, ctxBrchSz = len(phrase['text'])))\n",
    "\n",
    "for mention in wikified[1]:\n",
    "    mention[1] = id2title(mention[1])\n",
    "    \n",
    "print (\" \".join(wikified[0])).encode('utf-8').strip()\n",
    "print wikified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(33509L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "text = \" \".join([\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"])\n",
    "print text\n",
    "\n",
    "text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "text = text.replace('+', r'\\+')\n",
    "text = text.replace(\"-\", \"\\-\")\n",
    "text = text.replace(\"&&\", \"\\&&\")\n",
    "text = text.replace(\"||\", \"\\||\")\n",
    "text = text.replace(\"!\", \"\\!\")\n",
    "text = text.replace(\"(\", \"\\(\")\n",
    "text = text.replace(\")\", \"\\)\")\n",
    "text = text.replace(\"{\", \"\\{\")\n",
    "text = text.replace(\"}\", \"\\}\")\n",
    "text = text.replace(\"[\", \"\\[\")\n",
    "text = text.replace(\"]\", \"\\]\")\n",
    "text = text.replace(\"^\", \"\\^\")\n",
    "text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "text = text.replace(\"~\", \"\\~\")\n",
    "text = text.replace(\"*\", \"\\*\")\n",
    "text = text.replace(\"?\", \"\\?\")\n",
    "text = text.replace(\":\", \"\\:\")\n",
    "\n",
    "text = text.decode('string_escape')\n",
    "\n",
    "print text + '\\n\\n'\n",
    "\n",
    "addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "r = requests.post(addr, params=params, data=text)\n",
    "textData = r.json()['tags']\n",
    "\n",
    "print textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phraseData = {\"text\": [\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"], \"mentions\": [[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]}\n",
    "print str(phraseData) + '\\n'\n",
    "phraseData = mentionStartsAndEnds(phraseData)\n",
    "print phraseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hermann_Joseph_Muller\n",
      "Müller_(lunar_crater)\n",
      "Müller_(footballer)\n",
      "Muller\n",
      "Müller_(company)\n",
      "Heiner_Müller\n",
      "Müller_(surname)\n",
      "Harold_Muller\n",
      "Georg_Elias_Müller\n",
      "Cornelius_Herman_Muller\n",
      "Muller_automaton\n",
      "Muller_(restaurant)\n",
      "Müller_(German_trade_company)\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(anchor2concept('Muller'), key=itemgetter(1), reverse = True):\n",
    "    print id2title(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
