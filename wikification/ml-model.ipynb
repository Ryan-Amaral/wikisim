{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model-create.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model-create.py \n",
    "\n",
    "\"\"\"\n",
    "Train model and everything here in a script because ssh and jupyter are failing me.\n",
    "\"\"\"\n",
    "\n",
    "allX = []\n",
    "allY = []\n",
    "allMId = []\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainMId = []\n",
    "valiX = []\n",
    "valiY = []\n",
    "valiMId = []\n",
    "testX = []\n",
    "testY = []\n",
    "testMId = []\n",
    "\n",
    "linesToUse = 1000000 # limit amount of total data\n",
    "totalLines = 0\n",
    "# first try with just getting all data\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-5000.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        totalLines += 1\n",
    "        if totalLines > linesToUse:\n",
    "            break\n",
    "        data = line.split(',')\n",
    "        allX.append([float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])])\n",
    "        allY.append(int(data[1]))\n",
    "        allMId.append(long(data[7]))\n",
    "        \n",
    "# split 75, 25\n",
    "trainLines = int(totalLines * 0.75)\n",
    "valiLines = int(totalLines * 0.0)\n",
    "testLines = int(totalLines * 0.25)\n",
    "\n",
    "for i in range(0, trainLines):\n",
    "    trainX.append(allX[i])\n",
    "    trainY.append(allY[i])\n",
    "    trainMId.append(allMId[i])\n",
    "\n",
    "for i in range(trainLines, trainLines + valiLines):\n",
    "    valiX.append(allX[i])\n",
    "    valiY.append(allY[i])\n",
    "    valiMId.append(allMId[i])\n",
    "    \n",
    "for i in range(trainLines + valiLines, trainLines + valiLines + testLines):\n",
    "    testX.append(allX[i])\n",
    "    testY.append(allY[i])\n",
    "    testMId.append(allMId[i])\n",
    "    \n",
    "print len(trainX)\n",
    "print len(testX)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "etr.fit(trainX, trainY)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "rfr.fit(trainX, trainY)\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=300, max_depth=3, learning_rate=0.1, loss='ls', random_state=1)\n",
    "gbr.fit(trainX, trainY)\n",
    "\n",
    "lmart = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "lmart.fit(trainX, trainY, trainMId)\n",
    "\n",
    "\"\"\"\n",
    "Save the model.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(etr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-etr-2.pkl', 'wb'))\n",
    "pickle.dump(rfr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-rfr-2.pkl', 'wb'))\n",
    "pickle.dump(gbr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbr-2.pkl', 'wb'))\n",
    "pickle.dump(lmart, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 170097\n",
      "56699\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell is to get all the data for the ml model\n",
    "\"\"\"\n",
    "\n",
    "allX = []\n",
    "allY = []\n",
    "allMId = []\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainMId = []\n",
    "valiX = []\n",
    "valiY = []\n",
    "valiMId = []\n",
    "testX = []\n",
    "testY = []\n",
    "testMId = []\n",
    "\n",
    "linesToUse = 1000000 # limit amount of total data\n",
    "totalLines = 0\n",
    "# first try with just getting all data\n",
    "with open('/users/cs/amaral/wikisim/wikification/learning-data/el-5000.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        totalLines += 1\n",
    "        if totalLines > linesToUse:\n",
    "            break\n",
    "        data = line.split(',')\n",
    "        allX.append([float(data[2]), float(data[3]), float(data[4]), float(data[5]), float(data[6])])\n",
    "        allY.append(int(data[1]))\n",
    "        allMId.append(long(data[7]))\n",
    "        \n",
    "# split 75, 25\n",
    "trainLines = int(totalLines * 0.75)\n",
    "valiLines = int(totalLines * 0.0)\n",
    "testLines = int(totalLines * 0.25)\n",
    "\n",
    "for i in range(0, trainLines):\n",
    "    trainX.append(allX[i])\n",
    "    trainY.append(allY[i])\n",
    "    trainMId.append(allMId[i])\n",
    "\n",
    "for i in range(trainLines, trainLines + valiLines):\n",
    "    valiX.append(allX[i])\n",
    "    valiY.append(allY[i])\n",
    "    valiMId.append(allMId[i])\n",
    "    \n",
    "for i in range(trainLines + valiLines, trainLines + valiLines + testLines):\n",
    "    testX.append(allX[i])\n",
    "    testY.append(allY[i])\n",
    "    testMId.append(allMId[i])\n",
    "    \n",
    "print len(trainX)\n",
    "print len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score    Remaining                           Monitor Output \n",
      "    1       0.8987       88.16m                                         \n",
      "    2       0.9178       86.97m                                         \n",
      "    3       0.9288       86.27m                                         \n",
      "    4       0.9290       85.70m                                         \n",
      "    5       0.9356       85.35m                                         \n",
      "    6       0.9362       84.96m                                         \n",
      "    7       0.9387       84.65m                                         \n",
      "    8       0.9411       84.34m                                         \n",
      "    9       0.9412       84.02m                                         \n",
      "   10       0.9414       83.73m                                         \n",
      "   15       0.9439       82.20m                                         \n",
      "   20       0.9479       80.73m                                         \n",
      "   25       0.9519       79.28m                                         \n",
      "   30       0.9546       77.82m                                         \n",
      "   35       0.9556       76.38m                                         \n",
      "   40       0.9571       74.97m                                         \n",
      "   45       0.9580       73.55m                                         \n",
      "   50       0.9585       72.14m                                         \n"
     ]
    }
   ],
   "source": [
    "\"\"\" This and other cells helped by: https://github.com/ogrisel/notebooks/blob/master/Learning%20to%20Rank.ipynb\n",
    "This cell is to train the model.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "#etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "#etr.fit(trainX, trainY)\n",
    "\n",
    "#rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "#rfr.fit(trainX, trainY)\n",
    "\n",
    "#gbr = GradientBoostingRegressor(n_estimators=300, max_depth=3, learning_rate=0.1, loss='ls', random_state=1)\n",
    "#gbr.fit(trainX, trainY)\n",
    "\n",
    "#gbc = GradientBoostingClassifier(n_estimators=200, min_samples_split=5, random_state=1)\n",
    "#gbc.fit(trainX, trainY)\n",
    "\n",
    "lmart = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "lmart.fit(trainX, trainY, trainMId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Regressor:\n",
      "R^2 Score: 0.843993888891\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6134 0.993199481865 \n",
      "FP: 8991 0.17795855353 \n",
      "TN: 41532 0.82204144647 \n",
      "FN: 42 0.00680051813472\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 5993 0.970369170984 \n",
      "FP: 1909 0.0377847712923 \n",
      "TN: 48614 0.962215228708 \n",
      "FN: 183 0.0296308290155\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5897 0.954825129534 \n",
      "FP: 1204 0.0238307305584 \n",
      "TN: 49319 0.976169269442 \n",
      "FN: 279 0.0451748704663\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5788 0.937176165803 \n",
      "FP: 878 0.0173782237793 \n",
      "TN: 49645 0.982621776221 \n",
      "FN: 388 0.0628238341969\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5679 0.919527202073 \n",
      "FP: 667 0.0132019080419 \n",
      "TN: 49856 0.986798091958 \n",
      "FN: 497 0.0804727979275\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5533 0.895887305699 \n",
      "FP: 496 0.0098173109277 \n",
      "TN: 50027 0.990182689072 \n",
      "FN: 643 0.104112694301\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5372 0.86981865285 \n",
      "FP: 374 0.00740256912693 \n",
      "TN: 50149 0.992597430873 \n",
      "FN: 804 0.13018134715\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5177 0.838244818653 \n",
      "FP: 270 0.00534410070661 \n",
      "TN: 50253 0.994655899293 \n",
      "FN: 999 0.161755181347\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4894 0.792422279793 \n",
      "FP: 174 0.00344397601093 \n",
      "TN: 50349 0.996556023989 \n",
      "FN: 1282 0.207577720207\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4462 0.722474093264 \n",
      "FP: 91 0.00180115986778 \n",
      "TN: 50432 0.998198840132 \n",
      "FN: 1714 0.277525906736\n",
      "\n",
      "\n",
      "Random Forest Regressor:\n",
      "R^2 Score: 0.84456202897\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6122 0.991256476684 \n",
      "FP: 6381 0.126298913366 \n",
      "TN: 44142 0.873701086634 \n",
      "FN: 54 0.00874352331606\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 5994 0.970531088083 \n",
      "FP: 1866 0.0369336737723 \n",
      "TN: 48657 0.963066326228 \n",
      "FN: 182 0.0294689119171\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5895 0.954501295337 \n",
      "FP: 1227 0.0242859687667 \n",
      "TN: 49296 0.975714031233 \n",
      "FN: 281 0.0454987046632\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5796 0.938471502591 \n",
      "FP: 883 0.0174771886072 \n",
      "TN: 49640 0.982522811393 \n",
      "FN: 380 0.0615284974093\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5683 0.920174870466 \n",
      "FP: 659 0.0130435643172 \n",
      "TN: 49864 0.986956435683 \n",
      "FN: 493 0.0798251295337\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5540 0.897020725389 \n",
      "FP: 503 0.00995586168676 \n",
      "TN: 50020 0.990044138313 \n",
      "FN: 636 0.102979274611\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5393 0.873218911917 \n",
      "FP: 381 0.00754111988599 \n",
      "TN: 50142 0.992458880114 \n",
      "FN: 783 0.126781088083\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5185 0.83954015544 \n",
      "FP: 271 0.00536389367219 \n",
      "TN: 50252 0.994636106328 \n",
      "FN: 991 0.16045984456\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4902 0.79371761658 \n",
      "FP: 166 0.00328563228629 \n",
      "TN: 50357 0.996714367714 \n",
      "FN: 1274 0.20628238342\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4490 0.727007772021 \n",
      "FP: 101 0.00199908952358 \n",
      "TN: 50422 0.998000910476 \n",
      "FN: 1686 0.272992227979\n",
      "\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "R^2 Score: 0.848231559571\n",
      "\n",
      "BDB =  0.0 \n",
      "TP: 6157 0.99692357513 \n",
      "FP: 26752 0.529501415197 \n",
      "TN: 23771 0.470498584803 \n",
      "FN: 19 0.00307642487047\n",
      "\n",
      "BDB =  0.1 \n",
      "TP: 6012 0.973445595855 \n",
      "FP: 1951 0.0386160758466 \n",
      "TN: 48572 0.961383924153 \n",
      "FN: 164 0.0265544041451\n",
      "\n",
      "BDB =  0.2 \n",
      "TP: 5894 0.954339378238 \n",
      "FP: 1210 0.0239494883518 \n",
      "TN: 49313 0.976050511648 \n",
      "FN: 282 0.0456606217617\n",
      "\n",
      "BDB =  0.3 \n",
      "TP: 5794 0.938147668394 \n",
      "FP: 864 0.0171011222611 \n",
      "TN: 49659 0.982898877739 \n",
      "FN: 382 0.0618523316062\n",
      "\n",
      "BDB =  0.4 \n",
      "TP: 5679 0.919527202073 \n",
      "FP: 629 0.0124497753498 \n",
      "TN: 49894 0.98755022465 \n",
      "FN: 497 0.0804727979275\n",
      "\n",
      "BDB =  0.5 \n",
      "TP: 5551 0.898801813472 \n",
      "FP: 487 0.00963917423748 \n",
      "TN: 50036 0.990360825763 \n",
      "FN: 625 0.101198186528\n",
      "\n",
      "BDB =  0.6 \n",
      "TP: 5412 0.876295336788 \n",
      "FP: 356 0.00704629574649 \n",
      "TN: 50167 0.992953704254 \n",
      "FN: 764 0.123704663212\n",
      "\n",
      "BDB =  0.7 \n",
      "TP: 5201 0.842130829016 \n",
      "FP: 253 0.00500762029175 \n",
      "TN: 50270 0.994992379708 \n",
      "FN: 975 0.157869170984\n",
      "\n",
      "BDB =  0.8 \n",
      "TP: 4882 0.790479274611 \n",
      "FP: 144 0.00285018704352 \n",
      "TN: 50379 0.997149812956 \n",
      "FN: 1294 0.209520725389\n",
      "\n",
      "BDB =  0.9 \n",
      "TP: 4279 0.692843264249 \n",
      "FP: 73 0.00144488648734 \n",
      "TN: 50450 0.998555113513 \n",
      "FN: 1897 0.307156735751\n",
      "\n",
      "\n",
      "Gradient Boosting Classifier:\n",
      "R^2 Score: 0.980017284255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell tells the accuracy of the model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = etr\n",
    "print 'Extra Trees Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = rfr\n",
    "print 'Random Forest Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = gbr\n",
    "print 'Gradient Boosting Regressor:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    printEval(model, testX, testY, i)\n",
    "    print\n",
    "print\n",
    "\n",
    "model = gbc\n",
    "print 'Gradient Boosting Classifier:'\n",
    "print 'R^2 Score:', model.score(testX, testY)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def printEval(model, X, y, bdb = 0.5):\n",
    "    predY = model.predict(X)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if predY[i] > bdb and y[i] == 1:\n",
    "            tp += 1\n",
    "        elif predY[i] > bdb and y[i] == 0:\n",
    "            fp += 1\n",
    "        elif predY[i] <= bdb and y[i] == 1:\n",
    "            fn += 1\n",
    "        elif predY[i] <= bdb and y[i] == 0:\n",
    "            tn += 1\n",
    "            \n",
    "    print 'BDB = ', bdb, '\\nTP:', tp, tp/(tp+fn), '\\nFP:', fp, fp/(fp+tn), '\\nTN:', tn, tn/(fp+tn), '\\nFN:', fn, fn/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the model.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "#pickle.dump(etr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-etr-1.pkl', 'wb'))\n",
    "#pickle.dump(rfr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-rfr-1.pkl', 'wb'))\n",
    "#pickle.dump(gbr, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbr-1.pkl', 'wb'))\n",
    "#pickle.dump(gbc, open('/users/cs/amaral/wikisim/wikification/ml-models/model-gbc-1.pkl', 'wb'))\n",
    "#pickle.dump(lmart, open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.38906408, -0.38142295,  0.14399559, -1.21259834,  6.16869182,\n",
       "        1.39566366,  6.07528854,  5.27429442, -5.77873421, -3.68008578,\n",
       "       -7.37840903, -6.02215263, -4.83235341, -5.99034009, -4.64082089,\n",
       "       -6.54484845, -3.38575076, -5.05438867])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "\n",
    "model = pickle.load(open('/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-1.pkl', 'rb'))\n",
    "\n",
    "model.predict(testX[2:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16, 0.37197801830625504, 0.75, 0.0, 0.8058462601912664], [0.12, 0.12183860865881956, 0.0, 0.0, 0.003000814788764128], [0.08, 0.15388780453451467, 0.0, 0.0, 0.0197314194384961], [0.04, 0.12183860865881956, 0.0, 0.0, 0.00597341299761156], [0.6666666666666666, 0.5524107765948493, 0.75, 0.0, 0.6424277609623221], [0.2222222222222222, 0.43160739972500395, 0.0, 0.0, 0.04122347126384451], [0.9875, 0.9595134734099676, 0.9090909090909091, 0.25918609576466367, 0.9999999999999998], [0.9585703450891164, 0.1346076964691621, 0.9090909090909091, 0.265528050509531, 0.6452690222713563], [0.010586525091644546, 0.0, 0.0, 0.13257166946600463, 0.0008528990630150002], [0.006415118189862217, 0.07584652897571567, 0.0, 0.18615200157548295, 0.021349869684028633], [0.004361016306408798, 0.0, 0.0, 0.13820647183391077, 0.0], [0.0034445708507141954, 0.0, 0.0, 0.2197684333582972, 0.001374533158081892], [0.002844141069397042, 0.06692287344969088, 0.0, 0.1855829880405545, 0.0038656172984994353], [0.0027177347996460623, 0.0, 0.0, 0.12775189841427181, 0.0008678591415569592], [0.002085703450891164, 0.0741983070523882, 0.0, 0.1666364333953726, 0.014134646571460907], [0.0015800783718872456, 0.0, 0.0, 0.13560885982084758, 0.0010101363516014095], [0.0011692579951965618, 0.13443111220948226, 0.0, 0.1846330840700947, 0.07855439768226724], [0.0009796485905700922, 0.0992152278375584, 0.0, 0.20990917625900218, 0.03458034001283328]]\n",
      "\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print testX[2:20]\n",
    "print\n",
    "print testY[2:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
