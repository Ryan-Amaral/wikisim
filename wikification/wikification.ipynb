{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification\n",
    "This notebook contains code for doing wikification, as well as evaluating it.\n",
    "\n",
    "## Overview of our Wikification Method\n",
    "The following flowchart describes how our wikification method works without much technical detail.\n",
    "<img src=\"https://docs.google.com/drawings/d/19fInwE2C_fsAFiMNnIPe0cFldIHbJTHrXrLlLg6nbwI/pub?w=728&h=600\">\n",
    "<center><strong>Figure 1.</strong> A flowchart describing our wikification method at a relatively basic level.</center>\n",
    "\n",
    "## With More Detail\n",
    "In reference to figure 1.:\n",
    "\n",
    "### 1.  Input Some Text\n",
    "Self explanatory, just feed into the wikifier some text that is desired to be wikified. In the evaluation part of our code, the text comes pre-split from the datasets. We either keep the text split to focus more on our wikification, or join the text with spaces to evaluate while taking our own mention extraction into account.\n",
    "\n",
    "### 2. Tokenize Text\n",
    "The text is tokenized by a [Solr](http://lucene.apache.org/solr/) extension called [Solr Text Tagger](https://github.com/OpenSextant/SolrTextTagger), this tokenizer returns all potential mentions that it detects in the text. Our code is configured so that the tokenizer returns all overlaps. So if given the text: 'The United States of America', the tokenizer would return all of 'The United States', 'The United States of America', 'United States', and 'United States of America'. These overlaps are undesirable for our wikification purposes. However we choose to enable the overlaps so that we can obtain more potential mentions that we can deal with later more intelligently than the tokenizer can (without configuring it deeply). The overlaps are dealt with in the next step, though future work may make it better to deal with them later in the process.\n",
    "\n",
    "### 3. Remove Overlaps\n",
    "This part as-is is a work in progress. Currently our method is to first group all overlapping mentions into what we call overlap sets. Each overlap set is comprised of overlapping mentions that start at the same letter. The mention 'probability' of each mention is calculated at this time. The mention 'probability' is not truly a probability, it is defined as the amount of times the mention text is a mention in Wikipedia divided by the amount of documents it shows up in Wikipedia (it would be preferable to have the denominator be the total amount of times the mention text shows up in Wikipedia (to be an actual probability)). The mention with the highest 'probability' in each overlap set is the sole mention that is kept.\n",
    "\n",
    "There of-course may still be overlaps remaining at this point, now the residual overlaps are to be dealt with. It is important to note that for the following part, the mentions are stored in the order that they appear in the text, by their beginning letters'. When we say the first mention we mean the mention that appears first in the text, and by next mention we mean the mention that appears next in the text. To deal with the residual overlaps we call the first mention the anchor, and all of the next mentions that start before the anchor mention ends, all get grouped together with the anchor mention in an overlap set. Just like before the most 'probable' mention in this set is kept, all others are discarded from the original set. Once again the first mention that in the updated original set is selected as the anchor, the same process is repeated. If the overlap set only contains the anchor mention, the whole process is repeated on the next mention. This process is repeated until there is no next mention to go to.\n",
    "\n",
    "This step needs more investigation, perhaps the first part does not even need to be done.\n",
    "\n",
    "### 4. Filter with POS Tags and Mention Probability\n",
    "We use [Natural Language Toolkit (NLTK)](http://www.nltk.org/) to tag all of the mentions (though we should tag all of the text together to get more accurate results (update to come)). Using the POS tags helps us filter out bad mentions. Approximately 99% of all mentions in our datasets where either any type of noun, an adjective, or a cardinal number. The tags are displayed as 'NN', 'NNS', 'NNP', and 'NNPS' for nouns, 'JJ' for adjectives, and 'CD' for cardinal numbers.\n",
    "\n",
    "In addition to filtering with POS tags, we also filter out any mention that have a 'probability' of being a mention of less than 0.001.\n",
    "\n",
    "### 5. Candidate Generation\n",
    "Now that the mentions are all extracted, we must generate a list of possible entities that each mention can refer to, we call these, entity candidates. To generate entity candidates, we select the entities (Wikipedia page) that the given mention refers to most on Wikipedia. We refer to this measure as popularity. This selection is limited to an amount of candidates that is inputted into the wikification method. This method of candidate generation is objectively the best way to select candidates of the ways we have tried. Of 27092 mentions in our datasets, this method puts the correct candidate in the first position for 22896 mentions (when they are sorted descending by popularity). When selecting 20 candidates, 532 mentions do not have their correct entity as one of the candidates. Which means that if we are selecting 20 candidates for each mention, then we will have about 98% of the mentions containing the correct entity in its candidates.\n",
    "\n",
    "### 6. Candidate Scoring\n",
    "For each mention, all of the candidates must be scored on some metric. The candidate with the best score will be selected as the proposed entity for the mention. Currently, we are only working with individual elementary methods. By 'elementary methods', we mean that the methods are basic algorithmic processes and do not make any use of machine learning. In the future we will look at ways to combine the elementary methods together using a learning to rank algorithm, and also potentially look into deep learning for this problem.\n",
    "\n",
    "#### Popularity\n",
    "This method simply chooses the most popular candidate (most popular as described in section 5. Candidate Generation). This method performs very well but is undesirable due to the fact that it is just blindly guessing, and could be horribly wrong in some cases. See [this comic](https://comic.hmp.is.it/comic/rainy-days/) for an example of someone who does not quite get this concept.\n",
    "\n",
    "#### Context 1\n",
    "For this method, the sentence that contains the mention is extracted and called the context. The mention is removed from this context. We then use Solr to search for the most similar document (Wikipedia page) by searching in the document text field for the context, as well as searching in the document title field for the mention text. The set of documents that we are searching through in Solr is of-course limited to those that are the candidates of the mention. We do what is called boosting to make the results more weighted by the title field, the results from this are boosted by 1.35 (multiplied) on each document. The document with the highest score is deemed the most similar and is selected as the proposed entity for the mention.\n",
    "\n",
    "#### Context 2\n",
    "This method is slightly similar to Context 1 as it also uses Solr and it uses the sentence as a context in the same way. The difference is that we use a different index for this method. The index for this method, rather than containing whole documents (Wikipedia pages), contains all instances of all mentions with the surrounding context of each mention as a record. For example, a record could be for the mention 'David', the record will also have n (5 in this example) words before the mention: 'is a soccer player named', n words after the mention: 'he played for Manchester United', the Wikipedia page that the mention is in, and the Wikipedia page that the mention refers to. Using this index we search in the collection of all records that have the mention refer our candidate, for each candidate. The n words before and after are searched in for our context sentence, whichever entity has the highest number of relevant examples is selected as the proposed entity for that mention.\n",
    "\n",
    "#### Word2Vec\n",
    "For this method we have Word2Vec create a vector space model of concepts from a Wikipedia corpus. The entities, as well as regular words all have their own vector representation. To use this method, we select n words before and after the mention, and get the vector representation of each of these words. All of these vectors are added together to become a context vector. This context vector is compared to the vector representation of each of the candidates. The candidate vector that is most similar (by cosine similarity) to the context vector is selected as the proposed entity for the mention.\n",
    "\n",
    "#### Coherence\n",
    "This method uses the reverse page rank algorithm to determine which combination of candidates from all mentions makes the most sense together. This method looks at the quality of all of the proposed entities from all mentions together, instead of individually selecting the proposed entity for each individual mention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification Evaluation Code\n",
    "The code in this cell is used to evaluate the precision and recall of the wikification code as well as other wikification methods.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "### KORE\n",
    "* 50 records.\n",
    "* Relatively small pieces of text with the main goal of being tricky for wikification systems.\n",
    "\n",
    "### Aquaint\n",
    "* 50 records.\n",
    "* News.\n",
    "\n",
    "### MSNBC\n",
    "* 20 records.\n",
    "* News.\n",
    "\n",
    "### Wiki[n]\n",
    "* n records (we usually use 500 or 5000).\n",
    "* Opening paragraph of a variety of randomly selected Wikipedia articles.\n",
    "\n",
    "### nopop\n",
    "* 2304 records.\n",
    "* Comprised of subsets of the other datasets.\n",
    "* Only contains records where the most popular candidate is never the correct entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikification_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikification_eval.py \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods.\n",
    "\"\"\"\n",
    "\n",
    "from wikification import *\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import os\n",
    "import json\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# many different option for combonations of datasets for smaller tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "# 'popular', 'context1', 'context2', 'word2vec', 'coherence', 'tagme'\n",
    "methods = ['popularity']\n",
    "\n",
    "if 'word2vec' in methods:\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "doSplit = True\n",
    "doManual = True\n",
    "\n",
    "verbose = True\n",
    "\n",
    "maxCands = 20\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            if verbose:\n",
    "                print str(totalLines + 1)\n",
    "            \n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "            \n",
    "            # get results for pre split string\n",
    "            if doSplit and mthd <> 'tagme': # presplit no work on tagme\n",
    "                # original split string with mentions given\n",
    "                resultS = wikifyEval(copy.deepcopy(line), True, maxC = maxCands, method = mthd)\n",
    "                precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "                recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Split: ' + str(precS) + ', ' + str(recS)\n",
    "                \n",
    "                # track results\n",
    "                totalPrecS += precS\n",
    "                totalRecS += recS\n",
    "            else:\n",
    "                totalPrecS = 0\n",
    "                totalRecS = 0\n",
    "                \n",
    "            # get results for manually split string\n",
    "            if doManual:\n",
    "                # tagme has separate way to do things\n",
    "                if mthd == 'tagme':\n",
    "                    antns = tagme.annotate(\" \".join(line['text']))\n",
    "                    resultM = []\n",
    "                    for an in antns.get_annotations(0.005):\n",
    "                        resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "                else:\n",
    "                    # unsplit string to be manually split and mentions found\n",
    "                    resultM = wikifyEval(\" \".join(line['text']), False, maxC = maxCands, method = mthd)\n",
    "                \n",
    "                precM = precision(trueEntities, resultM) # precision of manual split\n",
    "                recM = recall(trueEntities, resultM) # recall of manual split\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Manual: ' + str(precM) + ', ' + str(recM)\n",
    "                    \n",
    "                # track results\n",
    "                totalPrecM += precM\n",
    "                totalRecM += recM\n",
    "            else:\n",
    "                totalPrecM = 0\n",
    "                totalRecM = 0\n",
    "                \n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines\n",
    "                                              }\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/wikification_results.txt', 'a') as resultFile:\n",
    "    resultFile.write('\\nmaxC: ' + str(maxCands) + '\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        for mthd in methods:\n",
    "            if doSplit and doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec']) + '\\n')\n",
    "            elif doSplit:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec']) + '\\n')\n",
    "            elif doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec']) + '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikification_eval_bot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikification_eval_bot.py \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods using BOT F1 score\n",
    "as described here: http://cogcomp.cs.illinois.edu/papers/RRDA11.pdf.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from wikification import *\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import os\n",
    "import json\n",
    "from sets import Set\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# many different option for combonations of datasets for smaller tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "\n",
    "# 'popular', 'context1', 'context2', 'word2vec', 'coherence', 'tagme'\n",
    "methods = ['popular']\n",
    "\n",
    "if 'word2vec' in methods:\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "doSplit = True\n",
    "doManual = False\n",
    "\n",
    "verbose = True\n",
    "\n",
    "maxCands = 20\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalBotF1S = 0\n",
    "        totalBotF1M = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            if verbose:\n",
    "                print str(totalLines + 1)\n",
    "            \n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "            trueSet = Set()\n",
    "            for truEnt in trueEntities:\n",
    "                trueSet.add(truEnt[2])\n",
    "            \n",
    "            # get results for pre split string\n",
    "            if doSplit and mthd <> 'tagme': # presplit no work on tagme\n",
    "                # original split string with mentions given\n",
    "                resultS = wikifyEval(copy.deepcopy(line), True, maxC = maxCands, method = mthd)\n",
    "                spltSet = Set()\n",
    "                for res in resultS:\n",
    "                    spltSet.add(res[2])\n",
    "                \n",
    "                precS = len(trueSet & spltSet)/len(spltSet)\n",
    "                recS = len(trueSet & spltSet)/len(trueSet)\n",
    "                f1 = (2*precS*recS)/(precS+recS)\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Split: ' + str(precS) + ', ' + str(recS) + ', ' + str(f1)\n",
    "                \n",
    "                # track results\n",
    "                totalPrecS += precS\n",
    "                totalRecS += recS\n",
    "                totalBotF1S += f1\n",
    "            else:\n",
    "                totalPrecS = 0\n",
    "                totalRecS = 0\n",
    "                totalBotF1S = 0\n",
    "                \n",
    "            # get results for manually split string\n",
    "            if doManual:\n",
    "                # tagme has separate way to do things\n",
    "                if mthd == 'tagme':\n",
    "                    antns = tagme.annotate(\" \".join(line['text']))\n",
    "                    resultM = []\n",
    "                    for an in antns.get_annotations(0.005):\n",
    "                        resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "                else:\n",
    "                    # unsplit string to be manually split and mentions found\n",
    "                    resultM = wikifyEval(\" \".join(line['text']), False, maxC = maxCands, method = mthd)\n",
    "                \n",
    "                manSet = Set()\n",
    "                for res in resultM:\n",
    "                    manSet.add(res[2])\n",
    "                \n",
    "                precM = len(trueSet & manSet)/len(manSet)\n",
    "                recM = len(trueSet & manSet)/len(trueSet)\n",
    "                f1 = (2*precM*recM)/(precM+recM)\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Manual: ' + str(precM) + ', ' + str(recM) + ', ' + str(f1)\n",
    "                \n",
    "                # track results\n",
    "                totalPrecM += precM\n",
    "                totalRecM += recM\n",
    "                totalBotF1M += f1\n",
    "            else:\n",
    "                totalPrecM = 0\n",
    "                totalRecM = 0\n",
    "                totalBotF1M = 0\n",
    "                \n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines,\n",
    "                                               'S BOT F1':totalBotF1S/totalLines,\n",
    "                                               'M BOT F1':totalBotF1M/totalLines\n",
    "                                              }\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/wikification_results.txt', 'a') as resultFile:\n",
    "    resultFile.write('\\nmaxC: ' + str(maxCands) + '\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        for mthd in methods:\n",
    "            if doSplit and doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "                       + '\\n    S BOT F1 :' + str(performances[dataset['name']][mthd]['S BOT F1'])\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec'])\n",
    "                       + '\\n    M BOT F1 :' + str(performances[dataset['name']][mthd]['M BOT F1'])+ '\\n')\n",
    "            elif doSplit:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "                       + '\\n    S BOT F1 :' + str(performances[dataset['name']][mthd]['S BOT F1'])+ '\\n')\n",
    "            elif doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec'])\n",
    "                       + '\\n    M BOT F1 :' + str(performances[dataset['name']][mthd]['M BOT F1'])+ '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Main Wikification Code\n",
    "The code in this cell contains all of the logic to do wikification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikification.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikification.py \n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "sys.path.append('../wikisim/')\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "from calcsim import *\n",
    "sys.path.append('../')\n",
    "from wsd.wsd import *\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps that start at same letter from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words until not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            score = mentionProb(textData[i][2])\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "    \n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "     \n",
    "def mentionExtract(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        phrase: The text to be split.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "    textData0 = r.json()['tags']\n",
    "    \n",
    "    splitText = [] # the text now in split form\n",
    "    mentions = [] # mentions before remove inadequate ones\n",
    "    \n",
    "    textData = [] # [[begin,end,word,anchorProb],...]\n",
    "    \n",
    "    i = 0 # for wordIndex\n",
    "    # get rid of extra un-needed Solr data, and add in anchor probability\n",
    "    for item in textData0:\n",
    "        totalMentions = get_mention_count(text[item[1]:item[3]])\n",
    "        totalAppearances = get_solr_count(text[item[1]:item[3]].replace(\".\", \"\"))\n",
    "        if totalAppearances == 0:\n",
    "            anchorProb = 0\n",
    "        else:\n",
    "            anchorProb = totalMentions/totalAppearances\n",
    "        # put in the new clean textData\n",
    "        textData.append([item[1], item[3], text[item[1]:item[3]], anchorProb, i])\n",
    "        i += 1\n",
    "        \n",
    "        # also fill split text\n",
    "        splitText.append(text[item[1]:item[3]])\n",
    "    \n",
    "    # get rid of overlaps\n",
    "    textData = destroyExclusiveOverlaps(textData)\n",
    "    textData = destroyResidualOverlaps(textData)\n",
    "        \n",
    "    # gets the POS labels for the words\n",
    "    postrs = []\n",
    "    for item in textData:\n",
    "        postrs.append(item[2])\n",
    "    postrs = nltk.pos_tag(postrs)\n",
    "    for i in range(0,len(textData)):\n",
    "        textData[i].append(postrs[i]) # [5][1] is index of type of word\n",
    "    \n",
    "    mentionPThrsh = 0.001 # for getting rid of unlikelies\n",
    "    \n",
    "    # put in only good mentions\n",
    "    for item in textData:\n",
    "        if (item[3] >= mentionPThrsh # if popular enough, and either some type of noun or JJ or CD\n",
    "                and (item[5][1][0:2] == 'NN' or item[5][1] == 'JJ' or item[5][1] == 'CD')):\n",
    "            mentions.append([item[4], item[0], item[1]]) # wIndex, start, end\n",
    "    \n",
    "    # get in same format as dataset provided data\n",
    "    newTextData = {'text':splitText, 'mentions':mentions}\n",
    "    \n",
    "    return newTextData\n",
    "\n",
    "def generateCandidates(textData, maxC):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData.\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        results = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)\n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def mentionPrecision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherMentions against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def mentionRecall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherMentions against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    if asList == True:\n",
    "        words = (text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    else:\n",
    "        words = \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return words\n",
    "\n",
    "def getMentionSentence(text, mention, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            if asList == True:\n",
    "                sentence = (s.replace(text[mention[1]:mention[2]],\"\")).split(\" \")\n",
    "            else:\n",
    "                sentence = s.replace(text[mention[1]:mention[2]],\"\")\n",
    "            \n",
    "            return sentence\n",
    "        \n",
    "    # in case it missed\n",
    "    if asList == True:\n",
    "        return []\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def bestContext1Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    #for doc in r.json()['response']['docs']:\n",
    "        #print '[' + id2title(doc['id']) + '] -> ' + str(doc['score'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def bestContext2Match(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+context.encode('utf-8')+')',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        scoreDict[str(doc['entityid'])] += 1\n",
    "    \n",
    "    # get the index that has the best score\n",
    "    bestScore = 0\n",
    "    bestIndex = 0\n",
    "    curIndex = 0\n",
    "    for cand in candidates:\n",
    "        if scoreDict[str(cand[0])] > bestScore:\n",
    "            bestScore = scoreDict[str(cand[0])]\n",
    "            bestIndex = curIndex\n",
    "        curIndex += 1\n",
    "            \n",
    "    return bestIndex\n",
    "\n",
    "def bestWord2VecMatch(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the candidate with the best similarity to the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best similarity score with the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    bestIndex = 0\n",
    "    bestScore = 0\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        #print '[' + id2title(cand[0]) + ']' + ' -> ' + str(score)\n",
    "        # update score and index\n",
    "        if score > bestScore: \n",
    "            bestIndex = i\n",
    "            bestScore = score\n",
    "            \n",
    "        i += 1 # next index\n",
    "            \n",
    "    return bestIndex\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyContext(textData, candidates, oText, useSentence = False, window = 7, method2 = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + context\n",
    "            if method2 == False:\n",
    "                bestIndex = bestContext1Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            else:\n",
    "                bestIndex = bestContext2Match(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest similarity to the context.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window, asList = True)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention, asList = True)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + \" \".join(context)\n",
    "            bestIndex = bestWord2VecMatch(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyCoherence(textData, candidates, ws = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest coherence according to rvs pagerank method.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ws: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCands = [] # the top candidate from each candidate list\n",
    "    candsScores = coherence_scores_driver(candidates, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    i = -1 # track what mention we are on\n",
    "    for cScores in candsScores:\n",
    "        i += 1\n",
    "        \n",
    "        if len(cScores) == 0:\n",
    "            continue # nothing to do with this one\n",
    "            \n",
    "        bestScore = sorted(cScores, reverse = True)[0]\n",
    "        curIndex = 0\n",
    "        for score in cScores:\n",
    "            if score == bestScore:\n",
    "                topCands.append([textData['mentions'][i][1], textData['mentions'][i][2], candidates[i][curIndex][0]])\n",
    "                break\n",
    "            curIndex += 1\n",
    "            \n",
    "    return topCands\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', strict = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        textData = mentionExtract(text) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'context1':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    elif method == 'context2':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        wikified = wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "        \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
