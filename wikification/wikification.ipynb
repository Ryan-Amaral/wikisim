{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification\n",
    "This notebook contains code for doing wikification, as well as evaluating it.\n",
    "\n",
    "## Overview of our Wikification Method\n",
    "The following flowchart describes how our wikification method works without much technical detail.\n",
    "<img src=\"https://docs.google.com/drawings/d/19fInwE2C_fsAFiMNnIPe0cFldIHbJTHrXrLlLg6nbwI/pub?w=728&h=600\">\n",
    "<center><strong>Figure 1.</strong> A flowchart describing our wikification method at a relatively basic level.</center>\n",
    "\n",
    "## With More Detail\n",
    "In reference to figure 1.:\n",
    "\n",
    "### 1.  Input Some Text\n",
    "Self explanatory, just feed into the wikifier some text that is desired to be wikified. In the evaluation part of our code, the text comes pre-split from the datasets. We either keep the text split to focus more on our wikification, or join the text with spaces to evaluate while taking our own mention extraction into account.\n",
    "\n",
    "### 2. Tokenize Text\n",
    "The text is tokenized by a [Solr](http://lucene.apache.org/solr/) extension called [Solr Text Tagger](https://github.com/OpenSextant/SolrTextTagger), this tokenizer returns all potential mentions that it detects in the text. Our code is configured so that the tokenizer returns all overlaps. So if given the text: 'The United States of America', the tokenizer would return all of 'The United States', 'The United States of America', 'United States', and 'United States of America'. These overlaps are undesirable for our wikification purposes. However we choose to enable the overlaps so that we can obtain more potential mentions that we can deal with later more intelligently than the tokenizer can (without configuring it deeply). The overlaps are dealt with in the next step, though future work may make it better to deal with them later in the process.\n",
    "\n",
    "### 3. Remove Overlaps\n",
    "This part as-is is a work in progress. Currently our method is to first group all overlapping mentions into what we call overlap sets. Each overlap set is comprised of overlapping mentions that start at the same letter. The mention 'probability' of each mention is calculated at this time. The mention 'probability' is not truly a probability, it is defined as the amount of times the mention text is a mention in Wikipedia divided by the amount of documents it shows up in Wikipedia (it would be preferable to have the denominator be the total amount of times the mention text shows up in Wikipedia (to be an actual probability)). The mention with the highest 'probability' in each overlap set is the sole mention that is kept.\n",
    "\n",
    "There of-course may still be overlaps remaining at this point, now the residual overlaps are to be dealt with. It is important to note that for the following part, the mentions are stored in the order that they appear in the text, by their beginning letters'. When we say the first mention we mean the mention that appears first in the text, and by next mention we mean the mention that appears next in the text. To deal with the residual overlaps we call the first mention the anchor, and all of the next mentions that start before the anchor mention ends, all get grouped together with the anchor mention in an overlap set. Just like before the most 'probable' mention in this set is kept, all others are discarded from the original set. Once again the first mention that in the updated original set is selected as the anchor, the same process is repeated. If the overlap set only contains the anchor mention, the whole process is repeated on the next mention. This process is repeated until there is no next mention to go to.\n",
    "\n",
    "This step needs more investigation, perhaps the first part does not even need to be done.\n",
    "\n",
    "### 4. Filter with POS Tags and Mention Probability\n",
    "We use [Natural Language Toolkit (NLTK)](http://www.nltk.org/) to tag all of the mentions (though we should tag all of the text together to get more accurate results (update to come)). Using the POS tags helps us filter out bad mentions. Approximately 99% of all mentions in our datasets where either any type of noun, an adjective, or a cardinal number. The tags are displayed as 'NN', 'NNS', 'NNP', and 'NNPS' for nouns, 'JJ' for adjectives, and 'CD' for cardinal numbers.\n",
    "\n",
    "In addition to filtering with POS tags, we also filter out any mention that have a 'probability' of being a mention of less than 0.001.\n",
    "\n",
    "### 5. Candidate Generation\n",
    "Now that the mentions are all extracted, we must generate a list of possible entities that each mention can refer to, we call these, entity candidates. To select n candidates, we first try selecting n/2 entities (Wikipedia page) that the given mention refers to most on Wikipedia. We refer to this measure as popularity. Once the n/2 most popular are selected, the remaining n/2 entities are selected based on the context similarity with other mentions in the same sentence. If selecting most popular fails to return n/2 results, we try selecting however many we need based on context. So if popularity returns 0 entities, we select n from the most contextual. This method of mixing popular and contextual candidates is called the hybrid method.\n",
    "\n",
    "The hybrid method of candidate generation scores best on average on each of our datasets. Selecting candidates based on popularity alone initially seemed more promising, because it scored better on the largest dataset (wiki5000), thus giving an overall higher recall. But on each independent dataset, the popularity method only scored better on wiki5000, by about one percent, whereas other datasets scored worse, from 3 to 13 percent.\n",
    "\n",
    "### 6. Candidate Scoring\n",
    "For each mention, all of the candidates must be scored on some metric. The candidate with the best score will be selected as the proposed entity for the mention. All of these methods rely on basic scores such as the popularity of an entity given the mention, or some measure of similarity from the context of a mention to the document of a candidate. Individually some of these methods perform well on select datasets, but combined together using machine learning gives the best results overall. Using a learning to rank algorithm ([LambdaMART](https://github.com/jma127/pyltr)), we achieve a score better than any best idividual on each of our datasets.\n",
    "\n",
    "#### Popularity\n",
    "This method simply chooses the most popular candidate (most popular as described in section 5. Candidate Generation). This method performs very well but is undesirable due to the fact that it is just blindly guessing, and could be horribly wrong in some cases. See [this comic](https://comic.hmp.is.it/comic/rainy-days/) for an example of someone who does not quite get this concept.\n",
    "\n",
    "#### Context 1\n",
    "For this method, the sentence that contains the mention is extracted and called the context. The mention is removed from this context. We then use Solr to search for the most similar document (Wikipedia page) by searching in the document text field for the context, as well as searching in the document title field for the mention text. The set of documents that we are searching through in Solr is of-course limited to those that are the candidates of the mention. We do what is called boosting to make the results more weighted by the title field, the results from this are boosted by 1.35 (multiplied) on each document. The document with the highest score is deemed the most similar and is selected as the proposed entity for the mention.\n",
    "\n",
    "#### Context 2\n",
    "This method is slightly similar to Context 1 as it also uses Solr and it uses the sentence as a context in the same way. The difference is that we use a different index for this method. The index for this method, rather than containing whole documents (Wikipedia pages), contains all instances of all mentions with the surrounding context of each mention as a record. For example, a record could be for the mention 'David', the record will also have n (5 in this example) words before the mention: 'is a soccer player named', n words after the mention: 'he played for Manchester United', the Wikipedia page that the mention is in, and the Wikipedia page that the mention refers to. Using this index we search in the collection of all records that have the mention refer our candidate, for each candidate. The n words before and after are searched in for our context sentence, whichever entity has the highest number of relevant examples is selected as the proposed entity for that mention.\n",
    "\n",
    "#### Word2Vec\n",
    "For this method we have Word2Vec create a vector space model of concepts from a Wikipedia corpus. The entities, as well as regular words all have their own vector representation. To use this method, we select n words before and after the mention, and get the vector representation of each of these words. All of these vectors are added together to become a context vector. This context vector is compared to the vector representation of each of the candidates. The candidate vector that is most similar (by cosine similarity) to the context vector is selected as the proposed entity for the mention.\n",
    "\n",
    "#### Coherence\n",
    "This method uses the reverse page rank algorithm to determine which combination of candidates from all mentions makes the most sense together. This method looks at the quality of all of the proposed entities from all mentions together, instead of individually selecting the proposed entity for each individual mention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification Evaluation Code\n",
    "The code in this cell is used to evaluate the precision and recall of the wikification code as well as other wikification methods.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "### KORE\n",
    "* 50 records.\n",
    "* Relatively small pieces of text with the main goal of being tricky for wikification systems.\n",
    "\n",
    "### Aquaint\n",
    "* 50 records.\n",
    "* News.\n",
    "\n",
    "### MSNBC\n",
    "* 20 records.\n",
    "* News.\n",
    "\n",
    "### Wiki[n]\n",
    "* n records (we usually use 500 or 5000).\n",
    "* Opening paragraph of a variety of randomly selected Wikipedia articles.\n",
    "\n",
    "### nopop\n",
    "* 2304 records.\n",
    "* Comprised of subsets of the other datasets.\n",
    "* Only contains records where the most popular candidate is not the correct entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikification_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikification_eval.py \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods (Macro).\n",
    "\"\"\"\n",
    "\n",
    "from wikification import *\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import os\n",
    "import json\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# many different option for combonations of datasets for smaller tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "# 'popular', 'context1', 'context2', 'word2vec', 'coherence', 'tagme', 'multi'\n",
    "#methods = ['multi']\n",
    "methods = ['abc', 'bgc', 'etc', 'gbc', 'rfc', 'lsvc', 'svc', 'lmart']\n",
    "# 'lmart', 'gbr', 'etr', 'rfr'\n",
    "mlModels = 'lmart'\n",
    "\n",
    "if 'word2vec' in methods or 'multi' in methods or True:\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "doSplit = True\n",
    "doManual = False\n",
    "\n",
    "verbose = True\n",
    "\n",
    "maxCands = 20\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalF1S = 0\n",
    "        totalF1M = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            if verbose:\n",
    "                print str(totalLines + 1)\n",
    "            \n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "            \n",
    "            oData = copy.deepcopy(line)\n",
    "            \n",
    "            # get results for pre split string\n",
    "            if doSplit and mthd <> 'tagme': # presplit no work on tagme\n",
    "                # original split string with mentions given\n",
    "                resultS = wikifyEval(copy.deepcopy(line), True, hybridC = True, maxC = maxCands, method = 'multi', model = mthd)\n",
    "                precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "                recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "                try:\n",
    "                    f1S = (2*precS*recS)/(precS+recS)\n",
    "                except:\n",
    "                    f1S = 0\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Split: ' + str(precS) + ', ' + str(recS) + ', ' + str(f1S)\n",
    "                \n",
    "                # track results\n",
    "                totalPrecS += precS\n",
    "                totalRecS += recS\n",
    "                totalF1S += f1S\n",
    "                \n",
    "                \"\"\"j = 0\n",
    "                for mention in oData['mentions']:\n",
    "                    try:\n",
    "                        print oData['text'][mention[0]].encode('utf-8') + ':  ' + mention[1] + ' --> ' + id2title(resultS[j][2])\n",
    "                    except:\n",
    "                        pass\n",
    "                    j += 1\"\"\"\n",
    "                \n",
    "            else:\n",
    "                totalPrecS = 0\n",
    "                totalRecS = 0\n",
    "                totalF1S = 0\n",
    "                \n",
    "            # get results for manually split string\n",
    "            if doManual:\n",
    "                # tagme has separate way to do things\n",
    "                if mthd == 'tagme':\n",
    "                    antns = tagme.annotate(\" \".join(line['text']))\n",
    "                    resultM = []\n",
    "                    for an in antns.get_annotations(0.005):\n",
    "                        resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "                else:\n",
    "                    # unsplit string to be manually split and mentions found\n",
    "                    resultM = wikifyEval(\" \".join(line['text']), False, maxC = maxCands, method = mthd)\n",
    "                \n",
    "                precM = precision(trueEntities, resultM) # precision of manual split\n",
    "                recM = recall(trueEntities, resultM) # recall of manual split\n",
    "                try:\n",
    "                    f1M = (2*precM*recM)/(precM+recM)\n",
    "                except:\n",
    "                    f1M = 0\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Manual: ' + str(precM) + ', ' + str(recM) + ', ' + str(f1M)\n",
    "                    \n",
    "                # track results\n",
    "                totalPrecM += precM\n",
    "                totalRecM += recM\n",
    "                totalF1M += f1M\n",
    "            else:\n",
    "                totalPrecM = 0\n",
    "                totalRecM = 0\n",
    "                totalF1M = 0\n",
    "                \n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines,\n",
    "                                               'S F1':totalF1S/totalLines,\n",
    "                                               'M F1':totalF1M/totalLines\n",
    "                                              }\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/wikification_results.txt', 'a') as resultFile:\n",
    "    resultFile.write('\\nmaxC: ' + str(maxCands) + '\\n' + str(datetime.now()) + '\\n\\n')\n",
    "    resultFile.write('Doing hybrid candidate generation before hybrid training.\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        for mthd in methods:\n",
    "            if doSplit and doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "                       + '\\n    S F1 :' + str(performances[dataset['name']][mthd]['S F1'])\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec'])\n",
    "                       + '\\n    M F1 :' + str(performances[dataset['name']][mthd]['M F1']) + '\\n')\n",
    "            elif doSplit:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec']) \n",
    "                       + '\\n    S F1 :' + str(performances[dataset['name']][mthd]['S F1']) + '\\n')\n",
    "            elif doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec'])\n",
    "                       + '\\n    M F1 :' + str(performances[dataset['name']][mthd]['M F1']) + '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kore\n",
      "\n",
      "context1\n",
      "2017-07-26 11:32:58.622574\n",
      "\n",
      "1\n",
      "Split: 2, 0.0\n",
      "2\n",
      "Split: 4, 0.0\n",
      "3\n",
      "Split: 6, 1.0\n",
      "4\n",
      "Split: 8, 1.0\n",
      "5\n",
      "Split: 9, 2.0\n",
      "6\n",
      "Split: 11, 2.0\n",
      "7\n",
      "Split: 14, 2.0\n",
      "8\n",
      "Split: 17, 2.0\n",
      "9\n",
      "Split: 19, 2.0\n",
      "10\n",
      "Split: 24, 3.0\n",
      "11\n",
      "Split: 28, 5.0\n",
      "12\n",
      "Split: 31, 5.0\n",
      "13\n",
      "Split: 34, 7.0\n",
      "14\n",
      "Split: 39, 9.0\n",
      "15\n",
      "Split: 45, 10.0\n",
      "16\n",
      "Split: 48, 12.0\n",
      "17\n",
      "Split: 51, 12.0\n",
      "18\n",
      "Split: 54, 12.0\n",
      "19\n",
      "Split: 59, 12.0\n",
      "20\n",
      "Split: 63, 13.0\n",
      "21\n",
      "Split: 66, 13.0\n",
      "22\n",
      "Split: 70, 14.0\n",
      "23\n",
      "Split: 73, 15.0\n",
      "24\n",
      "Split: 77, 15.0\n",
      "25\n",
      "Split: 79, 17.0\n",
      "26\n",
      "Split: 82, 17.0\n",
      "27\n",
      "Split: 84, 19.0\n",
      "28\n",
      "Split: 87, 21.0\n",
      "29\n",
      "Split: 90, 21.0\n",
      "30\n",
      "Split: 92, 21.0\n",
      "31\n",
      "Split: 94, 21.0\n",
      "32\n",
      "Split: 96, 21.0\n",
      "33\n",
      "Split: 98, 22.0\n",
      "34\n",
      "Split: 100, 22.0\n",
      "35\n",
      "Split: 103, 22.0\n",
      "36\n",
      "Split: 105, 22.0\n",
      "37\n",
      "Split: 108, 23.0\n",
      "38\n",
      "Split: 111, 23.0\n",
      "39\n",
      "Split: 113, 23.0\n",
      "40\n",
      "Split: 115, 23.0\n",
      "41\n",
      "Split: 119, 25.0\n",
      "42\n",
      "Split: 123, 25.0\n",
      "43\n",
      "Split: 126, 26.0\n",
      "44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e4da5f26034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoSplit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmthd\u001b[0m \u001b[0;34m<>\u001b[0m \u001b[0;34m'tagme'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# presplit no work on tagme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# original split string with mentions given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mresultS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikifyEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxCands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmthd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybridC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lmart'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mtotalRightS\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrueEntities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrueEntities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/wikisim/wikification/wikification.pyc\u001b[0m in \u001b[0;36mwikifyEval\u001b[0;34m(text, mentionsGiven, maxC, method, strict, hybridC, model)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mmaxC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# only need one cand for popular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateCandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybridC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'popular'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/wikisim/wikification/wikification.pyc\u001b[0m in \u001b[0;36mgenerateCandidates\u001b[0;34m(textData, maxC, hybrid)\u001b[0m\n\u001b[1;32m    579\u001b[0m                         'wt':'json', 'rows':str(ctxC)}\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 if ('response' in r.json() \n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    421\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    592\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, buffering)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%writefile wikification_eval_micro.py \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods (Micro).\n",
    "\"\"\"\n",
    "\n",
    "from wikification import *\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import os\n",
    "import json\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# many different option for combonations of datasets for smaller tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}, {'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "\n",
    "# 'popular', 'context1', 'context2', 'word2vec', 'coherence', 'tagme'\n",
    "methods = ['context1', 'context2']\n",
    "\n",
    "if 'word2vec' in methods or 'multi' in methods or True:\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "doSplit = True\n",
    "doManual = False\n",
    "\n",
    "verbose = True\n",
    "\n",
    "maxCands = 20\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalMentions = 0\n",
    "        totalRightS = 0\n",
    "        totalRightM = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            if verbose:\n",
    "                print str(totalLines + 1)\n",
    "            \n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "            \n",
    "            oData = copy.deepcopy(line)\n",
    "            \n",
    "            totalMentions += len(trueEntities)\n",
    "            \n",
    "            # get results for pre split string\n",
    "            if doSplit and mthd <> 'tagme': # presplit no work on tagme\n",
    "                # original split string with mentions given\n",
    "                resultS = wikifyEval(copy.deepcopy(line), True, maxC = maxCands, method = mthd, hybridC = True, model = 'lmart')\n",
    "                totalRightS += precision(trueEntities, resultS) * len(trueEntities)\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Split: ' + str(totalMentions) + ', ' + str(totalRightS)\n",
    "                \n",
    "            # get results for manually split string\n",
    "            if doManual:\n",
    "                # tagme has separate way to do things\n",
    "                if mthd == 'tagme':\n",
    "                    antns = tagme.annotate(\" \".join(line['text']))\n",
    "                    resultM = []\n",
    "                    for an in antns.get_annotations(0.005):\n",
    "                        resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "                else:\n",
    "                    # unsplit string to be manually split and mentions found\n",
    "                    resultM = wikifyEval(\" \".join(line['text']), False, maxC = maxCands, method = mthd)\n",
    "                \n",
    "                totalRightM += precision(trueEntities, resultM) * len(trueEntities)\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Manual: ' + str(totalMentions) + ', ' + str(totalRightM)\n",
    "                \n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S F1':totalRightS/totalMentions,\n",
    "                                               'M F1':totalRightM/totalMentions\n",
    "                                              }\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/wikification_results.txt', 'a') as resultFile:\n",
    "    resultFile.write('\\nmaxC: ' + str(maxCands) + '\\n' + str(datetime.now()) + '\\n\\n')\n",
    "    #resultFile.write('Doing hybrid candidate generation with new hybrid trained models.\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        for mthd in methods:\n",
    "            if doSplit and doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Micro F1 :' + str(performances[dataset['name']][mthd]['S F1'])\n",
    "                       + '\\n    M Micro F1 :' + str(performances[dataset['name']][mthd]['M F1']) + '\\n')\n",
    "            elif doSplit:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Micro F1 :' + str(performances[dataset['name']][mthd]['S F1']) + '\\n')\n",
    "            elif doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    M Micro F1 :' + str(performances[dataset['name']][mthd]['M F1']) + '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile wikification_eval_bot.py \n",
    "\n",
    "\"\"\"\n",
    "This is for testing performance of different wikification methods using BOT F1 score\n",
    "as described here: http://cogcomp.cs.illinois.edu/papers/RRDA11.pdf.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from wikification import *\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import os\n",
    "import json\n",
    "from sets import Set\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "    \n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# many different option for combonations of datasets for smaller tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "\n",
    "# 'popular', 'context1', 'context2', 'word2vec', 'coherence', 'tagme'\n",
    "methods = ['context2']\n",
    "\n",
    "if 'word2vec' in methods:\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "doSplit = True\n",
    "doManual = False\n",
    "\n",
    "verbose = True\n",
    "\n",
    "maxCands = 20\n",
    "\n",
    "performances = {}\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # put in all lines that contain proper ascii\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        # reset counters\n",
    "        totalPrecS = 0\n",
    "        totalPrecM = 0\n",
    "        totalRecS = 0\n",
    "        totalRecM = 0\n",
    "        totalBotF1S = 0\n",
    "        totalBotF1M = 0\n",
    "        totalLines = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            if verbose:\n",
    "                print str(totalLines + 1)\n",
    "            \n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "            trueSet = Set()\n",
    "            for truEnt in trueEntities:\n",
    "                trueSet.add(truEnt[2])\n",
    "            \n",
    "            # get results for pre split string\n",
    "            if doSplit and mthd <> 'tagme': # presplit no work on tagme\n",
    "                # original split string with mentions given\n",
    "                resultS = wikifyEval(copy.deepcopy(line), True, maxC = maxCands, method = mthd)\n",
    "                spltSet = Set()\n",
    "                for res in resultS:\n",
    "                    spltSet.add(res[2])\n",
    "                \n",
    "                precS = len(trueSet & spltSet)/len(spltSet)\n",
    "                recS = len(trueSet & spltSet)/len(trueSet)\n",
    "                try:\n",
    "                    f1 = (2*precS*recS)/(precS+recS)\n",
    "                except:\n",
    "                    f1 = 0\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Split: ' + str(precS) + ', ' + str(recS) + ', ' + str(f1)\n",
    "                \n",
    "                # track results\n",
    "                totalPrecS += precS\n",
    "                totalRecS += recS\n",
    "                totalBotF1S += f1\n",
    "            else:\n",
    "                totalPrecS = 0\n",
    "                totalRecS = 0\n",
    "                totalBotF1S = 0\n",
    "                \n",
    "            # get results for manually split string\n",
    "            if doManual:\n",
    "                # tagme has separate way to do things\n",
    "                if mthd == 'tagme':\n",
    "                    antns = tagme.annotate(\" \".join(line['text']))\n",
    "                    resultM = []\n",
    "                    for an in antns.get_annotations(0.005):\n",
    "                        resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "                else:\n",
    "                    # unsplit string to be manually split and mentions found\n",
    "                    resultM = wikifyEval(\" \".join(line['text']), False, maxC = maxCands, method = mthd)\n",
    "                \n",
    "                manSet = Set()\n",
    "                for res in resultM:\n",
    "                    manSet.add(res[2])\n",
    "                \n",
    "                precM = len(trueSet & manSet)/len(manSet)\n",
    "                recM = len(trueSet & manSet)/len(trueSet)\n",
    "                try:\n",
    "                    f1 = (2*precM*recM)/(precM+recM)\n",
    "                except:\n",
    "                    f1 = 0\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Manual: ' + str(precM) + ', ' + str(recM) + ', ' + str(f1)\n",
    "                \n",
    "                # track results\n",
    "                totalPrecM += precM\n",
    "                totalRecM += recM\n",
    "                totalBotF1M += f1\n",
    "            else:\n",
    "                totalPrecM = 0\n",
    "                totalRecM = 0\n",
    "                totalBotF1M = 0\n",
    "                \n",
    "            totalLines += 1\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # [avg precision split, avg precision manual, avg recall split, avg recall manual]\n",
    "        performances[dataset['name']][mthd] = {'S Prec':totalPrecS/totalLines, \n",
    "                                               'M Prec':totalPrecM/totalLines,\n",
    "                                              'S Rec':totalRecS/totalLines, \n",
    "                                               'M Rec':totalRecM/totalLines,\n",
    "                                               'S BOT F1':totalBotF1S/totalLines,\n",
    "                                               'M BOT F1':totalBotF1M/totalLines\n",
    "                                              }\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/wikification_results.txt', 'a') as resultFile:\n",
    "    resultFile.write('\\nmaxC: ' + str(maxCands) + '\\n' + str(datetime.now()) + '\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        for mthd in methods:\n",
    "            if doSplit and doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "                       + '\\n    S BOT F1 :' + str(performances[dataset['name']][mthd]['S BOT F1'])\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec'])\n",
    "                       + '\\n    M BOT F1 :' + str(performances[dataset['name']][mthd]['M BOT F1'])+ '\\n')\n",
    "            elif doSplit:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Prec :' + str(performances[dataset['name']][mthd]['S Prec'])\n",
    "                       + '\\n    S Rec :' + str(performances[dataset['name']][mthd]['S Rec'])\n",
    "                       + '\\n    S BOT F1 :' + str(performances[dataset['name']][mthd]['S BOT F1'])+ '\\n')\n",
    "            elif doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    M Prec :' + str(performances[dataset['name']][mthd]['M Prec'])\n",
    "                       + '\\n    M Rec :' + str(performances[dataset['name']][mthd]['M Rec'])\n",
    "                       + '\\n    M BOT F1 :' + str(performances[dataset['name']][mthd]['M BOT F1'])+ '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mention_extraction_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mention_extraction_eval.py \n",
    "\n",
    "\"\"\"\n",
    "This evaluates the quality of mention extraction, macro scores.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from wikification import *\n",
    "from datetime import datetime\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalF1 = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        if(verbose):\n",
    "            print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = mentionStartsAndEnds(line, True)\n",
    "        \n",
    "        \"\"\"\n",
    "        output = nlp.annotate(\" \".join(line['text']).encode('utf-8'), properties={\n",
    "            'annotators': 'entitymentions',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        myMentions = []\n",
    "        for sentence in output['sentences']:\n",
    "            for em in sentence['entitymentions']:\n",
    "                myMentions.append([em['characterOffsetBegin'], em['characterOffsetEnd']])\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\"\"\"\n",
    "        myMentions = mentionExtract(\" \".join(line['text']))['mentions']\n",
    "        \n",
    "        # put in right format\n",
    "        #print trueMentions\n",
    "        #print myMentions\n",
    "        for mention in myMentions:\n",
    "            mention[0] = mention[1]\n",
    "            mention[1] = mention[2]\n",
    "        \"\"\"\"\"\"\n",
    "            \n",
    "        prec = mentionPrecision(trueMentions, myMentions)\n",
    "        rec = mentionRecall(trueMentions, myMentions)\n",
    "        try:\n",
    "            f1 = (2*prec*rec)/(prec+rec)\n",
    "        except:\n",
    "            f1 = 0\n",
    "        \n",
    "        if(verbose):\n",
    "            print str(prec) + ' ' + str(rec) + ' ' + str(f1) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalF1 += f1\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines,\n",
    "                                     'F1':totalF1/totalLines}\n",
    "            \n",
    "with open('/users/cs/amaral/wikisim/wikification/mention_extraction_results.txt', 'a') as resultFile:\n",
    "    resultFile.write(str(datetime.now()) + '\\n')\n",
    "    resultFile.write('Using our classifier v2.' + '\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        resultFile.write('\\n    Prec :' + str(performances[dataset['name']]['Precision'])\n",
    "               + '\\n    Rec :' + str(performances[dataset['name']]['Recall'])\n",
    "               + '\\n    F1 :' + str(performances[dataset['name']]['F1']) + '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Main Wikification Code\n",
    "## The code in this cell contains all of the logic to do wikification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikification.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikification.py\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "sys.path.append('../wikisim/')\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "from calcsim import *\n",
    "sys.path.append('../')\n",
    "from wsd.wsd import *\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "scnlp = StanfordCoreNLP('http://localhost:9000')\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-filter-out-nonmentions.txt', 'r') as srcFile:\n",
    "    posFilter = srcFile.read().splitlines()\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "    \n",
    "def normalize(nums):\n",
    "    \"\"\"Normalizes a list of nums to its sum + 1\"\"\"\n",
    "    \n",
    "    numSum = sum(nums) + 1 # get max\n",
    "    \n",
    "    # fill with normalized\n",
    "    normNums = []\n",
    "    for num in nums:\n",
    "        normNums.append(num/numSum)\n",
    "        \n",
    "    return normNums\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps that start at same letter from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words until not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            score = mentionProb(textData[i][2])\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "    \n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "\n",
    "posBefDict = {\n",
    "    'IN':0,\n",
    "    'DT':1,\n",
    "    'NNP':2,\n",
    "    'JJ':3,\n",
    "    ',':4,\n",
    "    'CC':5,\n",
    "    'NN':6,\n",
    "    'VBD':7,\n",
    "    'CD':8,\n",
    "    '(':9,\n",
    "    'TO':10,\n",
    "    'FAIL':11\n",
    "}\n",
    "\n",
    "posCurDict = {\n",
    "    'NNP':0,\n",
    "    'NN':1,\n",
    "    'JJ':2,\n",
    "    'NNS':3,\n",
    "    'CD':4,\n",
    "    'NNPS':5,\n",
    "    'FAIL':6\n",
    "}\n",
    "\n",
    "posAftDict = {\n",
    "    ',':0,\n",
    "    '.':1,\n",
    "    'IN':2,\n",
    "    'NNP':3,\n",
    "    'CC':4,\n",
    "    'NN':5,\n",
    "    'VBD':6,\n",
    "    ':':7,\n",
    "    'VBZ':8,\n",
    "    'POS':9,\n",
    "    'NNS':10,\n",
    "    'TO':11,\n",
    "    'FAIL':12\n",
    "}\n",
    "     \n",
    "def getGoodMentions(splitText, mentions, model):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Finds the potential mentions that are deemed good by our classifier.\n",
    "    Args:\n",
    "        splitText: The text in split form.\n",
    "        mentions: All of the potential mentions [word index, start offset, end offset].\n",
    "        model: The machine learning model to predict with.\n",
    "    Return:\n",
    "        A subset of arg mentions with each element deemed to be worthy by the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    goodMentions = [] # the mentions to return\n",
    "    \n",
    "    #Get POS tags of all text\n",
    "    postrs = nltk.pos_tag(copy.deepcopy(splitText))\n",
    "\n",
    "    # get stanford core mentions\n",
    "    try:\n",
    "        stnfrdMentions0 = scnlp.annotate(\" \".join(splitText).encode('utf-8'), properties={\n",
    "            'annotators': 'entitymentions',\n",
    "            'outputFormat': 'json'})\n",
    "    except:\n",
    "        print 'Error!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\n",
    "        k = 1/0\n",
    "    stnfrdMentions = []\n",
    "    for sentence in stnfrdMentions0['sentences']:\n",
    "        for mention in sentence['entitymentions']:\n",
    "            stnfrdMentions.append(mention['text'])\n",
    "            \n",
    "    enc = OneHotEncoder(n_values = [12,7,13], categorical_features = [0,1,2])\n",
    "            \n",
    "    for i in range(len(mentions)):\n",
    "        aMention = [] # fill with attributes about current mention for prediction\n",
    "        \n",
    "        \"\"\" \n",
    "        Append POS tags of before, on, and after mention.\n",
    "        \"\"\"\n",
    "        if i == 0:\n",
    "            bef = 'NONE'\n",
    "        else:\n",
    "            bef = postrs[i-1][1] # pos tag of before\n",
    "        if bef in posBefDict:\n",
    "            bef = posBefDict[bef]\n",
    "        else:\n",
    "            bef = posBefDict['FAIL']\n",
    "            \n",
    "        on = postrs[i][1] # pos tag of mention\n",
    "        if on in posCurDict:\n",
    "            on = posCurDict[on]\n",
    "        else:\n",
    "            on = posCurDict['FAIL']\n",
    "        \n",
    "        if i == len(splitText) - 1:\n",
    "            aft = 'NONE'\n",
    "        else:\n",
    "            aft = postrs[i+1][1] # pos tag of after\n",
    "        if aft in posAftDict:\n",
    "            aft = posAftDict[aft]\n",
    "        else:\n",
    "            aft = posAftDict['FAIL']\n",
    "        \n",
    "        aMention.extend([bef, on, aft])\n",
    "        \n",
    "        \"\"\"\n",
    "        Append mention probability.\n",
    "        \"\"\"\n",
    "        aMention.append(mentionProb(splitText[mentions[i][0]]))\n",
    "        \n",
    "        \"\"\"\n",
    "        Find whether Stanford NER decides the word to be mention.\n",
    "        \"\"\"\n",
    "        if splitText[mentions[i][0]] in stnfrdMentions:\n",
    "            stnfrdMentions.remove(splitText[mentions[i][0]])\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether starts with capital.\n",
    "        \"\"\"\n",
    "        if splitText[mentions[i][0]][0].isupper():\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether there is an exact match in Wikipedia.\n",
    "        \"\"\"\n",
    "        if title2id(splitText[mentions[i][0]]) is not None:\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether word contains a space.\n",
    "        \"\"\"\n",
    "        if ' ' in splitText[mentions[i][0]]:\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether the word contains only ascii characters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            splitText[mentions[i][0]].decode('ascii')\n",
    "            aMention.append(1)\n",
    "        except:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Get all positive classified instances.\n",
    "        \"\"\"\n",
    "        aMention = enc.fit_transform([aMention]).toarray()[0]\n",
    "        if model.predict([aMention])[0] == 1:\n",
    "            goodMentions.append(mentions[i])\n",
    "            # put score of prediction\n",
    "            goodMentions[-1].append(model.predict_proba([aMention])[0][1]) # put score of prediction\n",
    "            \n",
    "    return goodMentions        \n",
    "    \n",
    "    \"\"\"\n",
    "    #Remove all overlaps in results.\n",
    "    \n",
    "    # sort on prediction probability descending\n",
    "    goodMentions = sorted(goodMentions, key = itemgetter(-1), reverse = True)\n",
    "    \n",
    "    try:\n",
    "        goodlen = len(goodMentions[0])\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    for mention1 in goodMentions:\n",
    "        if len(mention1) > goodlen:\n",
    "            continue\n",
    "        for mention2 in goodMentions:\n",
    "            # dont do anything with a previous or same one\n",
    "            if (mention2[0] == mention1[0] or\n",
    "                    mention1[-1] < mention2[-1] or\n",
    "                    len(mention2) > goodlen):\n",
    "                continue\n",
    "            # flag 2 if 2 starts before 1 ends and 2 ends after 1 starts\n",
    "            if mention2[1] < mention1[2] and mention2[2] >= mention1[1]:\n",
    "                print 'Overlap found', str(mention1), str(mention2)\n",
    "                mention2.append(0) # just increase length to flag for deletion\n",
    "                \n",
    "    finalMentions = []\n",
    "    for mention in goodMentions:\n",
    "        if len(mention) == goodlen:\n",
    "            finalMentions.append(mention[:3])\n",
    "            \n",
    "    finalMentions = sorted(finalMentions, key = itemgetter(1), reverse = False)\n",
    "            \n",
    "    return finalMentions\"\"\"\n",
    "    \n",
    "def mentionExtract(text, useCoreNLP = True):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        text: The text to be split.\n",
    "        useCoreNLP: Whether to use CoreNLP entity mention annotation.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    if useCoreNLP:\n",
    "        output = scnlp.annotate(text, properties={\n",
    "            'annotators': 'entitymentions',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        mentions = []\n",
    "        for sentence in output['sentences']:\n",
    "            for em in sentence['entitymentions']:\n",
    "                mentions.append([em['characterOffsetBegin'], em['characterOffsetEnd']])\n",
    "        curM = 0 # index of mention\n",
    "        splitText0 = text.split(' ')\n",
    "        splitText = []\n",
    "        charSoFar = 0\n",
    "        skip = 0\n",
    "        curW = 0\n",
    "        for txt in splitText0:\n",
    "            if skip > 0:\n",
    "                skip -= 1\n",
    "                continue\n",
    "            if curM < len(mentions) and charSoFar == mentions[curM][0]:\n",
    "                skip = text[mentions[curM][0]:mentions[curM][1]].count(' ')\n",
    "                splitText.append(text[mentions[curM][0]:mentions[curM][1]])\n",
    "                mentions[curM].insert(0, curW)\n",
    "                curM += 1\n",
    "            else:\n",
    "                splitText.append(txt)\n",
    "            charSoFar += 1 + len(splitText[-1])\n",
    "            curW += 1\n",
    "    else:\n",
    "        addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "        params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "        r = requests.post(addr, params=params, data=text.encode('utf-8'))\n",
    "        textData0 = r.json()['tags']\n",
    "        splitText = [] # the text now in split form\n",
    "        mentions = [] # mentions before remove inadequate ones\n",
    "        textData = [] # [[begin,end,word,anchorProb],...]\n",
    "        i = 0 # for wordIndex\n",
    "        # get rid of extra un-needed Solr data\n",
    "        for item in textData0:\n",
    "            mentions.append([i, item[1], item[3]])\n",
    "            i += 1\n",
    "            # also fill split text\n",
    "            splitText.append(text[item[1]:item[3]])\n",
    "        if 'gbc-er' not in mlModels:\n",
    "            mlModels['gbc-er'] = pickle.load(open(mlModelFiles['gbc-er'], 'rb'))\n",
    "        mentions = getGoodMentions(splitText, mentions, mlModels['gbc-er'])\n",
    "    \n",
    "    print splitText\n",
    "    print mentions\n",
    "    \n",
    "    return {'text':splitText, 'mentions':mentions}\n",
    "\n",
    "def getMentionsInSentence(textData, mainWord):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Finds all mentions that are in the same sentence as mainWord.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        mainWord: The index of the word that is in the wanted sentence\n",
    "    Return:\n",
    "        A list of mention texts that are in the same sentence as mainWord\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = nltk.sent_tokenize(\" \".join(textData['text']))\n",
    "    \n",
    "    # start and end of sentences (absolute)\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    mentionStrs = [] # the mentions\n",
    "    \n",
    "    curEnd = 0\n",
    "    for sent in sents:\n",
    "        curEnd += len(sent)\n",
    "        # if sentence ends after mention starts\n",
    "        if curEnd > mainWord[1]:\n",
    "            sEnd = curEnd\n",
    "            sStart = sEnd - len(sent)\n",
    "            mWIndex = textData['mentions'].index(mainWord) # index of mainWord\n",
    "            \n",
    "            # add every mention before main in sent to mentionsStr\n",
    "            for i in range(mWIndex-1, -1, -1):\n",
    "                if textData['mentions'][i][2] > sStart:\n",
    "                    mentionStrs.append(textData['text'][textData['mentions'][i][0]])\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            # add every mention after main in sent to mentionsStr\n",
    "            for i in range(mWIndex+1, len(textData['mentions'])):\n",
    "                if textData['mentions'][i][1] < sEnd:\n",
    "                    mentionStrs.append(textData['text'][textData['mentions'][i][0]])\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return \" \".join(mentionStrs).strip()\n",
    "\n",
    "def generateCandidates(textData, maxC, hybrid = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "        Hybrid: Whether to include best context fitting results too.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData. Each \n",
    "        mentions has its candidates of the form: [(wikiId, popularity),...]\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    ctxC0 = 0 # the amount of candidates to fill from best context.\n",
    "    if hybrid == True:\n",
    "        popC = int(maxC/2) + 1 # get ceil\n",
    "        ctxC0 = maxC - popC\n",
    "    else:\n",
    "        popC = maxC\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        resultT = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)[:popC]\n",
    "        results = [list(item) for item in resultT]\n",
    "        \n",
    "        # get the right amount to fill with context \n",
    "        if len(results) < popC and hybrid == True:\n",
    "            # fill in rest with context\n",
    "            ctxC = maxC - len(results)\n",
    "        elif hybrid == True:\n",
    "            ctxC = ctxC0\n",
    "        else:\n",
    "            ctxC = 0\n",
    "            \n",
    "        # get some context results from solr\n",
    "        if ctxC > 0:\n",
    "            mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "            ctxStr = escapeStringSolr(getMentionsInSentence(textData, mention))\n",
    "            \n",
    "            strIds = ['-id:' +  str(res[0]) for res in results]\n",
    "            \n",
    "            # select all the docs from Solr with the best scores, highest first.\n",
    "            addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "            \n",
    "            if len(ctxStr) == 0:\n",
    "                params={'fl':'id', 'indent':'on', 'fq':\" \".join(strIds),\n",
    "                        'q':'title:(' + mentionStr.encode('utf-8')+')^5',\n",
    "                        'wt':'json', 'rows': str(ctxC)}\n",
    "            else:\n",
    "                params={'fl':'id', 'indent':'on', 'fq':\" \".join(strIds),\n",
    "                        'q':'title:(' + mentionStr.encode('utf-8') + ')^5'\n",
    "                        + ' text:(' + ctxStr.encode('utf-8') + ')',\n",
    "                        'wt':'json', 'rows':str(ctxC)}\n",
    "            \n",
    "            r = requests.get(addr, params = params)\n",
    "            try:\n",
    "                if ('response' in r.json() \n",
    "                        and 'docs' in r.json()['response']\n",
    "                        and len(r.json()['response']['docs']) > 0):\n",
    "                    for doc in r.json()['response']['docs']:\n",
    "                        # get popularity of entity given the mention\n",
    "                        popularity = 0\n",
    "                        thingys = anchor2concept(textData['text'][mention[0]])\n",
    "                        for thingy in thingys:\n",
    "                            if thingy[0] == long(doc['id']):\n",
    "                                popularity = thingy[1]\n",
    "                                break\n",
    "                        \n",
    "                        results.append([long(doc['id']), popularity])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def mentionPrecision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherMentions against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def mentionRecall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherMentions against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    if asList == True:\n",
    "        words = (text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    else:\n",
    "        words = \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return words\n",
    "\n",
    "def getMentionSentence(text, mention, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            if asList == True:\n",
    "                sentence = (s.replace(text[mention[1]:mention[2]],\"\")).split(\" \")\n",
    "            else:\n",
    "                sentence = s.replace(text[mention[1]:mention[2]],\"\")\n",
    "            \n",
    "            return sentence\n",
    "        \n",
    "    # in case it missed\n",
    "    if asList == True:\n",
    "        return []\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def getContext1Scores(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    candScores = []\n",
    "    for i in range(len(candidates)):\n",
    "        candScores.append(0)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    try:\n",
    "        # assign the scores\n",
    "        for doc in r.json()['response']['docs']:\n",
    "            # find candidate of doc\n",
    "            i = 0\n",
    "            for cand in candidates:\n",
    "                if cand[0] == long(doc['id']):\n",
    "                    candScores[i] = doc['score']\n",
    "                    break\n",
    "                i += 1\n",
    "    except:\n",
    "        # keep zero scores\n",
    "        pass\n",
    "            \n",
    "    return candScores\n",
    "\n",
    "def bestContext1Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+context.encode('utf-8')+')^1 title:(' + mentionStr.encode('utf-8')+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    #for doc in r.json()['response']['docs']:\n",
    "        #print '[' + id2title(doc['id']) + '] -> ' + str(doc['score'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def getContext2Scores(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    candScores = []\n",
    "    for i in range(len(candidates)):\n",
    "        candScores.append(0)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+context.encode('utf-8')+') entity:(' + mentionStr.encode('utf-8') + ')^1',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    try:\n",
    "        # get count for each id\n",
    "        for doc in r.json()['response']['docs']:\n",
    "            scoreDict[str(doc['entityid'])] += 1\n",
    "    except:\n",
    "        # keep zero scores\n",
    "        pass\n",
    "    \n",
    "    # give scores to each cand\n",
    "    for j in range(0, len(candidates)):\n",
    "        candScores[j] = scoreDict[str(candidates[j][0])]\n",
    "            \n",
    "    return candScores\n",
    "\n",
    "def bestContext2Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+context.encode('utf-8')+') entity:(' + mentionStr.encode('utf-8') + ')^1',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        scoreDict[str(doc['entityid'])] += 1\n",
    "    \n",
    "    # get the index that has the best score\n",
    "    bestScore = 0\n",
    "    bestIndex = 0\n",
    "    curIndex = 0\n",
    "    for cand in candidates:\n",
    "        if scoreDict[str(cand[0])] > bestScore:\n",
    "            bestScore = scoreDict[str(cand[0])]\n",
    "            bestIndex = curIndex\n",
    "        curIndex += 1\n",
    "            \n",
    "    return bestIndex\n",
    "\n",
    "def getWord2VecScores(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the similarity scores of each mention to the context vector.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The scores of eac candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    candScores = []\n",
    "    for i in range(len(candidates)):\n",
    "        candScores.append(0)\n",
    "        \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        if math.isnan(score):\n",
    "            score = 0\n",
    "        candScores[i] = score\n",
    "        i += 1 # next index\n",
    "        \n",
    "    return candScores\n",
    "\n",
    "def bestWord2VecMatch(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the candidate with the best similarity to the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best similarity score with the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    bestIndex = 0\n",
    "    bestScore = 0\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        #print '[' + id2title(cand[0]) + ']' + ' -> ' + str(score)\n",
    "        # update score and index\n",
    "        if score > bestScore: \n",
    "            bestIndex = i\n",
    "            bestScore = score\n",
    "            \n",
    "        i += 1 # next index\n",
    "            \n",
    "    return bestIndex\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyContext(textData, candidates, oText, useSentence = False, window = 7, method2 = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                #context = getMentionSentence(oText, mention)\n",
    "                context = getMentionsInSentence(textData, mention)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + context\n",
    "            if method2 == False:\n",
    "                bestIndex = bestContext1Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            else:\n",
    "                bestIndex = bestContext2Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest similarity to the context.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window, asList = True)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention, asList = True)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + \" \".join(context)\n",
    "            bestIndex = bestWord2VecMatch(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyCoherence(textData, candidates, ws = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest coherence according to rvs pagerank method.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ws: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCands = [] # the top candidate from each candidate list\n",
    "    candsScores = coherence_scores_driver(candidates, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    i = -1 # track what mention we are on\n",
    "    for cScores in candsScores:\n",
    "        i += 1\n",
    "        \n",
    "        if len(cScores) == 0:\n",
    "            continue # nothing to do with this one\n",
    "            \n",
    "        bestScore = sorted(cScores, reverse = True)[0]\n",
    "        curIndex = 0\n",
    "        for score in cScores:\n",
    "            if score == bestScore:\n",
    "                topCands.append([textData['mentions'][i][1], textData['mentions'][i][2], candidates[i][curIndex][0]])\n",
    "                break\n",
    "            curIndex += 1\n",
    "            \n",
    "    return topCands\n",
    "\n",
    "mlModels = {} # dictionary of different models\n",
    "mlModelFiles = {\n",
    "    'abc': '/users/cs/amaral/wikisim/wikification/ml-models/model-abc-10000-hyb.pkl',\n",
    "    'bgc': '/users/cs/amaral/wikisim/wikification/ml-models/model-bgc-10000-hyb.pkl',\n",
    "    'etc': '/users/cs/amaral/wikisim/wikification/ml-models/model-etc-10000-hyb.pkl',\n",
    "    'gbc': '/users/cs/amaral/wikisim/wikification/ml-models/model-gbc-10000-hyb.pkl',\n",
    "    'rfc': '/users/cs/amaral/wikisim/wikification/ml-models/model-rfc-10000-hyb.pkl',\n",
    "    'lsvc': '/users/cs/amaral/wikisim/wikification/ml-models/model-lsvc-10000-hyb.pkl',\n",
    "    'svc': '/users/cs/amaral/wikisim/wikification/ml-models/model-svc-10000-hyb.pkl',\n",
    "    'lmart': '/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-10000-hyb.pkl',\n",
    "    'gbc-er': '/users/cs/amaral/wikisim/wikification/ml-models/er/er-model-gbc-30000.pkl',\n",
    "    'bgc-er': '/users/cs/amaral/wikisim/wikification/ml-models/er/er-model-bgc-30000.pkl'}\n",
    "\n",
    "def wikifyMulti(textData, candidates, oText, model, useSentence = True, window = 7):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Disambiguates each of the mentions with their given candidates using the desired\n",
    "        machine learned model.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text, unsplit.\n",
    "        model: The machine learned model to use for disambiguation: \n",
    "            'gbc' (gradient boosted classifier), 'etr' (extra trees regression), \n",
    "            'gbr' (gradient boosted regression), 'lmart' (LambdaMART (a learning to rank method)),\n",
    "            and 'rfr' (random forest regression).\n",
    "        useSentence: Whether to use windo size of sentence (for context methods)\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mlModel = mlModels[model] # get reference to model\n",
    "    \n",
    "    # get score from coherence\n",
    "    cohScores = coherence_scores_driver(candidates, 5, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    \n",
    "    i = 0\n",
    "    # get scores from each disambiguation method for all mentions\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > -1: # stub\n",
    "            # get the scores from each basic method.\n",
    "            \n",
    "            # normalize popularity scores\n",
    "            cScrs = []\n",
    "            for cand in candidates[i]:\n",
    "                cScrs.append(cand[1])\n",
    "            cScrs = normalize(cScrs)\n",
    "            j = 0\n",
    "            for cand in candidates[i]:\n",
    "                cand[1] = cScrs[j]\n",
    "                j += 1\n",
    "            \n",
    "            contextMInS = getMentionsInSentence(textData, textData['mentions'][i])\n",
    "            contextS = getMentionSentence(oText, textData['mentions'][i], asList = True)\n",
    "            \n",
    "            # context 1 scores\n",
    "            cScrs = getContext1Scores(textData['text'][mention[0]], contextMInS, candidates[i])\n",
    "            cScrs = normalize(cScrs)\n",
    "            # apply score to candList\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cScrs[j])\n",
    "            \n",
    "            # context 2 scores\n",
    "            cScrs = getContext2Scores(textData['text'][mention[0]], contextMInS, candidates[i])\n",
    "            cScrs = normalize(cScrs)\n",
    "            # apply score to candList\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cScrs[j])\n",
    "\n",
    "            # get score form word2vec\n",
    "            cScrs = getWord2VecScores(contextS, candidates[i])\n",
    "            #cScrs = normalize(cScrs)\n",
    "            # apply score to candList\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cScrs[j])\n",
    "\n",
    "            # get score from coherence\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cohScores[i][j])\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    topCandidates = []\n",
    "    \n",
    "    i = 0\n",
    "    # go through all mentions again to disambiguate with ml model\n",
    "    for mention in textData['mentions']:\n",
    "        try:\n",
    "            Xs = [cand[1:] for cand in candidates[i]]\n",
    "            if len(Xs) == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            pred = mlModel.predict(Xs)\n",
    "        except:\n",
    "            try:\n",
    "                Xs = [cand[1:] for cand in candidates[i]]\n",
    "                pred = mlModel.predict(np.array(candidates[i][1:]).reshape(1, -1))\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "        cur = 0\n",
    "        best = 0\n",
    "        bestI = 0\n",
    "        for j in range(len(pred)):\n",
    "            if pred[j] > best:\n",
    "                best = pred[j]\n",
    "                bestI = j\n",
    "        \n",
    "        topCandidates.append([mention[1], mention[2], candidates[i][bestI][0]])\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', strict = False, hybridC = True, model = 'lmart'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "        hybridC: Whether to split generated candidates between best of most frequent of most context related.\n",
    "        model: What model to use if using machine learning based method. LambdaMART as 'lmart' is default.\n",
    "            Other options are: 'gbc' (gradient boosted classifier), 'etr' (extra trees regression), \n",
    "            'gbr' (gradient boosted regression), and 'rfr' (random forest regression).\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        textData = mentionExtract(text) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    if method == 'popular':\n",
    "        maxC = 1 # only need one cand for popular\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC, hybridC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'context1':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    elif method == 'context2':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        wikified = wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    elif method == 'multi':\n",
    "        if model not in mlModels:\n",
    "            mlModels[model] = pickle.load(open(mlModelFiles[model], 'rb'))\n",
    "        wikified = wikifyMulti(textData, candidates, oText, model, useSentence = True, window = 7)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "    \n",
    "    return wikified\n",
    "\n",
    "def doWikify(text, maxC = 20, hybridC = False, method = 'multi'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in text, and returns the location of mentions as well as the\n",
    "        entities they refer to.\n",
    "    Args:\n",
    "        text: The text to be wikified.\n",
    "    Return:\n",
    "        A list of mentions where each element contains the character offset\n",
    "        start and end, as well as the corresponding wikipedia page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the mentions\n",
    "    # text data now has text in split form and, the mentions\n",
    "    textData = mentionExtract(text)\n",
    "    \n",
    "    # generate candidates\n",
    "    candidates = generateCandidates(textData, maxC, hybridC)\n",
    "    \n",
    "    # disambiguate each mention to its candidates\n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'context1':\n",
    "        wikified = wikifyContext(textData, candidates, text, useSentence = True, window = 7)\n",
    "    elif method == 'context2':\n",
    "        wikified = wikifyContext(textData, candidates, text, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        try:\n",
    "            word2vec\n",
    "        except:\n",
    "            word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "        wikified = wikifyWord2Vec(textData, candidates, text, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    elif method == 'multi':\n",
    "        try:\n",
    "            word2vec\n",
    "        except:\n",
    "            word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "        if 'lmart' not in mlModels:\n",
    "            mlModels['lmart'] = pickle.load(open(mlModelFiles['lmart'], 'rb'))\n",
    "        wikified = wikifyMulti(textData, candidates, text, 'lmart', useSentence = True, window = 7)\n",
    "    \n",
    "    return wikified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryan Amaral', 'is', 'a', 'student', 'at', 'Dalhousie University', 'in', 'Halifax', 'Nova', 'Scotia.']\n",
      "[[0, 0, 11], [5, 28, 48], [7, 52, 59], [61, 72]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0fb3ad670f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwikipedia\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mid2title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor2concept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mthe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikifyEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ryan Amaral is a student at Dalhousie University in Halifax, Nova Scotia.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'context2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybridC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#for thing in the:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/wikisim/wikification/wikification.py\u001b[0m in \u001b[0;36mwikifyEval\u001b[0;34m(text, mentionsGiven, maxC, method, strict, hybridC, model)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0mmaxC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# only need one cand for popular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateCandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybridC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'popular'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/cs/amaral/wikisim/wikification/wikification.py\u001b[0m in \u001b[0;36mgenerateCandidates\u001b[0;34m(textData, maxC, hybrid)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mentions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         resultT = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n\u001b[0m\u001b[1;32m    579\u001b[0m                           reverse = True)[:popC]\n\u001b[1;32m    580\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresultT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from wikification import wikifyEval\n",
    "from wikipedia import id2title, anchor2concept\n",
    "\n",
    "the = wikifyEval('Ryan Amaral is a student at Dalhousie University in Halifax, Nova Scotia.', False, method = 'context2', hybridC = False)\n",
    "#print the\n",
    "#for thing in the:\n",
    "#    print id2title(thing[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile word-pos-data.py\n",
    "from __future__ import division\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Gets stats on the POS tag data of mentions and non-mentions.\n",
    "\"\"\"\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "dsPath = os.path.join(pathStrt,'wiki-mentions.30000.json')\n",
    "\n",
    "with open(dsPath, 'r') as dataFile:\n",
    "    dataLines = []\n",
    "    skip = 0\n",
    "    amount = 30000 # do 30000 for full\n",
    "    i = 0\n",
    "    for line in dataFile:\n",
    "        if i >= skip:\n",
    "            dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        i += 1\n",
    "        if i >= skip + amount:\n",
    "            break\n",
    "            \n",
    "mentionB = {}\n",
    "mentionC = {}\n",
    "mentionA = {}\n",
    "mentionBA = {}\n",
    "\n",
    "nonmentionB = {}\n",
    "nonmentionC = {}\n",
    "nonmentionA = {}\n",
    "nonmentionBA = {}\n",
    "\n",
    "mentions = 0\n",
    "nonmentions = 0\n",
    "\n",
    "lnum = 0\n",
    "for line in dataLines:\n",
    "    lnum += 1\n",
    "    print 'Line: ' + str(lnum)\n",
    "    \n",
    "    pos = nltk.pos_tag(line['text'])\n",
    "    for i in range(len(line['text'])):\n",
    "        # before\n",
    "        if i == 0:\n",
    "            keyB = 'NONE'\n",
    "        else:\n",
    "            keyB = pos[i-1][1]\n",
    "            \n",
    "        # current\n",
    "        keyC = pos[i][1]\n",
    "        \n",
    "        # after\n",
    "        if i == len(line['text']) - 1:\n",
    "            keyA = 'NONE'\n",
    "        else:\n",
    "            keyA = pos[i+1][1]\n",
    "        \n",
    "        if i in [mnt[0] for mnt in line['mentions']]: # is mention\n",
    "            mentions += 1\n",
    "            # before\n",
    "            try:\n",
    "                mentionB[keyB][0] += 1\n",
    "            except:\n",
    "                mentionB[keyB] = [1]\n",
    "            # current\n",
    "            try:\n",
    "                mentionC[keyC][0] += 1\n",
    "            except:\n",
    "                mentionC[keyC] = [1]\n",
    "            # after\n",
    "            try:\n",
    "                mentionA[keyA][0] += 1\n",
    "            except:\n",
    "                mentionA[keyA] = [1]\n",
    "            # before and after\n",
    "            try:\n",
    "                mentionBA[keyB + ' : ' + keyA][0] += 1\n",
    "            except:\n",
    "                mentionBA[keyB + ' : ' + keyA] = [1]\n",
    "        else: # is nonmention\n",
    "            nonmentions += 1\n",
    "            # before\n",
    "            try:\n",
    "                nonmentionB[keyB][0] += 1\n",
    "            except:\n",
    "                nonmentionB[keyB] = [1]\n",
    "            # current\n",
    "            try:\n",
    "                nonmentionC[keyC][0] += 1\n",
    "            except:\n",
    "                nonmentionC[keyC] = [1]\n",
    "            # after\n",
    "            try:\n",
    "                nonmentionA[keyA][0] += 1\n",
    "            except:\n",
    "                nonmentionA[keyA] = [1]\n",
    "            # before and after\n",
    "            try:\n",
    "                nonmentionBA[keyB + ' : ' + keyA][0] += 1\n",
    "            except:\n",
    "                nonmentionBA[keyB + ' : ' + keyA] = [1]\n",
    "                \n",
    "print 'Mentions', mentions\n",
    "print 'Non-Mentions', nonmentions\n",
    "                \n",
    "# apply portion to each pos tag (mentions)\n",
    "for key in mentionB.keys():\n",
    "    mentionB[key].append(mentionB[key][0]/mentions)\n",
    "for key in mentionC.keys():\n",
    "    mentionC[key].append(mentionC[key][0]/mentions)\n",
    "for key in mentionA.keys():\n",
    "    mentionA[key].append(mentionA[key][0]/mentions)\n",
    "for key in mentionBA.keys():\n",
    "    mentionBA[key].append(mentionBA[key][0]/mentions)\n",
    "# apply portion to each pos tag (nonmentions)\n",
    "for key in nonmentionB.keys():\n",
    "    nonmentionB[key].append(nonmentionB[key][0]/nonmentions)\n",
    "for key in nonmentionC.keys():\n",
    "    nonmentionC[key].append(nonmentionC[key][0]/nonmentions)\n",
    "for key in nonmentionA.keys():\n",
    "    nonmentionA[key].append(nonmentionA[key][0]/nonmentions)\n",
    "for key in nonmentionBA.keys():\n",
    "    nonmentionBA[key].append(nonmentionBA[key][0]/nonmentions)\n",
    "\n",
    "\n",
    "\"\"\" Already have this data\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-bef.tsv', 'w') as f:\n",
    "    for key in mentionB.keys():\n",
    "        f.write(key + '\\t' + str(mentionB[key][0]) + '\\t' + str(mentionB[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-cur.tsv', 'w') as f:\n",
    "    for key in mentionC.keys():\n",
    "        f.write(key + '\\t' + str(mentionC[key][0]) + '\\t' + str(mentionC[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-aft.tsv', 'w') as f:\n",
    "    for key in mentionA.keys():\n",
    "        f.write(key + '\\t' + str(mentionA[key][0]) + '\\t' + str(mentionA[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-bef.tsv', 'w') as f:\n",
    "    for key in nonmentionB.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionB[key][0]) + '\\t' + str(nonmentionB[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-cur.tsv', 'w') as f:\n",
    "    for key in nonmentionC.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionC[key][0]) + '\\t' + str(nonmentionC[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-aft.tsv', 'w') as f:\n",
    "    for key in nonmentionA.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionA[key][0]) + '\\t' + str(nonmentionA[key][1]) + '\\n')\n",
    "\"\"\"\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-befaft.tsv', 'w') as f:\n",
    "    for key in mentionBA.keys():\n",
    "        f.write(key + '\\t' + str(mentionBA[key][0]) + '\\t' + str(mentionBA[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-befaft.tsv', 'w') as f:\n",
    "    for key in nonmentionBA.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionBA[key][0]) + '\\t' + str(nonmentionBA[key][1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
