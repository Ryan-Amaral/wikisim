{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd_util.py \n",
    "\"\"\"A few general modules for disambiguation\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import unicodedata\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from wikisim.config import *\n",
    "\n",
    "from wikisim.calcsim import *\n",
    "def generate_candidates(S, M, max_t=20, enforce=False):\n",
    "    \"\"\" Given a sentence list (S) and  a mentions list (M), returns a list of candiates\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            max_t: maximum candiate per mention\n",
    "            enforce: Makes sure the \"correct\" entity is among the candidates\n",
    "        Outputs:\n",
    "         Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n",
    "    \n",
    "    \"\"\"\n",
    "    candslist=[]\n",
    "    for m in M:\n",
    "        \n",
    "        clist = anchor2concept(S[m[0]])\n",
    "        if not clist:\n",
    "            clist=((0L,1L),)\n",
    "        \n",
    "        clist = sorted(clist, key=lambda x: -x[1])\n",
    "        clist = clist[:max_t]\n",
    "        \n",
    "        smooth=0    \n",
    "        if enforce:          \n",
    "            wid = title2id(m[1])            \n",
    "    #         if wid is None:\n",
    "    #             raise Exception(m[1].encode('utf-8') + ' not found')\n",
    "            \n",
    "                        \n",
    "            trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n",
    "            if not trg:\n",
    "                trg=[(len(clist), (wid,0))]\n",
    "                smooth=1\n",
    "\n",
    "                \n",
    "            if smooth==1 or trg[0][0]>=max_t: \n",
    "                if clist:\n",
    "                    clist.pop()\n",
    "                clist.append(trg[0][1])\n",
    "            \n",
    "        s = sum(c[1]+smooth for c in clist )        \n",
    "        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n",
    "            \n",
    "        candslist.append(clist)\n",
    "    return  candslist \n",
    "\n",
    "def get_tp(gold_titles, ids):\n",
    "    \"\"\"Returns true positive number\n",
    "       Inputs: goled_titles: The correct titles\n",
    "               ids: The given ids\n",
    "       Outputs: returns a tuple of (true_positives, total_number_of_ids)\n",
    "    \n",
    "    \"\"\"\n",
    "    tp=0\n",
    "    for m,id2 in zip(gold_titles, ids):\n",
    "        if title2id(m[1]) == id2:\n",
    "            tp += 1\n",
    "    return [tp, len(ids)]\n",
    "\n",
    "def get_prec(tp_list):\n",
    "    \"\"\"Returns precision\n",
    "       Inputs: a list of (true_positive and total number) lists\n",
    "       Output: Precision\n",
    "    \"\"\"\n",
    "    overall_tp = 0\n",
    "    simple_count=0\n",
    "    overall_count=0\n",
    "    macro_prec = 0;\n",
    "    for tp, count in tp_list:\n",
    "        if tp is None:\n",
    "            continue\n",
    "        simple_count +=1    \n",
    "        overall_tp += tp\n",
    "        overall_count += count\n",
    "        macro_prec += float(tp)/count\n",
    "        \n",
    "    macro_prec = macro_prec/simple_count\n",
    "    micro_prec = float(overall_tp)/overall_count\n",
    "    \n",
    "    return micro_prec, macro_prec\n",
    "\n",
    "def solr_escape(s):\n",
    "    \"\"\"\n",
    "        Escape a string for solr\n",
    "    \"\"\"\n",
    "    #ToDo: probably && and || nead to be escaped as a whole, and also AND, OR, NOT are not included\n",
    "    to_sub=re.escape(r'+-&&||!(){}[]^\"~*?:\\/')\n",
    "    return re.sub('[%s]'%(to_sub,), r'\\\\\\g<0>', s)\n",
    "\n",
    "def solr_unescape(s):\n",
    "    \"\"\"\n",
    "        Escape a string for solr\n",
    "    \"\"\"\n",
    "    #ToDo: probably && and || nead to be escaped as a whole, and also AND, OR, NOT are not included\n",
    "    to_sub=re.escape(r'+-&&||!(){}[]^\"~*?:\\/')\n",
    "    return re.sub('\\\\\\([%s])'%(to_sub,), r'\\g<1>', s)\n",
    "\n",
    "def solr_encode(inputstr):\n",
    "    '''This function \"ideally\" should prepare the text in the correct encoding\n",
    "        which is utf-16, but I couldn't (cf. my encoding notes)\n",
    "        so for know, just make everything ascii!\n",
    "        Input: \n",
    "            A unicode string with any encoding\n",
    "        Output: \n",
    "            Ascii encoded string\n",
    "    '''\n",
    "    return unicodedata.normalize('NFKD', inputstr).encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coherence.py \n",
    "\"\"\"Diiferent coherence (context, key-entity) calculation, and \n",
    "    disambiguation.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "from wsd_util import *\n",
    "import numpy as np\n",
    "\n",
    "def get_candidate_representations(candslist, direction, method):\n",
    "    '''returns an array of vector representations. \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "      Outputs\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "                   is the representation of a candidate\n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where \n",
    "                   the embeddings for a concepts indicates start and end. In other words\n",
    "                   The embedding of candidates [ci1...cik] in candslist is\n",
    "                   cvec_arr[cveclist_bdrs[i][0]:cveclist_bdrs[i][1]] \n",
    "    '''\n",
    "    \n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(encode_entity(c[0], method, get_id=False), method=method, direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    return cvec_arr, cveclist_bdrs\n",
    "\n",
    "def entity_to_context_scores(candslist, direction, method):\n",
    "    ''' finds the similarity between each entity and its context representation\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            direction: embedding direction\n",
    "            method: similarity method\n",
    "        Returns:\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "                   reperesentation of the candidates for cij reside        \n",
    "           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n",
    "                    where sij  is the similarity of c[i,j] to to ci-th context\n",
    "                    \n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    cands_score_list=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        S=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                s = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                s=0                \n",
    "            if np.isnan(s):\n",
    "                s=0\n",
    "            S.append(s)\n",
    "        cands_score_list.append(S)\n",
    "\n",
    "    return cvec_arr, cveclist_bdrs, cands_score_list\n",
    "\n",
    "def key_criteria(cands_score):\n",
    "    ''' helper function for find_key_concept: returns a score indicating how good a key is x\n",
    "        Input:\n",
    "            scroes for candidates [ci1, ..., cik] in the form of (i, [(ri1, si1), ..., (rik, sik)] ) \n",
    "            where (rij,sij) indicates that sij is the similarity of c[i][rij] to to cith context\n",
    "            \n",
    "    '''\n",
    "    if len(cands_score[1])==0:\n",
    "        return -float(\"inf\")    \n",
    "    if len(cands_score[1])==1 or cands_score[1][1][1]==0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    return (cands_score[1][0][1]-cands_score[1][1][1]) / cands_score[1][1][1]\n",
    "\n",
    "def find_key_concept(candslist, direction, method):\n",
    "    ''' finds the key entity in the candidate list\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "            cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "            cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "            key_concept: the concept forwhich one of the candidates is the key entity\n",
    "            key_entity: candidate index for key_cancept that is detected to be key_entity\n",
    "            key_entity_vector: The embedding of key entity\n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs, cands_score_list = entity_to_context_scores(candslist, direction, method);\n",
    "    S=[sorted(enumerate(S), key=lambda x: -x[1]) for S in cands_score_list]\n",
    "        \n",
    "    key_concept, _ = max(enumerate(S), key=key_criteria)\n",
    "    key_entity = S[key_concept][0][0]\n",
    "    \n",
    "    b,e = cveclist_bdrs[key_concept]\n",
    "    \n",
    "    key_entity_vector =  cvec_arr[b:e][key_entity]    \n",
    "    return cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector\n",
    "\n",
    "def keyentity_candidate_scores(candslist, direction, method):\n",
    "    '''returns entity scores using key-entity scoring \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           \n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector = find_key_concept(candslist, direction, method)\n",
    "    \n",
    "    # Iterate \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        cand_scores=[]\n",
    "\n",
    "        for v in cvec:\n",
    "            try:\n",
    "                d = 1-sp.spatial.distance.cosine(key_entity_vector, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "    return candslist_scores\n",
    "\n",
    "\n",
    "\n",
    "def coherence_scores_driver(C, ws=5, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\"):\n",
    "    \"\"\" Assigns a score to every candidate \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: Windows size for chunking\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method, either keyentity or entitycontext\n",
    "        Output:\n",
    "            Candidate Scores\n",
    "        \n",
    "    \"\"\"\n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "    scores=[]    \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        if op_method == 'keydisamb':\n",
    "            scores += keyentity_candidate_scores(chunk_c, direction, method)\n",
    "            \n",
    "        if op_method == 'entitycontext':\n",
    "            _, _, candslist_scores = entity_to_context_scores(chunk_c, direction, method);\n",
    "            scores += candslist_scores\n",
    "            \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing Coherence\n",
    "\"\"\"\n",
    "from coherence import *\n",
    "# S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "# M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "\n",
    "S=[\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"]\n",
    "M=[[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]\n",
    "\n",
    "# S=[\"Phoenix, Arizona\"] \n",
    "# M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "\n",
    "coh_scores = coherence_scores_driver(C, ws=5, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "print coh_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd.py \n",
    "\"\"\"Context-based disambiguation and also Learning-To-Rank combination\n",
    "    of several features.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from collections import Counter\n",
    "import cPickle as pickle\n",
    "import sys\n",
    "from coherence import *\n",
    "#sys.path.insert(0,'..')\n",
    "\n",
    "#from wikisim.calcsim import *\n",
    "#from wsd.wsd import *\n",
    "# My methods\n",
    "#from senseembed_train_test.ipynb\n",
    "\n",
    "disam_model_file_name = os.path.join(home,'backup/datasets/ner/ltr.pkl')\n",
    "disam_model = pickle.load(open(disam_model_file_name, 'rb'))    \n",
    "\n",
    "\n",
    "def get_context(anchor, eid, rows=50000):\n",
    "    \"\"\"Returns the context\n",
    "       Inputs: \n",
    "           anchor: the anchor text\n",
    "           eid: The id of the entity this anchor points to\n",
    "       Output:\n",
    "           The context (windows size is, I guess, 20)       \n",
    "    \"\"\"\n",
    "    params={'wt':'json', 'rows':rows}\n",
    "    anchor = solr_escape(anchor)\n",
    "    \n",
    "    q='anchor:\"%s\" AND entityid:%s' % (anchor, eid)\n",
    "    params['q']=q\n",
    "    \n",
    "#     session = requests.Session()\n",
    "#     http_retries = Retry(total=20,\n",
    "#                     backoff_factor=.1)\n",
    "#     http = requests.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "#     session.mount('http://localhost:8983/solr', http)\n",
    "    \n",
    "    r = session.get(qstr, params=params).json()\n",
    "    if 'response' not in r: \n",
    "        print \"[terminating]\\t%s\",(str(r),)\n",
    "        sys.stdout.flush()\n",
    "        os._exit(0)\n",
    "        \n",
    "    if not r:\n",
    "        return []\n",
    "    return r['response']['docs']\n",
    "\n",
    "#from wsd\n",
    "def word2vec_context_candidate_scores (S, M, candslist, ws=5):\n",
    "    '''returns entity scores using the similarity with their context\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+1:pos+ws+1]\n",
    "        context_vec = sp.zeros(getword2vec_model().vector_size)\n",
    "        for c in context:\n",
    "            context_vec += getword2vector(c).as_matrix()\n",
    "        cand_scores=[]\n",
    "\n",
    "        for c in cands:\n",
    "            try:\n",
    "                cand_vector = getentity2vector(encode_entity(c[0],'word2vec', get_id=False))\n",
    "                d = 1-sp.spatial.distance.cosine(context_vec, cand_vector);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "#from wsd\n",
    "def word2vec_context_disambiguate(S, M, candslist):\n",
    "    '''Disambiguate a sentence using word-context similarity\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           \n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = word2vec_context_candidate_scores (S, M, candslist)\n",
    "                      \n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles \n",
    "\n",
    "\n",
    "\n",
    "#from wikisim\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "    q='+text:(%s)'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    D = r.json()['response']\n",
    "    return D['numFound']\n",
    "\n",
    "\n",
    "\n",
    "# Editing Ryan's code\n",
    "def context_to_profile_sim(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # put text in right format\n",
    "    if not context:\n",
    "        return [0]*len(candidates)\n",
    "    context = solr_escape(context)\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['id:' +  str(tid) for tid,_ in candidates])\n",
    "        \n",
    "\n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qst = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    #q='text:('+context+')^1 title:(' + mention+')^1.35'\n",
    "    q='text:('+context+')'\n",
    "    \n",
    "    params={'fl':'id score', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q, 'wt':'json','rows':len(candidates)}\n",
    "    \n",
    "    \n",
    "    r = requests.get(qst, params = params).json()['response']['docs']\n",
    "    id_score_map=defaultdict(float, {long(ri['id']):ri['score'] for ri in r})\n",
    "    id_score=[id_score_map[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "# Important TODO\n",
    "# This queriy is very much skewed toward popularity, better to replace space with AND\n",
    "#!!!! I don't like this implementation, instead of retrieving and counting, better to let the \n",
    "# solr does the counting, \n",
    "def context_to_context_sim(mention, context, candidates, rows=1000):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    if not context:\n",
    "        return [0]*len(candidates)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = solr_escape(context)\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['entityid:' +  str(tid) for tid,_ in candidates])\n",
    "    \n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    q=\"_context_:(%s) entity:(%s)\" % (context,mention)\n",
    "    q=\"_context_:(%s) \" % (context)\n",
    "    \n",
    "    params={'fl':'entityid', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q,'wt':'json', 'rows':rows}\n",
    "    r = requests.get(qstr, params = params)\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        cnt[long(doc['entityid'])] += 1\n",
    "    \n",
    "    id_score=[cnt[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "\n",
    "def context_candidate_scores (S, M, candslist, ws=5, method='c2c', skip_current=1):\n",
    "    '''returns entity scores using  context seatch\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "            method: Either 'c2p': for context to profile, or 'c2c' for context to context\n",
    "            skip_current: Whether or not include the current mention in the context\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        mention=S[pos]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+skip_current:pos+ws+1]\n",
    "        context=\" \".join(context)\n",
    "        \n",
    "        if method == 'c2p':\n",
    "            cand_scores=context_to_profile_sim(mention, context, cands)\n",
    "        if method == 'c2c':\n",
    "            cand_scores=context_to_context_sim(mention, context, cands)\n",
    "            \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "def mention_to_title_sim(mention, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the string similarity scores between the mention candidates.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # put text in right format\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['id:' +  str(tid) for tid,_ in candidates])\n",
    "        \n",
    "\n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qst = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    q='title:(' + mention+')'\n",
    "    \n",
    "    params={'fl':'id score', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q, 'wt':'json','rows':len(candidates)}\n",
    "    \n",
    "    \n",
    "    r = requests.get(qst, params = params).json()['response']['docs']\n",
    "    id_score_map=defaultdict(float, {long(ri['id']):ri['score'] for ri in r})\n",
    "    id_score=[id_score_map[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "def mention_candidate_score(S, M, candslist):\n",
    "    return [mention_to_title_sim(S[m[0]], c) for m,c in zip(M,candslist) ]\n",
    "\n",
    "def popularity_score(candslist):\n",
    "    \"\"\"Retrieves the popularity score from the candslist\n",
    "    \"\"\"\n",
    "    scores=[[s for _, s in cands] for cands in candslist]\n",
    "    return scores\n",
    "\n",
    "def normalize(scores_list):\n",
    "    \"\"\"Normalize a matrix, row-wise\n",
    "    \"\"\"\n",
    "    normalized_scoreslist=[]\n",
    "    for scores in scores_list:\n",
    "        smooth=0\n",
    "        if 0 in scores:\n",
    "            smooth=1\n",
    "        sum_s = sum(s+smooth for s in scores )        \n",
    "        n_scores = [float(s+smooth)/sum_s for s in scores]\n",
    "        normalized_scoreslist.append(n_scores)\n",
    "    return normalized_scoreslist\n",
    "        \n",
    "def normalize_minmax(scores_list):\n",
    "    \"\"\"Normalize a matrix, row-wise, using minmax technique\n",
    "    \"\"\"\n",
    "    normalized_scoreslist=[]\n",
    "    for scores in scores_list:\n",
    "        scores_min = min(scores)        \n",
    "        scores_max = max(scores)        \n",
    "        if scores_min == scores_max:\n",
    "            n_scores = [0]*len(scores)\n",
    "        else:\n",
    "            n_scores = [(float(s)-scores_min)/(scores_max-scores_min) for s in scores]\n",
    "        normalized_scoreslist.append(n_scores)\n",
    "    return normalized_scoreslist\n",
    "\n",
    "def find_max(candslist,candslist_scores):\n",
    "    '''Disambiguate a sentence using a list of candidate-score tuples\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, s11),...(c1k, s1k)],...[(cn1, sn1),...(c1m, s1m)]]\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "            \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "#Delete, useless\n",
    "def disambiguate_random(C):\n",
    "    '''Disambiguate using the given order (which can be random)\n",
    "        Input:\n",
    "            C: Candlist\n",
    "        Output:\n",
    "            Disambiguated entities\n",
    "    '''\n",
    "    \n",
    "    ids = [c[0][0] for c in C ]\n",
    "    titles= ids2title(ids)\n",
    "    return ids, titles\n",
    "\n",
    "def get_scores(S, M, C, method):\n",
    "    \"\"\" Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            S: Sentence\n",
    "            M: Metntions\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             key: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    scores=None\n",
    "    if method == 'popularity'  :\n",
    "        scores = popularity_score(C)\n",
    "    if method == 'keydisamb'  :\n",
    "        scores = coherence_scores_driver(C, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    if method == 'entitycontext'  :\n",
    "        scores = coherence_scores_driver(C, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "    if method == 'mention2entity'  :\n",
    "        scores = mention_candidate_score (S, M, C)\n",
    "    if method == 'context2context'  :\n",
    "        scores = context_candidate_scores (S, M, C, method='c2c')\n",
    "    if method == 'context2profile'  :\n",
    "        scores = context_candidate_scores (S, M, C, method='c2p')    \n",
    "    if method == 'learned'  :\n",
    "        scores = learned_scores (S, M, C)    \n",
    "        \n",
    "    scores = normalize_minmax(scores)    \n",
    "    return scores\n",
    "\n",
    "def formated_scores(scores):\n",
    "    \"\"\"Only for pretty-printing\n",
    "    \"\"\"\n",
    "    scores = [['{0:.2f}'.format(s) for s in cand_scores] for cand_scores in scores]\n",
    "    return scores\n",
    "\n",
    "def formated_all_scores(scores):\n",
    "    \"\"\"Only for pretty-printing\n",
    "    \"\"\"\n",
    "    scores = [[tuple('{0:.2f}'.format(s) for s in sub_scores) for sub_scores in cand_scores] for cand_scores in scores]\n",
    "    return scores\n",
    "\n",
    "def get_all_scores(S, M, C):\n",
    "    \"\"\"Give all scores as different lists\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            C: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "\n",
    "        Output:\n",
    "            Scores, in this format [[(c111,.., c1k1),...(cm11,.., cmks)],...[(c1n1,.., pm1s),...(c1m1,.., p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "        \n",
    "            Scores, in this format [[(c111, c11s),...(c1k1, c1ks)],...[(cn11, pn1s),...(c1m1, p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "    \"\"\"\n",
    "    all_scores= [get_scores(S, M, C, method) for method in \\\n",
    "           ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile']]\n",
    "    return [zip(*s) for s in zip(*all_scores)]\n",
    "\n",
    "\n",
    "\n",
    "def keyentity_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank'):\n",
    "    '''Disambiguate a sentence using key-entity method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = keyentity_candidate_scores (candslist, direction, method)\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles  \n",
    "\n",
    "def learned_scores (S, M, candslist):\n",
    "    '''returns entity scores using the learned (learned-to-rank method)\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    all_scores=get_all_scores(S,M,candslist)\n",
    "    return [disam_model.predict(cand_scores) for cand_scores in all_scores] \n",
    "\n",
    "def wsd(S, M, C, method='learned'):\n",
    "    '''Gets a sentence, mentions and candslist, and returns disambiguation\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: disambiguation method \n",
    "       Returns: \n",
    "           A disambiguated list in the form of  (true_entities, titles)\n",
    "    \n",
    "    '''\n",
    "    candslist_scores = get_scores(S, M, C, method)\n",
    "    return find_max(C,candslist_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gen_trainrep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gen_trainrep.py \n",
    "\"\"\" Create a train-set \n",
    "    entity_id, query_id, scores1, score2, ..., scoren, true/false (is it a correct entity)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from wsd import *\n",
    "sys.stdout.flush()\n",
    "\n",
    "max_t=20\n",
    "max_count=15000\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "outfile = os.path.join(home,'backup/datasets/ner/trainrepository.%s.30000.tsv'%(max_count,))\n",
    "\n",
    "dsname = os.path.join(home,'backup/datasets/ner/wiki-mentions.30000.json')\n",
    "\n",
    "count = 0          \n",
    "with open(dsname,'r') as ds, open(outfile,'w') as outf:\n",
    "    qid=0\n",
    "    for line in ds:                           \n",
    "        js = json.loads(line.decode('utf-8').strip());\n",
    "        S = js[\"text\"]\n",
    "        M = js[\"mentions\"]\n",
    "        count +=1        \n",
    "        print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))        \n",
    "        C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "        all_scores=get_all_scores(S,M,C)\n",
    "        for i in  range(len(C)):\n",
    "            m=M[i]\n",
    "            cands = C[i]\n",
    "            cand_scores = all_scores[i]\n",
    "            wid = title2id(m[1]) \n",
    "            for (eid,_),scores in zip (cands, cand_scores):\n",
    "                is_true_eid = (wid == eid)\n",
    "                string_scores=[str(s) for s in scores]\n",
    "                outf.write(\"\\t\".join([str(eid), str(qid)]+string_scores+[str(int(is_true_eid))])+\"\\n\")\n",
    "            qid += 1\n",
    "        if count >= max_count:\n",
    "            break\n",
    "print \"Done\"             \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_ltr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_ltr.py \n",
    "\"\"\" Train a LambdaMart (LTR) Method\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import pyltr\n",
    "import pandas as pd\n",
    "import os\n",
    "from wsd import *\n",
    "nrows=50\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "tr_file_name = os.path.join(home,'backup/datasets/ner/trainrepository.30000.tsv')\n",
    "data=pd.read_table(tr_file_name, header=None)\n",
    "#data=data.head(100)\n",
    "num_cols = len(data.columns)\n",
    "\n",
    "grouped=data.groupby(1)\n",
    "total_len=len(grouped)\n",
    "group = grouped.filter(lambda x:x.iloc[0,1] >= 0 and x.iloc[0,1] < 0.6*total_len)\n",
    "X_train = group.iloc[:,2:num_cols-1].as_matrix()\n",
    "y_train = group.iloc[:,num_cols-1].as_matrix()\n",
    "qid_train = group.iloc[:,1].as_matrix()\n",
    "\n",
    "group=grouped.filter(lambda x:x.iloc[0,1] >= 0.6*total_len and x.iloc[0,1] < 0.8*total_len)\n",
    "X_validate = group.iloc[:,2:num_cols-1].as_matrix()\n",
    "y_validate = group.iloc[:,num_cols-1].as_matrix()\n",
    "qid_validate = group.iloc[:,1].as_matrix()\n",
    "\n",
    "\n",
    "group=grouped.filter(lambda x:x.iloc[0,1] >= 0.8*total_len and x.iloc[0,1] < 1.0*total_len)\n",
    "X_test = group.iloc[:,2:num_cols-1].as_matrix()\n",
    "y_test = group.iloc[:,num_cols-1].as_matrix()\n",
    "qid_test = group.iloc[:,1].as_matrix()\n",
    "\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "     X_validate, y_validate, qid_validate, metric=pyltr.metrics.NDCG(k=10), stop_after=250)\n",
    "model = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 0)\n",
    "#lmart.fit(TX, TY, Tqid, monitor=monitor)\n",
    "model.fit(X_train, y_train, qid_train, monitor=monitor)\n",
    "\n",
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "Ts_pred = model.predict(X_test)\n",
    "print 'Random ranking:', metric.calc_mean_random(qid_test, y_test)\n",
    "print 'Our model:', metric.calc_mean(qid_test, y_test, Ts_pred)\n",
    "\n",
    "import cPickle as pickle\n",
    "model_file_name = os.path.join(home,'backup/datasets/ner/ltr.pkl')\n",
    "\n",
    "pickle.dump(model, open(model_file_name, 'wb'))\n",
    "\n",
    "print 'Model saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mention_detection.py \n",
    "\n",
    "from wsd import *\n",
    "\n",
    "#constants\n",
    "CORE_NLP=0\n",
    "LEARNED_MENTION=1\n",
    "\n",
    "\n",
    "def tokenize_stanford(text):\n",
    "    addr = 'http://localhost:9001'\n",
    "    params={'annotators': 'tokenize', 'outputFormat': 'json'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "    \n",
    "    return [token['originalText'] for token in r.json()['tokens']]\n",
    "\n",
    "def encode_solrtexttagger_result(text,tags):\n",
    "    \"\"\" Convert the solrtext output to our M,S format\n",
    "        input:\n",
    "            text: The original text\n",
    "            tags: The result of the solrtexttagger\n",
    "        output:\n",
    "            S,M\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "    \"\"\"\n",
    "    start=0\n",
    "    termindex=0\n",
    "    S=[]\n",
    "    M=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for tag in tags:\n",
    "        assert text[tag[1]:tag[3]] == tag[5]\n",
    "        seg = text[start:tag[1]]\n",
    "        S += seg.strip().split()\n",
    "        M.append([len(S),'UNKNOWN'])\n",
    "        S += [\" \".join(text[tag[1]:tag[3]].split())]\n",
    "        start = tag[3]\n",
    "        \n",
    "    S += text[start:].strip().split()\n",
    "    return S, M\n",
    "\n",
    "def annotate_with_solrtagger(text):\n",
    "    ''' Annonate a text using solrtexttagger\n",
    "        Input: \n",
    "            text: The input text *must be unicode*\n",
    "        Output:\n",
    "            Annotated text\n",
    "    '''\n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on','matchText':'true'}\n",
    "    text=solr_escape(text)\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "\n",
    "    S,M = encode_solrtexttagger_result(text,r.json()['tags'])\n",
    "    return S,M\n",
    "\n",
    "\n",
    "def encode_corenlp_result(text,annotated):\n",
    "    \"\"\" Convert the corenlp output to our M,S format\n",
    "        input:\n",
    "            text: The original text\n",
    "            mentions: The result of the solrtexttagger\n",
    "        output:\n",
    "            S,M\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "    \"\"\"\n",
    "    #****** Important ****\n",
    "    #* The indices are not correct if it contains unicode, \n",
    "    #* in case you need to work with the indices, decode to utf-8\n",
    "    #******\n",
    "    S=[]\n",
    "    M=[]\n",
    "    P=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for sentence in annotated['sentences']: \n",
    "        start=0\n",
    "        \n",
    "        for mention in sentence['entitymentions']:\n",
    "            S += [token['originalText'] for token in sentence['tokens'][start:mention['tokenBegin']]]\n",
    "            M.append([len(S),'UNKNOWN'])\n",
    "            mentionstr = mention['text']\n",
    "            S += [mentionstr]\n",
    "            start = mention['tokenEnd']\n",
    "\n",
    "        S += [token['originalText'] for token in sentence['tokens'][start:]]\n",
    "        P += [[token['originalText'],token['pos']] for token in sentence['tokens']]\n",
    "    return S, M, P\n",
    "\n",
    "def annotate_with_corenlp(text):\n",
    "    ''' Annonate a text using coreNLP\n",
    "        Input: \n",
    "            text: The input text\n",
    "        Output:\n",
    "            Annotated text\n",
    "    '''\n",
    "    addr = 'http://localhost:9001'\n",
    "    params={'annotators': 'entitymentions', 'outputFormat': 'json'}\n",
    "    r = requests.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "    \n",
    "    S,M, P = encode_corenlp_result(text, r.json())\n",
    "    return S,M,P\n",
    "\n",
    "def solrtagger_pos(S,M,P):\n",
    "    ''' Alligns the tags from corenlp to solrtagger's mentions\n",
    "        Input:\n",
    "            S: Sentence \n",
    "            M: Mentions\n",
    "            P: POS of the mentions, from corenlp\n",
    "        Output:\n",
    "            Q: POS of solrtagger's mentions\n",
    "    '''\n",
    "    Q=[]\n",
    "    j=0\n",
    "    for i in range(len(M)):\n",
    "        m=tokenize_stanford(solr_unescape(S[M[i][0]])) \n",
    "        j_backup=j\n",
    "        q=[]\n",
    "        while j<len(P):\n",
    "            if similar(P[j][0], m[0])> .8:\n",
    "                k=0\n",
    "                while similar(P[j][0], m[k])>0.8:\n",
    "                    #q.append(P[j]) #good for debugging\n",
    "                    q.append(P[j][1]) #good for debugging\n",
    "                    k=k+1\n",
    "                    j=j+1\n",
    "                    if j >= len(P) or k>=len(m):\n",
    "                        break\n",
    "\n",
    "                Q.append(\" \".join(q))\n",
    "                break\n",
    "            j=j+1\n",
    "        if not q:\n",
    "            Q.append(\"NA\")\n",
    "            j=j_backup\n",
    "    return Q\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(c for _,c in anchor2concept(s))  \n",
    "\n",
    "def mention_prob(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_mentions = get_mention_count(text)\n",
    "    total_appearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    if total_appearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    return float(total_mentions)/total_appearances\n",
    "\n",
    "def get_mention_probs(S,M):\n",
    "    return [mention_prob(S[m[0]]) for m in M]\n",
    "\n",
    "\n",
    "def boil_down_candidate_score(score_list):\n",
    "    return [sum(scores)/len(scores) for scores in scores_list]\n",
    "        \n",
    "    \n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def mention_overlap(S1, M1, S2,M2):\n",
    "    '''Calculates the overlap between two given detected mentions\n",
    "        Input:\n",
    "            S1: Source Setnence\n",
    "            M1: Source Mention\n",
    "            S2: Destination Sentence\n",
    "            M2: Destination mention            \n",
    "        Output: A 0/1 vector of size M1, each element shows whether M1[i] is also in M2\n",
    "    '''\n",
    "    is_detected = []\n",
    "    for m1 in M1:\n",
    "        found = 0\n",
    "        for m2 in M2:\n",
    "            if similar(S1[m1[0]], S2[m2[0]])>0.8:\n",
    "                found=1\n",
    "        is_detected.append(found)\n",
    "    return is_detected\n",
    "\n",
    "def detect_and_score_mentions(text, max_t=5):\n",
    "    \"\"\"Give\n",
    "        Uses solrtagger to detect mentions, and score them\n",
    "        Inputs:\n",
    "            text: Given text\n",
    "        Output:\n",
    "            Scores, in this format [[(c111, c11s),...(c1k1, c1ks)],...[(cn11, pn1s),...(c1m1, p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "    \"\"\"\n",
    "    assert type(text) is str\n",
    "    solr_S, solr_M = annotate_with_solrtagger(text)\n",
    "    # max_t does not have to equal the number of candidates in wsd, it's just to \n",
    "    # get an average relevancy\n",
    "    solr_C = generate_candidates(solr_S, solr_M, max_t=max_t, enforce=False)\n",
    "    \n",
    "    \n",
    "    wsd_scores = [[sum(sc)/len(sc) for sc in get_scores(solr_S, solr_M, solr_C, method)] for method in \\\n",
    "               ['popularity','entitycontext','mention2entity','context2context','context2profile']]\n",
    "\n",
    "    mention_scores=[]\n",
    "    mention_scores.extend(wsd_scores)\n",
    "    mention_scores.append(get_mention_probs(solr_S, solr_M))\n",
    "    \n",
    "    core_S, core_M, core_P = annotate_with_corenlp(text)\n",
    "    overlap_with_corenlp = mention_overlap(solr_S, solr_M, core_S,core_M)\n",
    "    mention_scores.append(overlap_with_corenlp)\n",
    "    \n",
    "    pos_list = solrtagger_pos(solr_S, solr_M,core_P)\n",
    "    mention_scores.append(pos_list)\n",
    "    \n",
    "    return solr_S, solr_M, zip(*mention_scores)\n",
    "\n",
    "def get_learned_mentions(text):\n",
    "#     S_solr,M_solr,scores = detect_and_score_mentions(text)\n",
    "#     M_scores = mention_model.predict(scores) for cand_scores in all_scores\n",
    "#     M = [m for m_s, m in zip(M_scores, M_solr) if m_s==1]\n",
    "    return M,S\n",
    "    \n",
    "def detect_mentions(line, mentionmethod=CORE_NLP):\n",
    "#     if mentionmethod == CORE_NLP:\n",
    "#         return core_S, core_M = annotate_with_corenlp(line)        \n",
    "#     if mention_metho = LEARNED_MENTION:\n",
    "    return get_learned_mentions(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gen_trainrep_for_mention.py \n",
    "\"\"\" Create a train-set \n",
    "    entity_id, query_id, scores1, score2, ..., scoren, true/false (is it a correct entity)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from mention_detection import *\n",
    "from wsd import *\n",
    "sys.stdout.flush()\n",
    "\n",
    "max_count=2\n",
    "skip_lines=0\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "outfile = os.path.join(home,'backup/datasets/ner/mentiontrainrepository.%s.30000.tsv'%(max_count,))\n",
    "\n",
    "dsname = os.path.join(home,'backup/datasets/ner/wiki-mentions.30000.json')\n",
    "\n",
    "count = 0  \n",
    "mention_id = 0\n",
    "with open(dsname,'r') as ds, open(outfile,'w') as outf:\n",
    "    for line in ds:                           \n",
    "        count +=1  \n",
    "        if count <= skip_lines:\n",
    "            continue\n",
    "        js = json.loads(line.decode('utf-8').strip());\n",
    "        S = js[\"text\"]\n",
    "        M = js[\"mentions\"]\n",
    "        text= \" \".join(S)\n",
    "        text = solr_encode(text)\n",
    "        print \"%s:\\tS=%s\\n\\tM=%s\\ttext=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'),text)        \n",
    "        \n",
    "        solr_S, solr_M, scores = detect_and_score_mentions(text)\n",
    "        correct_mention = mention_overlap(solr_S, solr_M, S, M)\n",
    "        for i in  range(len(solr_M)):\n",
    "            string_scores=[str(s) for s in scores[i]]\n",
    "            outf.write(\"\\t\".join([str(mention_id)] + string_scores+[str(correct_mention[i])])+\"\\n\")\n",
    "            mention_id += 1\n",
    "        if count >= max_count:\n",
    "            break\n",
    "print \"Done\"             \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile wikify.py \n",
    "from __future__ import division\n",
    "from wsd import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wikify_string(line, mentionmethod=CORE_NLP):\n",
    "    S,M = detect_mentions(line, mentionmethod):        \n",
    "    C = generate_candidates(S, M, max_t=20, enforce=False)\n",
    "    E = wsd(S, M, C, method='learned'\n",
    "    return S,M,E\n",
    "\n",
    "def wikify_a_line(line, mentionmethod=CORE_NLP):\n",
    "    S, M, E = wikify_string(line, mentionmethod=CORE_NLP) \n",
    "    for m,e in zip(M, E[1]): \n",
    "        S[m[0]]=\"<a href=https://en.wikipedia.org/wiki/%s>%s</a>\"  % (S[m[0]],e)\n",
    "    S_reconcat = \" \".join(S)\n",
    "    return S_reconcat\n",
    "def wikify_api(text, mentionmethod=CORE_NLP):\n",
    "    for line in text.splitlines():\n",
    "        outlist.append(wikify_a_line(line, mentionmethod))\n",
    "    return \"\\n\".join(outlist)\n",
    "\n",
    "def wikify_from_file_api(infilename, outfilename, mentionmethod=CORE_NLP):\n",
    "    with open(infilename) as infile, open(outfilename, 'w') as outfile:\n",
    "        for line in infilename.readlines():\n",
    "            wikified = wikify_a_line(text, mentionmethod)\n",
    "            outfile.write(wikified + \"\\n\")\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
