{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wikify_util.py \n",
    "\"\"\"A few general modules for disambiguation\n",
    "\"\"\"\n",
    "import sys\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from wikisim.config import *\n",
    "\n",
    "from wikisim.calcsim import *\n",
    "\n",
    "def generate_candidates(S, M, max_t=10, enforce=False):\n",
    "    \"\"\" Given a sentence list (S) and  a mentions list (M), returns a list of candiates\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            max_t: maximum candiate per mention\n",
    "            enforce: Makes sure the \"correct\" entity is among the candidates\n",
    "        Outputs:\n",
    "         Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n",
    "    \n",
    "    \"\"\"\n",
    "    candslist=[]\n",
    "    for m in M:\n",
    "        \n",
    "        clist = anchor2concept(S[m[0]])\n",
    "        if not clist:\n",
    "            clist=((0L,1L),)\n",
    "        \n",
    "        clist = sorted(clist, key=lambda x: -x[1])\n",
    "        clist = clist[:max_t]\n",
    "        \n",
    "        smooth=0    \n",
    "        if enforce:          \n",
    "            wid = title2id(m[1])            \n",
    "    #         if wid is None:\n",
    "    #             raise Exception(m[1].encode('utf-8') + ' not found')\n",
    "            \n",
    "                        \n",
    "            trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n",
    "            if not trg:\n",
    "                trg=[(len(clist), (wid,0))]\n",
    "                smooth=1\n",
    "\n",
    "                \n",
    "            if smooth==1 or trg[0][0]>=max_t: \n",
    "                if clist:\n",
    "                    clist.pop()\n",
    "                clist.append(trg[0][1])\n",
    "            \n",
    "        s = sum(c[1]+smooth for c in clist )        \n",
    "        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n",
    "            \n",
    "        candslist.append(clist)\n",
    "    return  candslist \n",
    "\n",
    "def get_tp(gold_titles, ids):\n",
    "    \"\"\"Returns true positive number\n",
    "       Inputs: goled_titles: The correct titles\n",
    "               ids: The given ids\n",
    "       Outputs: returns a tuple of (true_positives, total_number_of_ids)\n",
    "    \n",
    "    \"\"\"\n",
    "    tp=0\n",
    "    for m,id2 in zip(gold_titles, ids):\n",
    "        if title2id(m[1]) == id2:\n",
    "            tp += 1\n",
    "    return [tp, len(ids)]\n",
    "\n",
    "def get_prec(tp_list):\n",
    "    \"\"\"Returns precision\n",
    "       Inputs: a list of (true_positive and total number) lists\n",
    "       Output: Precision\n",
    "    \"\"\"\n",
    "    overall_tp = 0\n",
    "    simple_count=0\n",
    "    overall_count=0\n",
    "    macro_prec = 0;\n",
    "    for tp, count in tp_list:\n",
    "        if tp is None:\n",
    "            continue\n",
    "        simple_count +=1    \n",
    "        overall_tp += tp\n",
    "        overall_count += count\n",
    "        macro_prec += float(tp)/count\n",
    "        \n",
    "    macro_prec = macro_prec/simple_count\n",
    "    micro_prec = float(overall_tp)/overall_count\n",
    "    \n",
    "    return micro_prec, macro_prec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Diiferent coherence (context, key-entity) calculation, and \n",
    "    disambiguation.\n",
    "\"\"\"\n",
    "%%writefile coherence.py \n",
    "\n",
    "from wikify_util import *\n",
    "import numpy as np\n",
    "\n",
    "def get_candidate_representations(candslist, direction, method):\n",
    "    '''returns an array of vector representations. \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "      Outputs\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "                   is the representation of a candidate\n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where \n",
    "                   the embeddings for a concepts indicates start and end. In other words\n",
    "                   The embedding of candidates [ci1...cik] in candslist is\n",
    "                   cvec_arr[cveclist_bdrs[i][0]:cveclist_bdrs[i][1]] \n",
    "    '''\n",
    "    \n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(encode_entity(c[0], method, get_id=False), method=method, direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    return cvec_arr, cveclist_bdrs\n",
    "\n",
    "def entity_to_context_scores(candslist, direction, method):\n",
    "    ''' finds the similarity between each entity and its context representation\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            direction: embedding direction\n",
    "            method: similarity method\n",
    "        Returns:\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "                   reperesentation of the candidates for cij reside        \n",
    "           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n",
    "                    where sij  is the similarity of c[i,j] to to ci-th context\n",
    "                    \n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    cands_score_list=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        S=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                s = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                s=0                \n",
    "            if np.isnan(s):\n",
    "                s=0\n",
    "            S.append(s)\n",
    "        cands_score_list.append(S)\n",
    "\n",
    "    return cvec_arr, cveclist_bdrs, cands_score_list\n",
    "\n",
    "def key_criteria(cands_score):\n",
    "    ''' helper function for find_key_concept: returns a score indicating how good a key is x\n",
    "        Input:\n",
    "            scroes for candidates [ci1, ..., cik] in the form of (i, [(ri1, si1), ..., (rik, sik)] ) \n",
    "            where (rij,sij) indicates that sij is the similarity of c[i][rij] to to cith context\n",
    "            \n",
    "    '''\n",
    "    if len(cands_score[1])==0:\n",
    "        return -float(\"inf\")    \n",
    "    if len(cands_score[1])==1 or cands_score[1][1][1]==0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    return (cands_score[1][0][1]-cands_score[1][1][1]) / cands_score[1][1][1]\n",
    "\n",
    "def find_key_concept(candslist, direction, method):\n",
    "    ''' finds the key entity in the candidate list\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "            cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "            cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "            key_concept: the concept forwhich one of the candidates is the key entity\n",
    "            key_entity: candidate index for key_cancept that is detected to be key_entity\n",
    "            key_entity_vector: The embedding of key entity\n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs, cands_score_list = entity_to_context_scores(candslist, direction, method);\n",
    "    S=[sorted(enumerate(S), key=lambda x: -x[1]) for S in cands_score_list]\n",
    "        \n",
    "    key_concept, _ = max(enumerate(S), key=key_criteria)\n",
    "    key_entity = S[key_concept][0][0]\n",
    "    \n",
    "    b,e = cveclist_bdrs[key_concept]\n",
    "    \n",
    "    key_entity_vector =  cvec_arr[b:e][key_entity]    \n",
    "    return cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector\n",
    "\n",
    "def keyentity_candidate_scores(candslist, direction, method, ver):\n",
    "    '''returns entity scores using key-entity scoring \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           ver: 1 for the method explained in the paper\n",
    "           \n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector = find_key_concept(candslist, direction, method)\n",
    "    \n",
    "    # Iterate \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        cand_scores=[]\n",
    "\n",
    "        for v in cvec:\n",
    "            try:\n",
    "                d = 1-sp.spatial.distance.cosine(key_entity_vector, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "    return candslist_scores\n",
    "\n",
    "\n",
    "\n",
    "def coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\"):\n",
    "    \"\"\" Assigns a score to every candidate \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: Windows size for chunking\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method, either keyentity or entitycontext\n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "    scores=[]    \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        if op_method == 'keydisamb':\n",
    "            scores += keyentity_candidate_scores(chunk_c, direction, method,4)\n",
    "            \n",
    "        if op_method == 'entitycontext':\n",
    "            _, _, candslist_scores = entity_to_context_scores(chunk_c, direction, method);\n",
    "            scores += candslist_scores\n",
    "            \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing Coherence\n",
    "\"\"\"\n",
    "\n",
    "# S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "# M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "\n",
    "S=[\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"]\n",
    "M=[[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]\n",
    "\n",
    "# S=[\"Phoenix, Arizona\"] \n",
    "# M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "\n",
    "coh_scores = coherence_scores_driver(C, ws=5, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "print coh_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Context-based disambiguation and also Learning-To-Rank combination\n",
    "    of several features.\n",
    "\"\"\"\n",
    "%%writefile wikify.py \n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from collections import Counter\n",
    "import cPickle as pickle\n",
    "import sys\n",
    "from coherence import *\n",
    "#sys.path.insert(0,'..')\n",
    "\n",
    "#from wikisim.calcsim import *\n",
    "#from wsd.wsd import *\n",
    "# My methods\n",
    "#from senseembed_train_test.ipynb\n",
    "\n",
    "disam_model_file_name = os.path.join(home,'backup/datasets/ner/ltr.pkl')\n",
    "disam_model = pickle.load(open(disam_model_file_name, 'rb'))    \n",
    "\n",
    "def get_context(anchor, eid):\n",
    "    \"\"\"Returns the context\n",
    "       Inputs: \n",
    "           anchor: the anchor text\n",
    "           eid: The id of the entity this anchor points to\n",
    "       Output:\n",
    "           The context (windows size is, I guess, 20)       \n",
    "    \"\"\"\n",
    "    params={'wt':'json', 'rows':'50000'}\n",
    "    anchor = solr_escape(anchor)\n",
    "    \n",
    "    q='anchor:\"%s\" AND entityid:%s' % (anchor, eid)\n",
    "    params['q']=q\n",
    "    \n",
    "#     session = requests.Session()\n",
    "#     http_retries = Retry(total=20,\n",
    "#                     backoff_factor=.1)\n",
    "#     http = requests.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "#     session.mount('http://localhost:8983/solr', http)\n",
    "    \n",
    "    r = session.get(qstr, params=params).json()\n",
    "    if 'response' not in r: \n",
    "        print \"[terminating]\\t%s\",(str(r),)\n",
    "        sys.stdout.flush()\n",
    "        os._exit(0)\n",
    "        \n",
    "    if not r:\n",
    "        return []\n",
    "    return r['response']['docs']\n",
    "\n",
    "#from wsd\n",
    "def word2vec_context_candidate_scores (S, M, candslist, ws):\n",
    "    '''returns entity scores using the similarity with their context\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        #print \"At: \", M[i]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+1:pos+ws+1]\n",
    "        #print context\n",
    "        #print candslist[i], pos,context\n",
    "        context_vec = sp.zeros(getword2vec_model().vector_size)\n",
    "        for c in context:\n",
    "            #print \"getting vector for: \" , c\n",
    "            context_vec += getword2vector(c).as_matrix()\n",
    "        #print context_vec\n",
    "        cand_scores=[]\n",
    "\n",
    "        for c in cands:\n",
    "            try:\n",
    "                cand_vector = getentity2vector(encode_entity(c[0],'word2vec', get_id=False))\n",
    "                d = 1-sp.spatial.distance.cosine(context_vec, cand_vector);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "#from wsd\n",
    "def word2vec_context_disambiguate(S, M, candslist, ws ):\n",
    "    '''Disambiguate a sentence using word-context similarity\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           \n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = word2vec_context_candidate_scores (S, M, candslist, ws)\n",
    "                      \n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles \n",
    "\n",
    "\n",
    "def solr_escape(s):\n",
    "    \"\"\"\n",
    "        Escape a string for solr\n",
    "    \"\"\"\n",
    "    #ToDo: probably && and || nead to be escaped as a whole, and also AND, OR, NOT are not included\n",
    "    to_sub=re.escape(r'+-&&||!(){}[]^\"~*?:\\/')\n",
    "    return re.sub('[%s]'%(to_sub,), r'\\\\\\g<0>', s)\n",
    "\n",
    "#from wikisim\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "    q='+text:(%s)'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    D = r.json()['response']\n",
    "    return D['numFound']\n",
    "\n",
    "\n",
    "\n",
    "# Editing Ryan's code\n",
    "def context_to_profile_sim(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # put text in right format\n",
    "    if not context:\n",
    "        return [0]*len(candidates)\n",
    "    context = solr_escape(context)\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['id:' +  str(tid) for tid,_ in candidates])\n",
    "        \n",
    "\n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qst = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    q='text:('+context+')^1 title:(' + mention+')^1.35'\n",
    "    \n",
    "    params={'fl':'id score', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q, 'wt':'json'}\n",
    "    \n",
    "    #print params\n",
    "    \n",
    "    r = requests.get(qst, params = params).json()['response']['docs']\n",
    "    id_score_map=defaultdict(float, {long(ri['id']):ri['score'] for ri in r})\n",
    "    id_score=[id_score_map[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "# Important TODO\n",
    "# This queriy is very much skewed toward popularity, better to replace space with AND\n",
    "#!!!! I don't like this implementation, instead of retrieving and counting, better to let the \n",
    "# solr does the counting, \n",
    "def context_to_context_sim(mention, context, candidates, rows=10):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    if not context:\n",
    "        return [0]*len(candidates)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = solr_escape(context)\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['entityid:' +  str(tid) for tid,_ in candidates])\n",
    "    \n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    q=\"_context_:(%s) entity:(%s)\" % (context,mention)\n",
    "    \n",
    "    params={'fl':'entityid', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q,'wt':'json', 'rows':rows}\n",
    "    #print params\n",
    "    r = requests.get(qstr, params = params)\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        cnt[long(doc['entityid'])] += 1\n",
    "    \n",
    "    id_score=[cnt[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(c for _,c in anchor2concept(s))  \n",
    "\n",
    "def mention_prob(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_mentions = get_mention_count(text)\n",
    "    total_appearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    if total_appearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    return float(total_mentions)/total_appearances\n",
    "\n",
    "\n",
    "def context_candidate_scores (S, M, candslist, ws, method='c2c', rows=10):\n",
    "    '''returns entity scores using  context seatch\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "            method: Either 'c2p': for context to profile, or 'c2c' for context to context\n",
    "            rows: How many rows to retrieve \n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        mention=S[pos]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+1:pos+ws+1]\n",
    "        context=\" \".join(context)\n",
    "        #print \"mention: \",mention\n",
    "        #print \"context: \",context\n",
    "        \n",
    "        if method == 'c2p':\n",
    "            cand_scores=context_to_profile_sim(mention, context, cands)\n",
    "        if method == 'c2c':\n",
    "            cand_scores=context_to_context_sim(mention, context, cands, rows=rows)\n",
    "            \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "def popularity_score(candslist):\n",
    "    \"\"\"Retrieves the popularity score from the candslist\n",
    "    \"\"\"\n",
    "    scores=[[s for _, s in cands] for cands in candslist]\n",
    "    return scores\n",
    "\n",
    "def normalize(scores_list):\n",
    "    \"\"\"Normalize a matrix, row-wise\n",
    "    \"\"\"\n",
    "    normalized_scoreslist=[]\n",
    "    for scores in scores_list:\n",
    "        smooth=0\n",
    "        if 0 in scores:\n",
    "            smooth=1\n",
    "        sum_s = sum(s+smooth for s in scores )        \n",
    "        n_scores = [float(s+smooth)/sum_s for s in scores]\n",
    "        normalized_scoreslist.append(n_scores)\n",
    "    return normalized_scoreslist\n",
    "        \n",
    "def normalize_minmax(scores_list):\n",
    "    \"\"\"Normalize a matrix, row-wise, using minmax technique\n",
    "    \"\"\"\n",
    "    normalized_scoreslist=[]\n",
    "    for scores in scores_list:\n",
    "        scores_min = min(scores)        \n",
    "        scores_max = max(scores)        \n",
    "        if scores_min == scores_max:\n",
    "            n_scores = [0]*len(scores)\n",
    "        else:\n",
    "            n_scores = [(float(s)-scores_min)/(scores_max-scores_min) for s in scores]\n",
    "        normalized_scoreslist.append(n_scores)\n",
    "    return normalized_scoreslist\n",
    "\n",
    "def find_max(candslist,candslist_scores):\n",
    "    '''Disambiguate a sentence using a list of candidate-score tuples\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, s11),...(c1k, s1k)],...[(cn1, sn1),...(c1m, s1m)]]\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "            \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "\n",
    "def get_scores(S, M, C, ws, method, rows=10):\n",
    "    \"\"\" Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             key: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    scores=None\n",
    "    if method == 'popularity'  :\n",
    "        scores = popularity_score(C)\n",
    "    if method == 'keydisamb'  :\n",
    "        scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    if method == 'entitycontext'  :\n",
    "        scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "    if method == 'context2context'  :\n",
    "        scores = context_candidate_scores (S, M, C, ws, method='c2c', rows=rows)\n",
    "    if method == 'context2profile'  :\n",
    "        scores = context_candidate_scores (S, M, C, ws, method='c2p')    \n",
    "    if method == 'learned'  :\n",
    "        scores = learned_scores (S, M, C, ws)    \n",
    "        \n",
    "    scores = normalize_minmax(scores)    \n",
    "    return scores\n",
    "\n",
    "def formated_scores(scores):\n",
    "    \"\"\"Only for pretty-printing\n",
    "    \"\"\"\n",
    "    scores = [['{0:.2f}'.format(s) for s in cand_scores] for cand_scores in scores]\n",
    "    return scores\n",
    "\n",
    "def formated_all_scores(scores):\n",
    "    \"\"\"Only for pretty-printing\n",
    "    \"\"\"\n",
    "    scores = [[tuple('{0:.2f}'.format(s) for s in sub_scores) for sub_scores in cand_scores] for cand_scores in scores]\n",
    "    return scores\n",
    "\n",
    "def get_all_scores(S, M, C, ws, rows=10):\n",
    "    \"\"\"Give\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            C: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: windows size\n",
    "            rows: number of rows, for calculating context-based similarities\n",
    "\n",
    "        Output:\n",
    "            Scores, in this format [[(c111, c11s),...(c1k1, c1ks)],...[(cn11, pn1s),...(c1m1, p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "    \"\"\"\n",
    "    all_scores= [get_scores(S, M, C, ws, method, rows) for method in \\\n",
    "           ['popularity','keydisamb','entitycontext','context2context','context2profile']]\n",
    "    return [zip(*s) for s in zip(*all_scores)]\n",
    "\n",
    "\n",
    "\n",
    "def keyentity_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank', ver=4):\n",
    "    '''Disambiguate a sentence using key-entity method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           ver: 1 for the method explained in the paper\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = keyentity_candidate_scores (candslist, direction, method, ver)\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles  \n",
    "\n",
    "def learned_scores (S, M, candslist, ws):\n",
    "    '''returns entity scores using the learned (learned-to-rank method)\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    all_scores=get_all_scores(S,M,candslist, ws, 10)\n",
    "    return [disam_model.predict(cand_scores) for cand_scores in all_scores] \n",
    "\n",
    "def wikify(S, M, C, ws, method, rows=10):\n",
    "    candslist_scores = get_scores(S, M, C, ws, method, rows)\n",
    "    return find_max(C,candslist_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gen_trainrep.py \n",
    "\"\"\" Create a train-set \n",
    "    entity_id, query_id, scores1, score2, ..., scoren, true/false (is it a correct entity)\n",
    "\"\"\"\n",
    "from wikify import *\n",
    "sys.stdout.flush()\n",
    "max_t=5\n",
    "ws=5\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "outfile = os.path.join(home,'backup/datasets/ner/trainrepository.30000.5000.tsv')\n",
    "\n",
    "dsname = os.path.join(home,'backup/datasets/ner/wiki-mentions.30000.json')\n",
    "\n",
    "max_count=5000\n",
    "count = 0          \n",
    "with open(dsname,'r') as ds, open(outfile,'w') as outf:\n",
    "    qid=0\n",
    "    for line in ds:                           \n",
    "        js = json.loads(line.decode('utf-8').strip());\n",
    "        S = js[\"text\"]\n",
    "        M = js[\"mentions\"]\n",
    "        count +=1        \n",
    "        print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))        \n",
    "        C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "        #print C\n",
    "        all_scores=get_all_scores(S,M,C, ws, 10)\n",
    "        for i in  range(len(C)):\n",
    "            m=M[i]\n",
    "            cands = C[i]\n",
    "            cand_scores = all_scores[i]\n",
    "            wid = title2id(m[1]) \n",
    "            for (eid,_),scores in zip (cands, cand_scores):\n",
    "                is_true_eid = (wid == eid)\n",
    "                string_scores=[str(s) for s in scores]\n",
    "                outf.write(\"\\t\".join([str(eid), str(qid)]+string_scores+[str(int(is_true_eid))])+\"\\n\")\n",
    "            qid += 1\n",
    "        if count >= max_count:\n",
    "            break\n",
    "print \"Done\"             \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_ltr.py \n",
    "\"\"\" Train a LambdaMart (LTR) Method\n",
    "\"\"\"\n",
    "import pyltr\n",
    "import pandas as pd\n",
    "import os\n",
    "from wikify import *\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "tr_file_name = os.path.join(home,'backup/datasets/ner/trainrepository.30000.5000.tsv')\n",
    "data=pd.read_table(tr_file_name, header=None)\n",
    "#data=data.head(100)\n",
    "\n",
    "grouped=data.groupby(1)\n",
    "total_len=len(grouped)\n",
    "group = grouped.filter(lambda x:x.iloc[0,1] >= 0 and x.iloc[0,1] < 0.6*total_len)\n",
    "TrX = group.iloc[:,2:7].as_matrix()\n",
    "TrY = group.iloc[:,7].as_matrix()\n",
    "Trqid = group.iloc[:,1].as_matrix()\n",
    "\n",
    "group=grouped.filter(lambda x:x.iloc[0,1] >= 0.6*total_len and x.iloc[0,1] < 0.8*total_len)\n",
    "VaX = group.iloc[:,2:7].as_matrix()\n",
    "VaY = group.iloc[:,7].as_matrix()\n",
    "Vaqid = group.iloc[:,1].as_matrix()\n",
    "\n",
    "\n",
    "group=grouped.filter(lambda x:x.iloc[0,1] >= 0.8*total_len and x.iloc[0,1] < 1.0*total_len)\n",
    "TsX = group.iloc[:,2:7].as_matrix()\n",
    "TsY = group.iloc[:,7].as_matrix()\n",
    "Tsqid = group.iloc[:,1].as_matrix()\n",
    "\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "     VaX, VaY, Vaqid, metric=pyltr.metrics.NDCG(k=10), stop_after=250)\n",
    "model = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 0)\n",
    "#lmart.fit(TX, TY, Tqid, monitor=monitor)\n",
    "model.fit(TrX, TrY, Trqid, monitor=monitor)\n",
    "\n",
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "Ts_pred = model.predict(TsX)\n",
    "print 'Random ranking:', metric.calc_mean_random(Tsqid, TsY)\n",
    "print 'Our model:', metric.calc_mean(Tsqid, TsY, Ts_pred)\n",
    "\n",
    "import cPickle as pickle\n",
    "model_file_name = os.path.join(home,'backup/datasets/ner/ltr.pkl')\n",
    "\n",
    "pickle.dump(model, open(model_file_name, 'wb'))\n",
    "\n",
    "print 'Model saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from __future__ import division\n",
    "from wikify import *\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd.wsd import *\n",
    "\n",
    "import time\n",
    "ws=5\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "# S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "# M=[[2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "# S=[\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"]\n",
    "# M=[[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]\n",
    "\n",
    "# S=[\"Phoenix, Arizona\"] \n",
    "# M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "pop_scores = get_scores(S,M,C, ws, method=\"popularity\")\n",
    "print \"Key Scores_method_1: \", formated_scores(pop_scores), \"\\n\"\n",
    "\n",
    "candslist_scores = get_scores(S,M,C, ws, method=\"keydisamb\")\n",
    "print \"Key Scores_method_2: \", formated_scores(candslist_scores), \"\\n\"\n",
    "\n",
    "candslist_scores = get_scores(S,M,C, ws, method='entitycontext')\n",
    "print \"Key Scores_method_3: \", formated_scores(candslist_scores), \"\\n\"\n",
    "\n",
    "candslist_scores = get_scores(S,M,C, ws, method=\"context2context\")\n",
    "print \"Key Scores_method_4: \", formated_scores(candslist_scores), \"\\n\"\n",
    "\n",
    "candslist_scores = get_scores(S,M,C, ws, method='context2profile')\n",
    "print \"Key Scores_method_5: \", formated_scores(candslist_scores), \"\\n\"\n",
    "\n",
    "\n",
    "# candslist_scores = get_scores(S,M,C, ws, method='learned')\n",
    "# print \"Key Scores_method_learned: \", formated_scores(candslist_scores), \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'entitymentions': [],\n",
       "  u'index': 0,\n",
       "  u'tokens': [{u'after': u' ',\n",
       "    u'before': u'',\n",
       "    u'characterOffsetBegin': 0,\n",
       "    u'characterOffsetEnd': 1,\n",
       "    u'index': 1,\n",
       "    u'lemma': u'I',\n",
       "    u'ner': u'O',\n",
       "    u'originalText': u'I',\n",
       "    u'pos': u'PRP',\n",
       "    u'word': u'I'},\n",
       "   {u'after': u' ',\n",
       "    u'before': u' ',\n",
       "    u'characterOffsetBegin': 2,\n",
       "    u'characterOffsetEnd': 6,\n",
       "    u'index': 2,\n",
       "    u'lemma': u'like',\n",
       "    u'ner': u'O',\n",
       "    u'originalText': u'like',\n",
       "    u'pos': u'VBP',\n",
       "    u'word': u'like'},\n",
       "   {u'after': u' ',\n",
       "    u'before': u' ',\n",
       "    u'characterOffsetBegin': 7,\n",
       "    u'characterOffsetEnd': 13,\n",
       "    u'index': 3,\n",
       "    u'lemma': u'Python',\n",
       "    u'ner': u'O',\n",
       "    u'originalText': u'Python',\n",
       "    u'pos': u'NNP',\n",
       "    u'word': u'Python'},\n",
       "   {u'after': u' ',\n",
       "    u'before': u' ',\n",
       "    u'characterOffsetBegin': 14,\n",
       "    u'characterOffsetEnd': 25,\n",
       "    u'index': 4,\n",
       "    u'lemma': u'programming',\n",
       "    u'ner': u'O',\n",
       "    u'originalText': u'programming',\n",
       "    u'pos': u'NN',\n",
       "    u'word': u'programming'},\n",
       "   {u'after': u'',\n",
       "    u'before': u' ',\n",
       "    u'characterOffsetBegin': 26,\n",
       "    u'characterOffsetEnd': 34,\n",
       "    u'index': 5,\n",
       "    u'lemma': u'language',\n",
       "    u'ner': u'O',\n",
       "    u'originalText': u'language',\n",
       "    u'pos': u'NN',\n",
       "    u'word': u'language'}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Mention Detection Methods\"\"\"\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "core_nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "mentions = core_nlp.annotate(\"I like Python programming language\", properties={\n",
    "    'annotators': 'entitymentions',\n",
    "    'outputFormat': 'json'})\n",
    "mentions['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
