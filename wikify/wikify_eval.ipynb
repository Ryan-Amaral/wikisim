{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "** Local and Global Algorithms ... **\n",
    "\n",
    "AQUAINT: Milne\n",
    "\n",
    "MSNBC dataset, taken from (Cucerzan, 2007),\n",
    "\n",
    "ACE: Mechanical Turkn\n",
    "\n",
    "Wiki: choose those paragraphts that p(t|m) makes atleast 10% error\n",
    "\n",
    "For evaluation, check BOT evaluation, mentioned in Milne \n",
    "\n",
    "Downloadable from :\n",
    "http://cogcomp.cs.illinois.edu/page/resource_view/4\n",
    "\n",
    "\n",
    "**Spotlight**\n",
    "two datasets, a wiki selection\n",
    "35 paragraphs from New York times\n",
    "There is a website, but couldn't find it\n",
    "\n",
    "http://oldwiki.dbpedia.org/Datasets/NLP\n",
    "\n",
    "Tag me:\n",
    "Wiki and tweet, \n",
    "available, but looks old!\n",
    "http://acube.di.unipi.it/tagme-dataset/\n",
    "\n",
    "**AIDA**\n",
    "\n",
    "https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/\n",
    "AIDA CoNLL-YAGO Dataset: Hnad create from Conll\n",
    "AIDA-EE Dataset: Again hand done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove after transferring to wikification module\n",
    "    if  op_method == 'word2vec_word_context'  :\n",
    "        return word_context_disambiguate(S, M, C, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wikify import *\n",
    "import time\n",
    "ws=5\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"David\", \"met\", \"Victoria\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "S=[\"Phoenix, Arizona\"] \n",
    "M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "ids, titles = wikify(S,M,C, ws, method='context2context')\n",
    "\n",
    "\n",
    "#print \"Key Scores_method_1: \", candslist_scores, \"\\n\"\n",
    "print \"Best IDS\", ids, \"\\n\"\n",
    "print \"Best titles\", titles, \"\\n\"\n",
    "#print \"get_tp\",get_tp(M, ids) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy WSD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd_eval.py \n",
    "'''The Legacy WSD evaluation\n",
    "This is how we evaluated for the thesis. \n",
    "The main difference is \"enforce=True\" in the candidate generation, \n",
    "an option which always makes sure the correct entity is among the candidates\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "import time\n",
    "from random import shuffle\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "# max_t = 15\n",
    "# max_count = -1\n",
    "# verbose = True\n",
    "\n",
    "fresh_restart=True\n",
    "\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aida.json'),  \n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "methods = ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile', 'learned']\n",
    "methods = ['popularity','mention2entity','context2context','context2profile', 'learned']\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, op_method: %s, direction: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                \"rvspagerank\", method, \"both\", max_t, 5)\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join(['rvspagerank', str(DIR_BOTH), method, str(max_t), '5', os.path.basename(dsname)]))\n",
    "        \n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=True)\n",
    "                \n",
    "                try:\n",
    "                    #ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n",
    "                    ids, titles = wsd(S,M,C, method=method)\n",
    "                    tp = get_tp(ids, M) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        \n",
    "        detailedres ={\"dsname\":dsname, \"method\": \"rvspagerank\", \"op_method\": method, \"driection\": 'both',\n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": 5}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', 'rvspagerank', method, 'both', max_t , 5, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "        \n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wikify_link.py \n",
    "''' The new WSD evaluation method, here some of the options we already settled on\n",
    "    is fixed and hidden, such as similarity method or ws\n",
    "'''\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "import time\n",
    "from random import shuffle\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "# max_t = 15\n",
    "# max_count = -1\n",
    "# verbose = True\n",
    "\n",
    "fresh_restart=True\n",
    "\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "#          os.path.join(home,'backup/datasets/ner/aida.json'),  \n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "methods = ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile', 'learned']\n",
    "#methods = ['popularity','mention2entity','context2context','context2profile', 'learned']\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, max_t: %s ...\"  % (dsname,\n",
    "                method, max_t)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(max_t), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "                \n",
    "                try:\n",
    "                    #ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n",
    "                    ids, titles = wsd(S,M,C, method=method)\n",
    "                    tp = get_tp(ids, M) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"problem\": \"wsd\"}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, 'wikify_link\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, max_t, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile wikify_eval.py\n",
    "\"\"\"Evaluating the wsd module. It assumes the sentences are already segmented\n",
    "\"\"\"\n",
    "from wikify import *\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "import time\n",
    "from random import shuffle\n",
    "np.seterr(all='raise')\n",
    "\n",
    "# parser = OptionParser()\n",
    "# parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "# parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "# parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "# parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "#(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# max_t = options.max_t\n",
    "# max_count = options.max_count\n",
    "# verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "max_t = 20\n",
    "max_count = -1\n",
    "verbose = True\n",
    "\n",
    "fresh_restart=True\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "#          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "#          os.path.join(home,'backup/datasets/ner/aida.json'),  \n",
    "#          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "#          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "mentionmethods = (CORE_NLP, LEARNED_MENTION)\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for mentionmethod in mentionmethods:\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, mentionmethods: %s, max_t: %s ...\"  % (dsname,\n",
    "                mentionmethods, max_t)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([str(mentionmethod), str(max_t), os.path.basename(dsname)]))\n",
    "        mention_overall=[]\n",
    "        wikify_overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    \n",
    "                    if js['mention'] is not None:\n",
    "                        mention_overall.append(js['mention'])\n",
    "                        \n",
    "                    if js['wikify_overall'] is not None:\n",
    "                        wikify_overall.append(js['wikify_overall'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                text= \" \".join(S)    \n",
    "                S2,M2 = wikify_string(text, mentionmethod=mentionmethod)\n",
    "                mention_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=False)\n",
    "                mention_overall.append(mention_measures)\n",
    "                \n",
    "                wikify_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=True)\n",
    "                wikify_overall.append(wikify_measures)\n",
    "                \n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"mentionmethod\": mentionmethod, \n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        mention_overall_measures = get_overall_measures(mention_overall)    \n",
    "        output = ('mention_evaluation',mentionmethod, max_t , dsname) + mention_overall_measures + (elapsed,)\n",
    "        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n', output)\n",
    "        \n",
    "        wikify_overall_measures = get_overall_measures(wikify_overall)  \n",
    "        output = ('wikify_evaluation',mentionmethod, max_t , dsname) + wikify_overall_measures + (elapsed,)\n",
    "            \n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc_preprocessor loaded\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mention_overall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-befed8ade4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikify_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentionmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCORE_NLP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmention_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsd_measure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmention_overall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmention_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mention_overall' is not defined"
     ]
    }
   ],
   "source": [
    "from wikify import *\n",
    "\n",
    "S=[\"Mars\", \",\", \"Galaxy\", \",\", \"and\", \"Bounty\", \"are\", \"all\", \"chocolate\", \".\"]\n",
    "M=[[0, \"Mars_bar\"], [2, \"Galaxy_(chocolate)\"], [5, \"Bounty_(chocolate_bar)\"]]\n",
    "text = \" \".join(S)\n",
    "S2,M2 = wikify_string(text, mentionmethod=CORE_NLP)\n",
    "mention_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=False)\n",
    "mention_overall.append(mention_measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
