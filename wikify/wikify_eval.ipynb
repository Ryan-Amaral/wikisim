{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove after transferring to wikification module\n",
    "    if  op_method == 'word2vec_word_context'  :\n",
    "        return word_context_disambiguate(S, M, C, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wikify import *\n",
    "import time\n",
    "ws=5\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"David\", \"met\", \"Victoria\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "S=[\"Phoenix, Arizona\"] \n",
    "M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "ids, titles = wikify(S,M,C, ws, method='context2context')\n",
    "\n",
    "\n",
    "#print \"Key Scores_method_1: \", candslist_scores, \"\\n\"\n",
    "print \"Best IDS\", ids, \"\\n\"\n",
    "print \"Best titles\", titles, \"\\n\"\n",
    "#print \"get_tp\",get_tp(M, ids) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikify_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikify_eval.py \n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wikify import *\n",
    "import time\n",
    "np.seterr(all='raise')\n",
    "\n",
    "# parser = OptionParser()\n",
    "# parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "# parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "# parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "# parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "#(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# max_t = options.max_t\n",
    "# max_count = options.max_count\n",
    "# verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "max_t = 20\n",
    "max_count = -1\n",
    "verbose = True\n",
    "ws = 5\n",
    "\n",
    "\n",
    "fresh_restart=True\n",
    "\n",
    "#word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "#word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5')\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "           os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "           os.path.join(home,'backup/datasets/ner/aida.json'), \n",
    "           os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "           os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/wiki-mentions.30000.5000.json')]\n",
    "\n",
    "          \n",
    "# dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "#            os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "#           ]\n",
    "\n",
    "\n",
    "methods = ('popularity', 'keydisamb', 'entitycontext', 'context2context', 'context2profile','learned')\n",
    "\n",
    "\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                method, max_t, ws)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(max_t), str(ws), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "                \n",
    "                try:\n",
    "                    #ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n",
    "                    ids, titles = wikify(S,M,C, ws, method=method)\n",
    "                    tp = get_tp(M, ids) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, max_t , ws, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
