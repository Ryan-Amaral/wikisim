{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing wsd datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import MySQLdb\n",
    "import json\n",
    "# _db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "# _cursor = _db.cursor()\n",
    "# _cursor.execute(\"\"\"SELECT * FROM `anchors` group\"\"\", (wid,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "import sys\n",
    "import requests\n",
    "import MySQLdb\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "\n",
    "def qualify(r, min_mention=2, min_abmig=2):\n",
    "    \n",
    "    # condition 1\n",
    "    if len(r['opening_annotation']) < min_mention:\n",
    "        #print \"c1 failed\"\n",
    "        return None\n",
    "    \n",
    "    nottop = False\n",
    "    found_min_ambig=False\n",
    "    annotations = json.loads(r['opening_annotation'])\n",
    "    for ann in annotations:\n",
    "        surf = ann['surface_form']\n",
    "        linktitle = ann['url']\n",
    "        wid = title2id(linktitle.encode('utf-8').replace(' ','_'))\n",
    "        # condition 2\n",
    "        if wid is None:\n",
    "            #print \"c2 failed\", linktitle\n",
    "            return None\n",
    "        \n",
    "        allids = anchor2concept(surf)\n",
    "        # condition 3\n",
    "        if not allids:\n",
    "            #print \"c3 failed\"\n",
    "            return None\n",
    "        # condition 4\n",
    "        if len(allids) >= min_abmig:\n",
    "            found_min_ambig = True\n",
    "        allids = sorted(allids, key=lambda k:-k[1])\n",
    "        \n",
    "        # condition 5\n",
    "        #print allids\n",
    "        if allids[0][0] != wid:\n",
    "            nottop = True\n",
    "    if not found_min_ambig:\n",
    "        #print \"c4 failed\"\n",
    "        return None\n",
    "    return nottop                                                                                   \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test qualify\n",
    "#Conference\n",
    "#Oladapo\n",
    "#r={'opening_annotation':[{'url':'Bruce_Lee', 'surface_form': 'Bruce Lee'}, {'url':'Sanandaj', 'surface_form': 'Sanandaj'}]}\n",
    "#r={'opening_annotation':[{'url':'Oladapo', 'surface_form': 'Oladapo'}, {'url':'Do_while_loop', 'surface_form': 'do while loop'}]}\n",
    "#r={'opening_annotation':[{'url':'David_Beckham', 'surface_form': 'David'}, {'url':'Do_while_loop', 'surface_form': 'do while loop'}]}\n",
    "#print qualify(r)\n",
    "#print len(anchor2concept('Bruce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  314 , count_nottop:  100\n",
      "count:  500\n",
      "count:  689 , count_nottop:  200\n",
      "count:  1000\n",
      "count:  1201 , count_nottop:  300\n",
      "count:  1500\n",
      "count:  1584 , count_nottop:  400\n",
      "count:  1957 , count_nottop:  500\n",
      "count:  2000\n",
      "count:  2253 , count_nottop:  600\n",
      "count:  2500\n",
      "count:  2511 , count_nottop:  700\n",
      "count:  2785 , count_nottop:  800\n",
      "count:  3000\n",
      "count:  3034 , count_nottop:  900\n",
      "count:  3262 , count_nottop:  1000\n",
      "count:  3500\n",
      "count:  4000\n",
      "count:  4500\n",
      "count:  5000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "wanted=5000\n",
    "wanted_nottop=1000\n",
    "\n",
    "reopen()\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "q='*:*'\n",
    "params={'indent':'on', 'wt':'json', 'fl':'title opening_text opening_annotation', 'q':q, \"start\":0,\n",
    "        \"rows\":100}\n",
    "home = os.path.expanduser(\"~\");\n",
    "output = open(os.path.join(home,'backup/datasets/ner/wiki.5000.json'), 'w')\n",
    "\n",
    "start=0\n",
    "count=0\n",
    "count_nottop=0\n",
    "enough=False\n",
    "while True:\n",
    "    params[\"start\"] = start\n",
    "    start += 1\n",
    "    r = requests.get(qstr, params=params)\n",
    "    #print r.json()['response']['docs']\n",
    "    for d in r.json()['response']['docs']:\n",
    "        #print d[\"id\"]; enough=True; break\n",
    "        if not (count < wanted or count_nottop < wanted_nottop):\n",
    "            enough = True\n",
    "            break\n",
    "        qd = qualify(d) \n",
    "        if qd is None:\n",
    "            continue\n",
    "            \n",
    "        if count_nottop < wanted_nottop and qd :\n",
    "            output.write(json.dumps(d, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            count_nottop += 1\n",
    "            count += 1\n",
    "            if ((count) % int(wanted/10) ==0) or (count_nottop) % int(wanted_nottop/10) ==0:\n",
    "                print \"count: \", count, \", count_nottop: \", count_nottop\n",
    "            continue\n",
    "        if count < wanted:\n",
    "            count +=1\n",
    "            output.write(json.dumps(d, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            if (count) % int(wanted/10) ==0:   \n",
    "                print \"count: \", count\n",
    "            \n",
    "    #break\n",
    "    if enough:\n",
    "        break\n",
    "print 'done'        \n",
    "output.close()    \n",
    "#r=json.loads(r['opening_annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting json to input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json\n",
    "import string\n",
    "home = os.path.expanduser(\"~\");\n",
    "jsname = os.path.join(home,'backup/datasets/ner/wiki.5000.json')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json')\n",
    "\n",
    "\n",
    "    \n",
    "def splittext(text,url):\n",
    "    start=0\n",
    "    termindex=0\n",
    "    t=[]\n",
    "    mentions=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for u in url:\n",
    "        seg = text[start:u['from']]\n",
    "        t += seg.strip().split()\n",
    "        mentions.append([len(t),u['url']])\n",
    "        t+=[\" \".join(text[u['from']:u['to']].split())]\n",
    "        start = u['to']\n",
    "        \n",
    "    t += text[start:].strip().split()\n",
    "    return t, mentions\n",
    "\n",
    "with open(jsname) as jf, open(outjsname,'w') as outjs:\n",
    "    for line in jf:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        js = json.loads(line)\n",
    "        text = js['opening_text'] \n",
    "        url = json.loads(js['opening_annotation'])\n",
    "        t, m = splittext(text, url)\n",
    "        outjs.write((json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False)+'\\n').encode('utf-8'))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting KORE to input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import sys, os, json\n",
    "import string\n",
    "from HTMLParser import HTMLParser\n",
    "hp = HTMLParser()\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "reopen()\n",
    "\n",
    "\n",
    "infname = os.path.join(home,'backup/datasets/ner/KORE50/AIDA.tsv')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/kore.json')\n",
    "url_col = 3\n",
    "\n",
    "t=[]\n",
    "m=[]\n",
    "with open(infname) as inf, open(outjsname,'w') as outjs:\n",
    "    state = 0\n",
    "    for line in inf:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            line = line.decode('unicode-escape')\n",
    "        except:\n",
    "            pass            \n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            if m and t:\n",
    "                outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            t = []\n",
    "            m = []\n",
    "            continue\n",
    "                \n",
    "        w = line.split('\\t')\n",
    "        if len(w) == 1:\n",
    "            t.append(w[0])\n",
    "            continue\n",
    "            \n",
    "        if w[1] == 'B':\n",
    "            if title2id(w[3]) is None:\n",
    "                print w[3], \" not found\"\n",
    "            if w[url_col] != '--NME--' and (title2id(w[3]) is not None):\n",
    "                m.append((len(t), w[3]))\n",
    "            t.append(w[2])\n",
    "            \n",
    "        if w[1] == 'I':\n",
    "            continue\n",
    "    if m and t:\n",
    "        outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=\"Rudi_V\\u00f6ller\"\n",
    "print x.decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Aida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import sys, os, json\n",
    "import string\n",
    "import re\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "hp = HTMLParser()\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "reopen()\n",
    "\n",
    "pattern = re.compile(\"http://en.wikipedia.org/wiki/(.*)\")\n",
    "\n",
    "\n",
    "infname = os.path.join(home,'backup/datasets/ner/Aida01/aida-yago2-dataset/AIDA-YAGO2-annotations.tsv')\n",
    "#infname = os.path.join(home,'backup/datasets/ner/aida-shalam.tsv')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/aida.json')\n",
    "#outjsname = os.path.join(home,'backup/datasets/ner/aida-shalam.json')\n",
    "# set it to 3 for AIDA, \n",
    "url_col = 4\n",
    "pattern = re.compile(\"http://en.wikipedia.org/wiki/(.*)\")\n",
    "\n",
    "t=[]\n",
    "m=[]\n",
    "with open(infname) as inf, open(outjsname,'w') as outjs:\n",
    "    state = 0\n",
    "    for line in inf:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        #line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            if m and t:\n",
    "                outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "                #print (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            t = []\n",
    "            m = []\n",
    "            continue\n",
    "                \n",
    "        w = line.split('\\t')\n",
    "            \n",
    "        if w[1] == '--NME--':\n",
    "            continue\n",
    "        title = w[2]\n",
    "        r = re.match(pattern, title)\n",
    "        title = r.group(1)\n",
    "        rwd = title2id(title)\n",
    "            \n",
    "        rwd = title2id(title)\n",
    "            \n",
    "        anchors = id2anchor(rwd)\n",
    "        if not anchors:\n",
    "            print \"no anchor for: \", title\n",
    "            continue\n",
    "                            \n",
    "        c=max(anchors, key=lambda k:k[1])[0]    \n",
    "        m.append((len(t), title))\n",
    "        t.append(c.decode('utf-8'))\n",
    "            \n",
    "    if m and t:\n",
    "        outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        #print (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "                \n",
    "print \"done\"        \n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /home/sajadi/backup/archived/datasets/ner/shalam.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2title(12633520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2anchor(12633520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
