{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile toannonated.py\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "import sys\n",
    "from HTMLParser import HTMLParser\n",
    "sys.path.append('../../cgi-bin/')\n",
    "from wikipedia import *\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "# import StringIO\n",
    "# fileinput = StringIO.StringIO(inputstr)\n",
    "def replacelinks(text):\n",
    "    hp = HTMLParser()\n",
    "    \n",
    "    annotations = []\n",
    "    deltaStringLength = 0\n",
    "    hrefreg=r'<a href=\"([^\"]+)\">([^>]+)</a>'\n",
    "    ms = re.finditer(hrefreg, text)\n",
    "    \n",
    "    \n",
    "    for m in ms:     \n",
    "\n",
    "        url = m.group(1)\n",
    "        # in the parser, url->encode->quote->encode (does nothing, already unicode)->write to file\n",
    "        # we already have decoded while reading, we have to encode back, unquote, and decode, so that\n",
    "        # the last encode on writing can make it back to unicode\n",
    "        url=url.encode('utf-8')\n",
    "        url =  urllib.unquote(url)\n",
    "        url = url.decode('utf-8')\n",
    "        #sometimes we have [[&]] that has been encoded to &amp; \n",
    "        #sometimes we have [[&amp]] that has been encoded to &amp;amp; \n",
    "        #so we unexcape twice!\n",
    "        \n",
    "        url=hp.unescape(url)\n",
    "        url=hp.unescape(url)\n",
    "        url=url.replace(u\"\\xA0\",\" \")\n",
    "        \n",
    "        x = url.find(\"#\")\n",
    "        if x!=-1:\n",
    "            url=url[:x]\n",
    "        antext = m.group(2)\n",
    "        if '//' not in url:\n",
    "            annotations.append({\n",
    "                \"url\"    :   url, \n",
    "                \"surface_form\" :   antext, \n",
    "                \"from\"  :   m.start() - deltaStringLength,\n",
    "                \"to\"    :   m.start() - deltaStringLength+len(antext)\n",
    "            })\n",
    "\n",
    "        deltaStringLength += len(m.group(0)) - len(antext)\n",
    "\n",
    "    #As a second step, replace all links in the article by their label\n",
    "    text = re.sub(hrefreg, lambda m:m.group(2), text)  \n",
    "    return annotations, text\n",
    "def process():\n",
    "    hp = HTMLParser()\n",
    "    rstart=r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">'\n",
    "    rend=r'</doc>'\n",
    "\n",
    "#     open_ann= open ('open_annotation.json', 'w')\n",
    "#     ann= open ('annotation.json', 'w')\n",
    "#     wikitext=open('wikitext.json','w')\n",
    "    state=0\n",
    "    page=\"\"\n",
    "    textlist=[]\n",
    "    while True:\n",
    "        line = fileinput.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.decode('utf-8').strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        if re.match(rend,line):\n",
    "            # If you want to check the title, make sure to encode!\n",
    "            if textlist and (id2title(wid) is not None):\n",
    "                opening_ann, opening_text = replacelinks(opening_text)\n",
    "                ann,text = replacelinks(\"\\n\".join(textlist))\n",
    "                \n",
    "#                 ElasticSearch   \n",
    "#                 Buggy: convert annotations to text or do something about it\n",
    "#                 print json.dumps({\"index\":{\"_type\":\"page\",\"_id\":wid}}, ensure_ascii=False).encode('utf-8')\n",
    "#                 page={\"title\": wtitle, \"opening_text\":opening_text, \"opening_annotation\":opening_ann,\n",
    "#                       \"text\":text, \"annotation\": ann}\n",
    "                \n",
    "#                 print json.dumps(page, ensure_ascii=False).encode('utf-8')\n",
    "                \n",
    "#               General\n",
    "                page={\"id\":wid, \"title\": wtitle, \"opening_text\":opening_text, \"opening_annotation\":opening_ann,\n",
    "                      \"text\":text, \"annotation\": ann}\n",
    "                \n",
    "                print json.dumps(page, ensure_ascii=False).encode('utf-8')\n",
    "                \n",
    "            textlist=[]    \n",
    "            state=0\n",
    "            continue\n",
    "        if state==0:\n",
    "            if re.match('<doc', line):\n",
    "                ms = re.match(rstart, line)\n",
    "                wid=ms.group(1)\n",
    "                wtitle=hp.unescape(ms.group(3)).replace(u\"\\xA0\",\" \")\n",
    "\n",
    "                state = 1\n",
    "            continue    \n",
    "\n",
    "        \n",
    "        if state==1:\n",
    "            state=2\n",
    "            continue\n",
    "        textlist.append(line)\n",
    "            \n",
    "        if state==2:\n",
    "            opening_text=line\n",
    "            state=3\n",
    "            continue\n",
    "            \n",
    "#     wikitext.close()\n",
    "#     ann.close()\n",
    "if __name__ == \"__main__\": process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting extractanchors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile extractanchors.py\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "import StringIO\n",
    "#fileinput = StringIO.StringIO(inputstr)\n",
    "while True:\n",
    "    line = fileinput.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    line = line.decode('utf-8').strip()\n",
    "    js = json.loads(line)    \n",
    "    if \"annotation\" not in js:\n",
    "        continue\n",
    "    for a in js[\"annotation\"]:\n",
    "        print u\"{0}\\t{1}\".format(a[\"surface_form\"],a[\"url\"].replace(\" \",\"_\")).encode('utf-8')\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract\n",
    "\n",
    "`nohup ./WikiExtractor.py -o ~/backup/wikipedia/20160305/texts/xmltexts -b 1000M  -l  --no-templates ~/backup/wikipedia/20160305/original/enwiki-20160305-pages-articles.xml.bz2    >wikiext.log 2>&1 &`\n",
    "\n",
    "### get annonated\n",
    "`time cat ~/backup/wikipedia/20160305/texts/xmltexts/wiki_* | python toannonated.py > ~/backup/wikipedia/20160305/texts/enwiki-20160305-annonated.json`\n",
    "\n",
    "### get anchors\n",
    "\n",
    "`time cat ~/backup/wikipedia/20160305/texts/enwiki-20160305-annonated.json | python extractanchors.py > ~/backup/wikipedia/20160305/texts/enwiki-20160305-anchor-titles-from-text`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Augmenting anchors with titles\n",
    "Examples of titles with no anchors:\n",
    "Meiluawati\n",
    "ZdenÄ›k_Svoboda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting titles2anchors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile titles2anchors.py\n",
    "\n",
    "import MySQLdb\n",
    "_db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "_cursor = _db.cursor()\n",
    "_cursor.execute(\"select page_title from page where page_namespace=0\");\n",
    "rows = _cursor.fetchall();\n",
    "for row in rows:\n",
    "    title=row[0]\n",
    "    surface_form = title;\n",
    "    i=surface_form.find('(')\n",
    "    if i!=-1:\n",
    "        surface_form = surface_form[:i]\n",
    "    surface_form = surface_form.replace('_', ' ')\n",
    "    surface_form = surface_form.strip()\n",
    "    \n",
    "    print surface_form+\"\\t\"+title\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles to Anchors\n",
    "`time python titles2anchors.py > ~/backup/wikipedia/20160305/texts/enwiki-20160305-anchor-from-titles.tsv`\n",
    "\n",
    "`cp enwiki-20160305-anchor-titles-from-text.tsv  enwiki-20160305-anchor-titles.tmp.tsv`\n",
    "`cat enwiki-20160305-anchor-from-titles.tsv >> enwiki-20160305-anchor-titles.tmp.tsv`\n",
    "\n",
    "\n",
    "### modify anchors\n",
    "`time cat enwiki-20160305-anchor-titles.tmp.tsv | sort| uniq -c | sed 's/^ *//;s/ /\\t/' > enwiki-20160305-anchor-titles.tsv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving redirects    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile resolveredirs.py\n",
    "import sys\n",
    "sys.path.append('../../cgi-bin/')\n",
    "from collections import defaultdict\n",
    "import imp\n",
    "import json\n",
    "import urllib\n",
    "from wikipedia import *\n",
    "fileinput = sys.stdin\n",
    "errout = sys.stderr\n",
    "import StringIO\n",
    "#fileinput = StringIO.StringIO(inputstr)\n",
    "pairdict = defaultdict(int)\n",
    "while True:\n",
    "    line = fileinput.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    line = line.strip()\n",
    "    tp = line.split(\"\\t\")\n",
    "    if len(tp) !=3:\n",
    "        errout.write(\"Line Error: \" + line)\n",
    "        continue\n",
    "    freq,anchor,title=tp\n",
    "    wid=title2id(title)\n",
    "    if wid is None:\n",
    "        title=title[0].upper()+title[1:]\n",
    "    wid=title2id(title)\n",
    "    if wid is None:\n",
    "        errout.write(anchor+\"\\t\"+title+\"\\t\"+str(freq)+\"\\n\")\n",
    "        continue\n",
    "    pairdict[(anchor,wid)] += int(freq)\n",
    "for (anchor,wid),freq in pairdict.iteritems():\n",
    "    print anchor+\"\\t\"+str(wid)+\"\\t\"+str(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying resolves\n",
    "\n",
    "`time cat ~/backup/wikipedia/20160305/texts/enwiki-20160305-anchor-titles.tsv | python resolveredirs.py > ~/backup/wikipedia/20160305/texts/enwiki-20160305-anchor-ids.tsv 2> ~/backup/wikipedia/20160305/texts/enwiki-20160305-anchor-ids.dead.tsv`\n",
    "\n",
    "### importing to mysql\n",
    "`bash importall ~/backup/wikipedia/20160305/texts 20160305 root emilios`\n",
    "\n",
    "### extracting anchors lists\n",
    "cat enwiki-20160305-anchor-ids.tsv| cut -f1 -d$'\\t'| sort | uniq > enwiki-20160305-anchors.tsv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToSolr v2. converts the annotations to a jsonstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile tosolr.py\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "import StringIO\n",
    "#fileinput = StringIO.StringIO(inputstr)\n",
    "\n",
    "n=1000\n",
    "count=0;\n",
    "f=None\n",
    "os.mkdir(\"chunks\")\n",
    "seperator=''\n",
    "firstline=None\n",
    "while True:\n",
    "    line = fileinput.readline()\n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "    line = line.decode('utf-8').strip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    if count%n==0:\n",
    "        if f is not None:\n",
    "            f.write('\\n]\\n')\n",
    "            f.close()\n",
    "        f=open('chunks/chunk'+str(int(count/n)).zfill(10)+'.json','w')    \n",
    "        f.write('[\\n')\n",
    "        firstline=True\n",
    "        seperator=\"\"\n",
    "    count +=1\n",
    "    \n",
    "    page=json.loads(line)\n",
    "    page[\"annotation\"]=json.dumps(page[\"annotation\"], ensure_ascii=False)\n",
    "    page[\"opening_annotation\"]=json.dumps(page[\"opening_annotation\"], ensure_ascii=False)\n",
    "    f.write(seperator+json.dumps(page, ensure_ascii=False).encode('utf-8'))\n",
    "    if firstline:\n",
    "        firstline=False\n",
    "        seperator=\",\\n\"\n",
    "    \n",
    "    \n",
    "if f is not None:\n",
    "    f.write('\\n]\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparing solr importable\n",
    "`time python tosolr.py <~/backup/wikipedia/20160305/texts/enwiki-20160305-annonated.json`\n",
    "\n",
    "## creating solr nodes\n",
    "\n",
    "`bin/solr create -c enwiki20160305`\n",
    "\n",
    "`bin/solr stop`\n",
    "\n",
    "`cp ~/backup/projects/wikifier/preparation_script/solrsettings/conf/* .`\n",
    "\n",
    "`bin/solr start`\n",
    "\n",
    "## importing to solr\n",
    "`nohup time bash loadwiki >load.txt 2>&1 &`\n",
    "\n",
    "**count**\n",
    "curl 'http://localhost:8983/solr/enwiki20160305/select?indent=on&q=*:*&rows=0&wt=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tager preparation\n",
    "\n",
    "`bin/solr create -c enwikianchors20160305`\n",
    "\n",
    "`bin/solr stop`\n",
    "\n",
    "`cp -r ~/backup/projects/wikisim/preparation_script/text/solrtagger/lib/ server/solr/`\n",
    "\n",
    "\n",
    "`cp ~/backup/projects/wikifier/preparation_script/solrtagger/conf/* server/solr/enwikianchors20160305/conf/`\n",
    "\n",
    "\n",
    "`curl -X POST --data-binary @enwiki-20160305-anchors.tsv.txt -H 'Content-type:application/csv' \\\n",
    "  'http://localhost:8983/solr/enwikianchors20160305/update?commit=true&optimize=true&separator=%09&encapsulator=%00&fieldnames=name'`\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Access to paragraphs with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "q='title:Bruce_Lee'\n",
    "params={'indent':'on', 'wt':'json', 'q':q}\n",
    "r = requests.get(qstr, params=params)\n",
    "r =  r.json()['response']['docs'][0]\n",
    "\n",
    "pos=[m.start() for m in re.finditer('\\n', r['text'])]    \n",
    "n = index = random.randint(0,len(pos)-1)\n",
    "s = pos[n]\n",
    "e = pos[n+1] if n < len(pos)-1 else len(r['text'])\n",
    "par = r['text'][s:e]\n",
    "print r['title']\n",
    "print \"par no:\",n, par[:10],\"...\", par[len(par)-10:]\n",
    "for ann in json.loads(r['annotation']):\n",
    "    #print ann\n",
    "    if int(ann['from']) < s: \n",
    "        continue\n",
    "    if int(ann['to']) >= e: \n",
    "        break\n",
    "    surfintext = r['text'][ann['from']:ann['to']]\n",
    "    print \"detected: %s, in annotation: %s, title: %s\" % (surfintext, ann[u'surface_form'], ann['url'])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
