{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "** Local and Global Algorithms ... **\n",
    "\n",
    "AQUAINT: Milne\n",
    "\n",
    "MSNBC dataset, taken from (Cucerzan, 2007),\n",
    "\n",
    "ACE: Mechanical Turkn\n",
    "\n",
    "Wiki: choose those paragraphts that p(t|m) makes atleast 10% error\n",
    "\n",
    "For evaluation, check BOT evaluation, mentioned in Milne \n",
    "\n",
    "Downloadable from :\n",
    "http://cogcomp.cs.illinois.edu/page/resource_view/4\n",
    "\n",
    "\n",
    "**Spotlight**\n",
    "two datasets, a wiki selection\n",
    "35 paragraphs from New York times\n",
    "There is a website, but couldn't find it\n",
    "\n",
    "http://oldwiki.dbpedia.org/Datasets/NLP\n",
    "\n",
    "Tag me:\n",
    "Wiki and tweet, \n",
    "available, but looks old!\n",
    "http://acube.di.unipi.it/tagme-dataset/\n",
    "\n",
    "**AIDA**\n",
    "\n",
    ": https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/**\n",
    "AIDA CoNLL-YAGO Dataset: Hnad create from Conll\n",
    "AIDA-EE Dataset: Again hand done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile wsd.py \n",
    "\"\"\" Evaluating the method on Semantic Relatedness Datasets.\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time;\n",
    "import json \n",
    "import requests\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from wikisim.config import *\n",
    "\n",
    "from wikisim.calcsim import *\n",
    "#reopen()\n",
    "\n",
    "\n",
    "def generate_candidates(S, M, max_t=10, enforce=True):\n",
    "    \"\"\" Given a sentence list (S) and  a mentions list (M), returns a list of candiates\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            max_t: maximum candiate per mention\n",
    "            enforce: Makes sure the \"correct\" entity is among the candidates\n",
    "        Outputs:\n",
    "         Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n",
    "    \n",
    "    \"\"\"\n",
    "    candslist=[]\n",
    "    for m in M:\n",
    "        wid = title2id(m[1])\n",
    "        if wid is None:\n",
    "            raise Exception(m[1].encode('utf-8') + ' not found')\n",
    "        \n",
    "        clist = anchor2concept(S[m[0]])\n",
    "        clist = sorted(clist, key=lambda x: -x[1])\n",
    "\n",
    "        smooth=0    \n",
    "        trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n",
    "        if enforce and (not trg):\n",
    "            trg=[(len(clist), (wid,0))]\n",
    "            smooth=1\n",
    "\n",
    "            \n",
    "        clist = clist[:max_t]\n",
    "        if enforce and (smooth==1 or trg[0][0]>=max_t): \n",
    "            if clist:\n",
    "                clist.pop()\n",
    "            clist.append(trg[0][1])\n",
    "        s = sum(c[1]+smooth for c in clist )        \n",
    "        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n",
    "            \n",
    "        candslist.append(clist)\n",
    "    return  candslist \n",
    "\n",
    "def disambiguate(C, method, direction, op_method):\n",
    "    \"\"\" Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             key: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if op_method == 'ilp':\n",
    "        return disambiguate_ilp(C, method, direction)\n",
    "    if op_method == 'ilp2':\n",
    "        return disambiguate_ilp_2(C, method, direction)\n",
    "    if op_method == 'keyq':\n",
    "        return key_quad(C, method, direction)\n",
    "    if op_method == 'pkeyq':\n",
    "        return Pkey_quad(C, method, direction)\n",
    "    if  op_method == 'context1'  :\n",
    "        return contextdisamb_1(C, direction)\n",
    "    if  op_method == 'context2'  :\n",
    "        return contextdisamb_2(C, direction)\n",
    "    if  op_method == 'context3'  :\n",
    "        return contextdisamb_3(C, direction)\n",
    "    \n",
    "    if  op_method == 'context4_1'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 1)\n",
    "    if  op_method == 'context4_2'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 2)\n",
    "    if  op_method == 'context4_3'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 3)    \n",
    "    if  op_method == 'keydisamb'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 4)\n",
    "    \n",
    "    if  op_method == 'tagme'  :\n",
    "        return tagme(C, method, direction)\n",
    "    if  op_method == 'tagme2'  :\n",
    "        return tagme(C, method, direction, True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def disambiguate_driver(C, ws, method, direction, op_method):\n",
    "    \"\"\" Initiate the disambiguation by chunking the sentence \n",
    "        Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: Windows size for chunking\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             keyq: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    titles = []\n",
    "    \n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "        \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        chunk_ids, chunk_titles = disambiguate(chunk_c, method, direction, op_method)\n",
    "        ids += chunk_ids\n",
    "        titles += chunk_titles\n",
    "    return ids, titles     \n",
    "\n",
    "def get_tp(gold_titles, ids):\n",
    "    tp=0\n",
    "    for m,id2 in zip(gold_titles, ids):\n",
    "        if title2id(m[1]) == id2:\n",
    "            tp += 1\n",
    "    return [tp, len(ids)]\n",
    "\n",
    "def get_prec(tp_list):\n",
    "    overall_tp = 0\n",
    "    simple_count=0\n",
    "    overall_count=0\n",
    "    macro_prec = 0;\n",
    "    for tp, count in tp_list:\n",
    "        if tp is None:\n",
    "            continue\n",
    "        simple_count +=1    \n",
    "        overall_tp += tp\n",
    "        overall_count += count\n",
    "        macro_prec += float(tp)/count\n",
    "        \n",
    "    macro_prec = macro_prec/simple_count\n",
    "    micro_prec = float(overall_tp)/overall_count\n",
    "    \n",
    "    return micro_prec, macro_prec\n",
    "\n",
    "\n",
    "# Integer Programming\n",
    "\n",
    "from itertools import izip\n",
    "from itertools import product\n",
    "from pulp import *\n",
    "import random\n",
    "\n",
    "def getscore(x,y,method, direction):\n",
    "    \"\"\"Get similarity score for a method and a direction \"\"\"\n",
    "    return getsim(x,y ,method, direction)\n",
    "    #return random.random()\n",
    "\n",
    "def disambiguate_ilp(C, method, direction):\n",
    "    \"\"\" Disambiguate using ILP \n",
    "        Inputs: \n",
    "            C: Candidate List [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: Similarity method\n",
    "            direction: embedding direction\"\"\"\n",
    "    #C = [('a','b','c'), ('e', 'f', 'g'), ('h', 'i')]\n",
    "\n",
    "    R1 = [zip([i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "\n",
    "    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n",
    "    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n",
    "\n",
    "    #RI1_flat = list(itertools.chain(*RI1))\n",
    "\n",
    "\n",
    "    R2=[]\n",
    "    for e in combination(R1,2):\n",
    "        R2 += [r for r in itertools.product(e[0], e[1]) ]        \n",
    "\n",
    "\n",
    "    #R2 = {r:str(r) for r in RI2}\n",
    "\n",
    "\n",
    "    \n",
    "    S = {((u0,u1),(v0,v1)):getscore(C[u0][u1][0],C[v0][v1][0], method, direction) for ((u0,u1),(v0,v1)) in R2}\n",
    "\n",
    "\n",
    "    prob = LpProblem(\"wsd\", LpMaximize)\n",
    "\n",
    "    R=list(itertools.chain(*R1)) + R2\n",
    "    R_vars = LpVariable.dicts(\"R\",R,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "    prob += lpSum([S[r]*R_vars[r] for r in R2])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for ri in R1:\n",
    "        prob += lpSum([R_vars[rij] for rij in ri])==1, (\"R1 %s constraint\")%i\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for r in R2:\n",
    "        prob += lpSum([R_vars[r[0]],R_vars[r[1]],-2*R_vars[r]]) >=0, (\"R_%s_%s constraint\"%(r[0], r[1]))\n",
    "\n",
    "    prob.solve() \n",
    "    #print(\"Status:\", LpStatus[prob.status])\n",
    "    #print(\"Score:\", value(prob.objective))\n",
    "    ids    = [C[r[0]][r[1]][0] for r in list(itertools.chain(*R1)) if R_vars[r].value() == 1.0]\n",
    "    titles = ids2title(ids)\n",
    "    return ids, titles\n",
    "        \n",
    "def disambiguate_ilp_2(C, method, direction):\n",
    "    \n",
    "    #C = [('a','b','c'), ('e', 'f', 'g'), ('h', 'i')]\n",
    "\n",
    "    #R1 = [zip([\"R\"z]*len(c),zip([i]*len(c), range(len(c)))) for i,c in enumerate(C)]\n",
    "    R1 = [zip(['R']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "    Q = [zip(['Q']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "\n",
    "    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n",
    "    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n",
    "\n",
    "    #RI1_flat = list(itertools.chain(*RI1))\n",
    "\n",
    "\n",
    "    R2=[]\n",
    "    for e in combination(R1,2):\n",
    "            R2 += [('R',(i,k),(j,l)) for (_,i,k),(_,j,l) in itertools.product(e[0], e[1]) ]        \n",
    "\n",
    "\n",
    "    #R2 = {r:str(r) for r in RI2}\n",
    "\n",
    "\n",
    "\n",
    "    S = {('R',(i,k),(j,l)):getscore(C[i][k][0],C[j][l][0], method, direction) for _,(i,k),(j,l) in R2}\n",
    "\n",
    "\n",
    "    prob = LpProblem(\"wsd\", LpMaximize)\n",
    "\n",
    "    R=list(itertools.chain(*R1)) + R2\n",
    "    Q=list(itertools.chain(*Q))\n",
    "    R_vars = LpVariable.dicts(\"R\",R,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "\n",
    "    Q_vars = LpVariable.dicts(\"Q\",Q,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "\n",
    "    prob += lpSum([S[r]*R_vars[r] for r in R2])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for ri in R1:\n",
    "        prob += lpSum([R_vars[rij] for rij in ri])==1, (\"R1 %s constraint\")%i\n",
    "        i += 1\n",
    "\n",
    "    prob += lpSum(Q_vars.values())==1, (\"Q constraint\")\n",
    "\n",
    "\n",
    "    for _,(i,k),(j,l) in R2:\n",
    "        prob += lpSum([R_vars[('R',i,k)],R_vars[('R',j,l)],-2*R_vars[('R',(i,k),(j,l))]]) >=0, (\"R_%s_%s constraint\"%((i,k),(j,l)))\n",
    "        prob += lpSum([Q_vars[('Q',i,k)],Q_vars[('Q',j,l)], -1*R_vars[('R',(i,k),(j,l))]]) >=0, (\"Q_%s_%s constraint\"%((i,k),(j,l)))\n",
    "\n",
    "    prob.solve() \n",
    "#     print(\"Status:\", LpStatus[prob.status])\n",
    "#     print(\"Score:\", value(prob.objective))\n",
    "\n",
    "#     for (_,i,k), q in Q_vars.items():\n",
    "#         if q.value()==1:\n",
    "#             print(\"central concept: \", id2title(C[i][k][0]))\n",
    "    ids    = [C[i][k][0] for _,i,k in list(itertools.chain(*R1)) if R_vars[('R',i,k)].value() == 1.0]\n",
    "    \n",
    "    \n",
    "    titles = ids2title(ids)\n",
    "    return ids, titles\n",
    "\n",
    "# key\n",
    "def evalkey(c, a, candslist, simmatrix):\n",
    "    resolved=[]\n",
    "    score=0;\n",
    "    for i in  range(len(candslist)):\n",
    "        if a==i:\n",
    "            resolved.append(c[0])\n",
    "            continue\n",
    "        cands = candslist[i]\n",
    "        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n",
    "        max_concept, max_sc = max(vb, key=lambda x: x[1])\n",
    "        score += max_sc\n",
    "        resolved.append(max_concept)\n",
    "    return resolved,score\n",
    "\n",
    "def key_quad(candslist, method, direction):\n",
    "    res_all=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "\n",
    "    for i in range(len(candslist)):\n",
    "        for j in range(len(candslist[i])):\n",
    "            res_ij =  evalkey(candslist[i][j], i, candslist, simmatrix)\n",
    "            res_all.append(res_ij)\n",
    "    res, score = max(res_all, key=lambda x: x[1])\n",
    "    #print(\"Score:\", score)\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "# Parallel Keyquad\n",
    "from functools import partial\n",
    "from multiprocessing import Pool as ThreadPool \n",
    "def Pevalkey((c, a), candslist, simmatrix):\n",
    "    resolved=[]\n",
    "    score=0;\n",
    "    for i in  range(len(candslist)):\n",
    "        if a==i:\n",
    "            resolved.append(c[0])\n",
    "            continue\n",
    "        cands = candslist[i]\n",
    "        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n",
    "        max_concept, max_sc = max(vb, key=lambda x: x[1])\n",
    "        score += max_sc\n",
    "        resolved.append(max_concept)\n",
    "    return resolved,score\n",
    "def Pkey_quad(candslist, method, direction):\n",
    "    res_all=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "    pool = ThreadPool(25) \n",
    "    \n",
    "    partial_evalkey = partial(Pevalkey, candslist=candslist, simmatrix=simmatrix)\n",
    "    I=[[j]*len(candslist[j]) for j in range(len(candslist))]\n",
    "    \n",
    "    res_all= pool.map(partial_evalkey, zip(itertools.chain(*candslist), itertools.chain(*I)))\n",
    "    pool.close() \n",
    "    pool.join() \n",
    "    \n",
    "    res, score = max(res_all, key=lambda x: x[1])\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "\n",
    "# Context Vector\n",
    "\n",
    "        \n",
    "\n",
    "def contextdisamb_1(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    for cands in candslist:\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    # for cands in candslist:\n",
    "    #     cveclisttitles.append([conceptrep(c, direction, get_titles=False) for c in cands])\n",
    "\n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    convec = cvec_arr.sum(axis=0)\n",
    "    from itertools import izip\n",
    "    res=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        \n",
    "        maxd=-1\n",
    "        mi=0\n",
    "        for v in cvec:\n",
    "            d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        res.append(cands[index][0]) \n",
    "        #print index,\"\\n\"\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "                                   \n",
    "def contextdisamb_2(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    for cands in candslist:\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    res=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "\n",
    "        maxd=-1\n",
    "        index = -1\n",
    "        mi=0\n",
    "\n",
    "        for v in cvec:\n",
    "            d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        if index==-1:\n",
    "            index=0\n",
    "        res.append(cands[index][0]) \n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cveclist_bdrs[i] = (b+index,b+index+1)\n",
    "        \n",
    "        aggr_cveclist[i] =  cvec_arr[b:e][index]\n",
    "        \n",
    "        candslist[i] = candslist[i][index][0]\n",
    "        \n",
    "        \n",
    "\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles\n",
    "\n",
    "def contextdisamb_3(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    for resolved in range(ambig_count):\n",
    "        cands_score_list=[]        \n",
    "        for i in range(len(candslist)):\n",
    "            cands = candslist[i]\n",
    "            b,e = cveclist_bdrs[i]\n",
    "            cvec = cvec_arr[b:e]\n",
    "            convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "            D=[]    \n",
    "            for v in cvec:\n",
    "                d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "                if np.isnan(d):\n",
    "                    d=0\n",
    "                D.append(d)\n",
    "            D=sorted(enumerate(D), key=lambda x: -x[1])\n",
    "            cands_score_list.append(D)\n",
    "\n",
    "        max_concept, _ = max(enumerate(cands_score_list), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n",
    "        max_candidate = cands_score_list[max_concept][0][0]\n",
    "        \n",
    "        b,e = cveclist_bdrs[max_concept]\n",
    "        cveclist_bdrs[max_concept] = (b+max_concept,b+max_concept+1)\n",
    "        aggr_cveclist[max_concept] =  cvec_arr[b:e][max_candidate]\n",
    "        \n",
    "        candslist[max_concept] = [candslist[max_concept][max_candidate]]\n",
    "                                  \n",
    "        #cframelist[max_index] =  [cframelist[max_index][cands_score_list[max_index][0][0]]]\n",
    "        #break\n",
    "            #print index,\"\\n\"\n",
    "    res = [c[0][0] for c in candslist]\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles        \n",
    "    \n",
    "    \n",
    "#########################\n",
    "# KeyBased Method\n",
    "#########################\n",
    "\n",
    "def get_candidate_representations(candslist, direction, method):\n",
    "    '''returns an array of vector representations. \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "      Outputs\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "                   is the representation of a candidate\n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where \n",
    "                   the embeddings for a concepts indicates start and end. In other words\n",
    "                   The embedding of candidates [ci1...cik] in candslist is\n",
    "                   cvec_arr[cveclist_bdrs[i][0]:cveclist_bdrs[i][1]] \n",
    "    '''\n",
    "    \n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(c[0], method=method, direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    return cvec_arr, cveclist_bdrs\n",
    "\n",
    "def entity_to_context_scores(candslist, direction, method):\n",
    "    ''' finds the similarity between each entity and its context representation\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "                   reperesentation of the candidates for cij reside        \n",
    "           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n",
    "                    where sij  is the similarity of c[i,j] to to ci-th context\n",
    "                    \n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    cands_score_list=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        S=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                s = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                s=0                \n",
    "            if np.isnan(s):\n",
    "                s=0\n",
    "            S.append(s)\n",
    "        cands_score_list.append(S)\n",
    "\n",
    "    return cvec_arr, cveclist_bdrs, cands_score_list\n",
    "\n",
    "\n",
    "def key_criteria(cands_score):\n",
    "    ''' helper function for find_key_concept: returns a score indicating how good a key is x\n",
    "        Input:\n",
    "            scroes for candidates [ci1, ..., cik] in the form of (i, [(ri1, si1), ..., (rik, sik)] ) \n",
    "            where (rij,sij) indicates that sij is the similarity of c[i][rij] to to cith context\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    if len(cands_score[1])==1 or cands_score[1][1][1]==0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    return (cands_score[1][0][1]-cands_score[1][1][1]) / cands_score[1][1][1]\n",
    "\n",
    "def find_key_concept(candslist, direction, method, ver=4):\n",
    "    ''' finds the key entity in the candidate list\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "            cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "            cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "            key_concept: the concept forwhich one of the candidates is the key entity\n",
    "            key_entity: candidate index for key_cancept that is detected to be key_entity\n",
    "            key_entity_vector: The embedding of key entity\n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs, cands_score_list = entity_to_context_scores(candslist, direction, method);\n",
    "    S=[sorted(enumerate(S), key=lambda x: -x[1]) for S in cands_score_list]\n",
    "        \n",
    "    if ver ==1: \n",
    "        key_concept, _ = max(enumerate(S), key=lambda x: x[1][0][1] if len(x[1])>1 else -1)\n",
    "    elif ver ==2: \n",
    "        key_concept, _ = max(enumerate(S), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n",
    "    elif ver ==3: \n",
    "        key_concept, _ = max(enumerate(S), key=lambda x: (x[1][0][1]-x[1][1][1])/(x[1][0][1]+x[1][1][1]) if len(x[1])>1 else -1)\n",
    "    elif ver ==4: \n",
    "        key_concept, _ = max(enumerate(S), key=key_criteria)\n",
    "    key_entity = S[key_concept][0][0]\n",
    "    \n",
    "    b,e = cveclist_bdrs[key_concept]\n",
    "    \n",
    "    key_entity_vector =  cvec_arr[b:e][key_entity]    \n",
    "    return cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector\n",
    "\n",
    "\n",
    "def keyentity_candidate_scores(candslist, direction, method, ver=4):\n",
    "    '''returns entity scores using key-entity scoring \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           ver: 1 for the method explained in the paper\n",
    "           \n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector = find_key_concept(candslist, direction, method, ver)\n",
    "    \n",
    "    # Iterate \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        cand_scores=[]\n",
    "\n",
    "        for v in cvec:\n",
    "            try:\n",
    "                d = 1-sp.spatial.distance.cosine(key_entity_vector, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "    return candslist_scores\n",
    "\n",
    "def keyentity_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank', ver=4):\n",
    "    '''Disambiguate a sentence using key-entity method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           ver: 1 for the method explained in the paper\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = keyentity_candidate_scores (candslist, direction, method, ver)\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "######\n",
    "def get_sim_matrix(candslist,method, direction):\n",
    "    concepts=  list(chain(*candslist))\n",
    "    concepts=  list(set(c[0] for c in concepts))\n",
    "    sims = pd.DataFrame(index=concepts, columns=concepts)\n",
    "    for cands1,cands2 in combinations(candslist,2):\n",
    "        for c1,c2 in product(cands1,cands2):\n",
    "            sims[c1[0]][c2[0]]= sims[c2[0]][c1[0]] = getsim(c1[0],c2[0] , method, direction)\n",
    "    return sims        \n",
    "\n",
    "def tagme_vote(c, a, candslist, simmatrix, pop):\n",
    "    v = 0\n",
    "    for b in  range(len(candslist)):\n",
    "        if a==b:\n",
    "            continue\n",
    "        cands = candslist[b]\n",
    "        if pop:\n",
    "            vb = [ci[1]*simmatrix[c[0]][ci[0]] for ci in cands]\n",
    "        else:\n",
    "            vb = [simmatrix[c[0]][ci[0]]  for ci in cands]\n",
    "        vb = sum(vb) / len(vb)\n",
    "        v += vb    \n",
    "    return v\n",
    "\n",
    "def tagme(candslist, method, direction, pop=False):\n",
    "    res=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        \n",
    "        maxd=-1\n",
    "        mi=0\n",
    "        #print len(cands)\n",
    "        for c in cands:\n",
    "            d = tagme_vote(c, i, candslist , simmatrix, pop);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        res.append(cands[index][0]) \n",
    "        #print index,\"\\n\"\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile wsd_eval.py \n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "from wsd import *\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "#word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.28.5.5/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.28.5.5')\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aida.json'), \n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "methods = (('ams', DIR_BOTH,'ilp'), ('wlm', DIR_IN,'ilp'),('rvspagerank', DIR_BOTH, 'ilp'),\n",
    "           ('wlm', DIR_IN, 'tagme'), ('rvspagerank', DIR_BOTH, 'tagme'),\n",
    "           ('rvspagerank', DIR_BOTH, 'keydisamb'), \n",
    "          )\n",
    "\n",
    "#methods = (('word2vec.500', None,'context4_4'),)\n",
    "methods = (('rvspagerank', DIR_BOTH,'keydisamb'),)\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "ws = options.win_size\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method, direction, op_method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, op_method: %s, direction: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                method, op_method, direction, max_t, ws)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(direction), op_method, str(max_t), str(ws), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename):\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=True)\n",
    "                \n",
    "                try:\n",
    "                    ids, titles = disambiguate_driver(C, ws, method, direction, op_method)\n",
    "                    tp = get_tp(M, ids) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    #raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \"op_method\": op_method, \"driection\": direction,\n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, op_method, graphtype(direction), max_t , ws, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example to show  how to get scores only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates:  [[(8551L, 0.673049645390071), (36684L, 0.05673758865248227), (9072L, 0.05319148936170213), (8618L, 0.02198581560283688), (670599L, 0.02127659574468085), (147976L, 0.02056737588652482), (145801L, 0.015602836879432624), (2475199L, 0.01347517730496454), (6675189L, 0.01347517730496454), (7630732L, 0.01276595744680851), (34257637L, 0.01276595744680851), (1656498L, 0.012056737588652482), (29007L, 0.011347517730496455), (4686150L, 0.010638297872340425), (10625032L, 0.010638297872340425), (410004L, 0.009219858156028368), (147969L, 0.007801418439716312), (293411L, 0.007801418439716312), (659333L, 0.007801418439716312), (2059942L, 0.007801418439716312)], [(4689460L, 0.7683377650950126), (32388L, 0.07944743498281341), (1147963L, 0.07477787145729295), (47923L, 0.010441662883455476), (11498176L, 0.0092094169531098), (136747L, 0.00888514170828199), (338269L, 0.00823659121862637), (305382L, 0.007069200337246255), (3659788L, 0.0057072443089694535), (199625L, 0.004085868084830404), (670780L, 0.003696737791037032), (4903654L, 0.0029833322524158504), (34379855L, 0.002918477203450289), (892970L, 0.0027239120565536027), (844394L, 0.0025942019586224787), (9014716L, 0.002334781762760231), (261020L, 0.0019456514689668591), (349641L, 0.001815941371035735), (24723693L, 0.0014268110772423634), (310287L, 0.0013619560282768014)], [(41188263L, 0.9584905660377359), (69802L, 0.02034054302807179), (14256525L, 0.0035895075931891393), (44636014L, 0.0017487344684767603), (100383L, 0.0016566958122411412), (127192L, 0.0014726184997699034), (39048453L, 0.0014726184997699034), (31678725L, 0.0013805798435342844), (21811420L, 0.0011044638748274275), (42465280L, 0.0011044638748274275), (519564L, 0.0009203865623561896), (5721983L, 0.0009203865623561896), (112776L, 0.0008283479061205706), (126024L, 0.0008283479061205706), (35589271L, 0.0008283479061205706), (3823591L, 0.0007363092498849517), (31540321L, 0.0007363092498849517), (90168L, 0.0006442705936493328), (3948353L, 0.0006442705936493328), (26413L, 0.0005522319374137137)]] \n",
      "\n",
      "Key Scores:  [[0.001941270901576253, 0.0047014296751956008, 0.0050782627100117717, 0.0027893462212920106, 0.0, 0.1219643510570052, 0.0025419558343383608, 2.4707884940355918e-05, 9.4584440312628359e-05, 4.1268981201381116e-05, 5.866932407350145e-05, 0.00023055146943651295, 0.0012327400484372086, 2.049109621071743e-05, 4.116298395906437e-05, 0.00013687930520778213, 0.14326456840453272, 0.00096306389301503703, 0.0001605394556251083, 0.00010906393589726893], [0.01702687760269872, 0.020531621487451801, 0.00045201005952200113, 0.99999999999999989, 0.00012720315766223678, 4.1602411121499649e-05, 0.005159178367515671, 2.3244387702847646e-05, 5.7884930760643272e-05, 0.001332037616436188, 0.01794166680743825, 0.013599003679850119, 0.0048419552776207686, 0.020859200784089316, 0.00094067764671745824, 0.00069344722581221596, 0.52077330806642086, 0.0035460935348398115, 0.00021047863989298587, 0.0030924067329198834], [0.0023350448844976501, 0.00065365809231143768, 0.0, 6.639289875054466e-05, 0.00015081558121143868, 0.0, 7.3766885181347774e-05, 0.00011509487493488457, 8.6985235976611008e-05, 7.5065744184810157e-05, 0.00081559604957792686, 6.0376679759444052e-05, 0.0, 1.9806884192452578e-05, 4.4785698225968673e-05, 0.0, 4.2799504309187775e-05, 0.000287006173471549, 5.7691062703368878e-05, 0.00013206004191390619]] \n",
      "\n",
      "context Scores:  [[0.0018234345019569886, 0.0033640592469172947, 0.0021483046518014914, 0.011057693644951794, 0.0014504895525973005, 0.010636451431427774, 0.000570627038578575, 1.1672003176532719e-05, 5.2614449148724596e-05, 0.00011687914262892285, 6.841877886498704e-05, 0.011059775208632328, 0.00065798175353370958, 1.241749461977637e-05, 0.00014114883437610359, 0.00031661071167521371, 0.011870117352422382, 0.0024988656209687932, 0.00067648680697729446, 0.00020443747091225539], [0.0006638364211485337, 0.00140727736851054, 2.7890100451566369e-05, 0.023068537158887237, 7.708170956188809e-06, 0.0013299670113470219, 0.0010935604421716816, 0.0081309340041690881, 0.00078589324869993149, 0.0011557876925444299, 0.0018099425585278439, 0.00061884058079886284, 0.00011354004570418574, 0.00086704941331416929, 1.3490518970704635e-05, 7.237040672736228e-06, 0.0023655605133673374, 0.0042523728883847589, 0.0051127780103550347, 0.00040923538042236807], [0.0094997010677193572, 0.0023760309468522367, 9.4308798992526199e-05, 0.00022621758086838195, 0.0018828985800527498, 0.00031919740426999965, 0.00023882407909492365, 0.00072461716591210035, 0.00030866536641194209, 0.00026866736804864733, 0.0013166322413797893, 0.00028586166740851571, 0.00030317001717605763, 0.00033655136887733583, 0.00011671054588058283, 0.0073083590844965629, 0.00017445739595456367, 0.00080204841330422383, 0.00033597571432653073, 0.010064215804515553]] \n",
      "\n",
      "ids:  [147969L, 47923L, 41188263L]\n",
      "titles:  ['David_II_of_Scotland', 'Queen_Victoria', 'Madrid']\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wsd import *\n",
    "import time\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=20, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "candslist_scores = keyentity_candidate_scores (C, 2, 'rvspagerank')\n",
    "print \"Key Scores: \", candslist_scores, \"\\n\"\n",
    "_, _, cands_score_list = entity_to_context_scores(C, 2, 'rvspagerank');\n",
    "print \"context Scores: \", cands_score_list,\"\\n\"\n",
    "\n",
    "ids, titles = disambiguate_driver(C, 2, 'rvspagerank', 2, 'keydisamb')\n",
    "print \"ids: \", ids\n",
    "print \"titles: \", titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
