{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# April  16\n",
    "1. created a surface-replace by skipping the opening, it's being trained with windows sized 5 and 10\n",
    "a normal surface-replae is being trained with window 5, so now I have windows size 5 and 10 for both, with dimensionaltiy 500\n",
    "\n",
    "2. I modifid the context to contain source id and title, and paragraph no. It's been created, I deleted the index, and restarted, so ready to import\n",
    "\n",
    "## TODO\n",
    "1. run the split:\n",
    "go to    \n",
    "`cat ~/backup/datasets/cmod/contexts.json | split -a 10 -l 500000 - context`   \n",
    "2. bash loadwiki: done\n",
    "3. bash optimize: done\n",
    "4. ** HOW TO modify materialization? **\n",
    "5. Create the mentions for the guy (openning text?)\n",
    "4. Evaluate the graph embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# April 18\n",
    "1. word2vec Embedding are trained, with skip line and everything\n",
    "# TODO\n",
    "materialization kinda done, but not very much tested. you see a file (old.delet), that should be th ground truth\n",
    "so, go ahead and do the following\n",
    "0. There is an issue with the naming the train and test\n",
    "1. try with no skip line and downsample, check the results be the sample as the old one\n",
    "2. if yes, try with skip line, take a look, compare the logs (which tells you how many in train and test)\n",
    "3. if correct, run one with max_anchor 1000, and one with no down sample\n",
    "4. integize all\n",
    "5. zip the cmod dir and save it in a good place\n",
    "6. create the mentions for the guy\n",
    "7. sync with ares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a     test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "April 21\n",
    "almost all done\n",
    "The materialization is being done, just integize it and you're good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "# Wikisim\n",
    "## Wikisim provides the following services:\n",
    "* **Vector-Space Representation of Wikipedia Concepts**\n",
    "* **Semantic Relatedness between Wikipedia Concepts**\n",
    "* **Wikification: Entity Linking to Wikipedia**\n",
    "\n",
    "* For a more detailed document and the main algorithm, check:\n",
    "\n",
    "* [Wikisim Notebook](wikisim/wikisim.ipynb)\n",
    "This is a simple and step by step explanation of concept-representation and calculating semantic relatedness using Wikipedia. We start by preprocessing and building the api.\n",
    "\n",
    "* [Wikification Notebook](wikification/wikify.ipynb)\n",
    "This notbook contains Word-sense Disambiguation/Wikifaction modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"container\">\n",
    "<div class=\"panel-group\">\n",
    "    <div class=\"panel panel-default\">\n",
    "        <div class=\"panel-heading\">\n",
    "            <a name=\"doc\"></a>\n",
    "            <h1> Documentation </h1>\n",
    "        </div>\n",
    "        <div class=\"panel-body\">\n",
    "            <h2> Single mode </h2>\n",
    "            <p>The webservice provides three basic functions (or tasks): \n",
    "                Wikification, Simiarity and Embedding calculation. All requests can be processed in\n",
    "                <em>single</em> or <em>batch mode</em>.\n",
    "            </p>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <strong>Wikification:</strong> <br>\n",
    "                    parameters: <br>\n",
    "                    <code>mentionmethod</code>: should be 0 for using <em>CoreNLP</em>, 1 \n",
    "                    for our <em>high-precision trained model</em> and 2 for our <em>high-recall trained method</em><br>\n",
    "                    <code>text</code>: the text to be wikified\n",
    "                </li>\n",
    "                <li>\n",
    "                    <strong>Similarity Calculation</strong>: <br>\n",
    "                    parameters: <br>\n",
    "                    <code>task</code>: should be 'sim' for this task <br>\n",
    "                    <code>direction</code>: 0 for using incomming links, 1 for outgoing links and 2 for both. We recommend using only\n",
    "                    outgoing links as it provides decent results and is significantly faster<br>\n",
    "                    <code>c1 (and c2)</code>: the concept to be processed<br>\n",
    "                    Example (using curl): <br>\n",
    "                    <code>curl --request POST 'http://ares.research.cs.dal.ca/~sajadi/wikisim/cgi-bin/cgi-pairsim.py' --data \"task='sim'\" --data \"dir=1\"  --data \"c1=Tehran\" --data \"c2=Sanandaj\"</code>\n",
    "                </li>        \n",
    "                <li>\n",
    "                    <strong>Concept Representation (Embedding): </strong><br>\n",
    "                    parameters: <br>\n",
    "                    <code>task</code>: should be 'emb' for this task <br>\n",
    "                    <code>direction</code>: 0 for using\n",
    "                    <em>incomming links</em>, 1 for\n",
    "                    <em>outgoing links</em> and 2 for\n",
    "                    <em>both</em>. We recommend using only outgoing links as it provides decent results and is significantly\n",
    "                    faster\n",
    "                    <br>\n",
    "                    <code>cutoff</code>: the dimensionality of the embedding. This parameter is only used for returning the embeddings,\n",
    "                    the similarity calculation always uses all the dimensions. <br>\n",
    "                    <code>c1</code>: the concept to be processed <br>\n",
    "                    Example (using curl): <br>\n",
    "                    <code>curl --request POST 'http://ares.research.cs.dal.ca/~sajadi/wikisim/cgi-bin/cgi-pairsim.py' --data \"task=emb\" --data \"dir=1\" --data \"cutoff=10\" --data \"c1=Sanandaj\"</code>\n",
    "                </li>\n",
    "            </ul>\n",
    "            <h2> Batch mode </h2>\n",
    "            <p>We strongly recommend using batch mode, either by post request and sending the file, or simply uploading\n",
    "                the file in the batch mode input. For embeddings, each line of the file contains a single concept.\n",
    "                For similarity calculation, file should be tab seperated, each line containing a pair of Wikipedia\n",
    "                Concepts. </p>\n",
    "            <p>The parameters are the same, however, the target cgi-files are different:</p>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    Wikification: use <code>cgi-batchwikify.py</code><br>\n",
    "                    Example (using curl): <br>\n",
    "                    <code></code>            \n",
    "                </li>\n",
    "                <li>\n",
    "                    Wikification: use <code>cgi-batchsim.py</code><br>\n",
    "                    Example (using curl): <br>\n",
    "                    <code></code>            \n",
    "                </li>\n",
    "                <li>\n",
    "                    Wikification: use <code>cgi-batchsim.py</code><br>\n",
    "                    Example (using curl): <br>\n",
    "                    <code></code>            \n",
    "                </li>\n",
    "            </ul>\n",
    "        \n",
    "        </div>\n",
    "        \n",
    "    </div>\n",
    "</div>\n",
    "<div class=\"panel-group\">\n",
    "    <div class=\"panel panel-default\">\n",
    "        <div class=\"panel-heading\">\n",
    "            <a name=\"download\"></a>\n",
    "            <h1> Download </h1>\n",
    "            <h2> Option 1. Download the Embeddings </h2>\n",
    "            <p>\n",
    "                <strong> Current Version: enwiki20160305 </strong>\n",
    "            </p>\n",
    "            <p> You can obviously download only the embeddings, for the following reasons we recommend to use the\n",
    "                second option and use our small API:\n",
    "                <ol>\n",
    "                    <li>\n",
    "                        The embeddings are provided for wikipedia concepts page_ids and you need another table to find the concepts titles, moreover,\n",
    "                        redirect concepts are not included, so let's say you want to find the embeddings for\n",
    "                        \"US\", you have to follow the following steps:\n",
    "                        <ol>\n",
    "                            <li>\n",
    "                                From the page table, find \"US\", and you see that its redirect field is 1, meaning that it's a redirect page; take its id,\n",
    "                                that is: 31643\n",
    "                            </li>\n",
    "                            <li>\n",
    "                                Go to redirect table and find out that it's redirected to 3434750 (the id for United_States)\n",
    "                            </li>\n",
    "                            <li>\n",
    "                                Go to embedding table and find the embedding for 3434750\n",
    "                            </li>\n",
    "                        </ol>\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        The nonzero dimentions are not included in the embedding, so there is a need for efficient alignment\n",
    "                    </li>\n",
    "                </ol>\n",
    "                <p> But if you still want to use your own data-sturctures, download the following tables:</p>\n",
    "                <ol>\n",
    "                    <li>\n",
    "                        <a href=\"http://cgm6.research.cs.dal.ca/~sajadi/downloads/wikisim/enwiki-20160305-page.main.tsv.gz\">Page Table</a>\n",
    "                        <p>Layout:</p>\n",
    "                        <code>page_id , page_namespace (0:page,14: Category) , page_title , page_is_redirect </code>\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        <a href=\"http://cgm6.research.cs.dal.ca/~sajadi/downloads/wikisim/enwiki-20160305-redirect.main.tsv.gz\"> Redirect Table</a>\n",
    "                        <p>Layout:</p>\n",
    "                        <code> rd_from , rd_to </code>\n",
    "                    </li>\n",
    "                    <p>As stated in the paper, out-links are shorter and leads to faster process. If you want to\n",
    "                        get the full embedding for a word, find both its in-embedding and out-embedding and add\n",
    "                        them up</p>\n",
    "                    <li>\n",
    "                        <a href=\"http://cgm6.research.cs.dal.ca/~sajadi/downloads/wikisim/enwiki-20160305-pagelinksorderedin.main.tsv.gz\">Embeddings (in-links)</a>\n",
    "                        <p>Layout:</p>\n",
    "                        <code> page_id , embedding in json format {id1:value1, ..., idn,valuen} </code>\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        <a href=\"http://cgm6.research.cs.dal.ca/~sajadi/downloads/wikisim/enwiki-20160305-pagelinksorderedout.main.tsv.gz\">Embeddings (Out-Links) </a>\n",
    "                        <p>Layout:</p>\n",
    "                        <code> page_id , embedding in json format {id1:value1, ..., idn,valuen} </code>\n",
    "                    </li>\n",
    "                </ol>\n",
    "                <h2>Option 2. Pre-embedding + Small API (Recommended)</h2>\n",
    "                <p>\n",
    "                    <strong> Current Version: enwiki20160305 </strong>\n",
    "                </p>\n",
    "                <p>Follow the full instructions in the\n",
    "                    <a href=\"https://github.com/asajadi/wikisim/blob/master/api/api.ipynb\">the jupyter notebook </a>\n",
    "                </p>\n",
    "                <h2>Option 3. If you want to start from scratch and experience another version of Wikipedia: </h2>\n",
    "                <p>Follow the full instructions in\n",
    "                    <a href=\"https://github.com/asajadi/wikisim/blob/master/wikisim/wikisim.ipynb\">the jupyter notebook </a>\n",
    "                </p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"panel-group\">\n",
    "    <div class=\"panel panel-default\">\n",
    "        <div class=\"panel-heading\">\n",
    "            <a name=\"about\"></a>\n",
    "            <h1> About </h1>\n",
    "        </div>\n",
    "        <div class=\"panel-body\">\n",
    "            <p>\n",
    "                <a title=\"Armin Sajadi\" href=\"http://projects.cs.dal.ca/visualtextanalytics/people/sajadi/\">Armin Sajadi</a> - Faculty of Computer Science\n",
    "                <br> Ryan Amaral- Faculty of Computer Science\n",
    "            <p>\n",
    "                <strong>\n",
    "                    <a href=\"../resrc/HitSim.pdf\">Slides</a>\n",
    "                    <br />\n",
    "                </strong>\n",
    "            </p>\n",
    "            <p>\n",
    "                <strong>Published Reports</strong>\n",
    "            </p>\n",
    "            <p>Armin Sajadi, Evangelos E. Milios, Vlado Keselj, Jeannette C. M. Janssen, \"Domain-Specific Semantic\n",
    "                Relatedness from Wikipedia Structure: A Case Study in Biomedical Text\"\"\n",
    "                <a href=\"http://dblp.uni-trier.de/db/conf/cicling/cicling2015-1.html#SajadiMKJ15\">CICLing (1) 2015</a>: 347-360 (\n",
    "                <a href=\"http://dblp.uni-trier.de/rec/bibtex/conf/cicling/SajadiMKJ15\">bib</a>,\n",
    "                <a href=\"http://link.springer.com/chapter/10.1007%2F978-3-319-18111-0_26\">pdf</a>)</p>\n",
    "            <p>Armin Sajadi,\"\n",
    "                <em>Graph-Based Domain-Speciﬁc Semantic Relatedness from Wikipedia</em>\", Canadian AI 2014, LNAI\n",
    "                8436, pp. 381–386, 2014 (\n",
    "                <a href=\"../resrc/caai14.bib\">bib</a>,\n",
    "                <a href=\"http://link.springer.com/chapter/10.1007%2F978-3-319-06483-3_42#\">pdf</a>)</p>\n",
    "            <p>\n",
    "                <strong>Awards</strong>\n",
    "            </p>\n",
    "            <p>Verifiability, Reproducibility, and Working Description Award, Computational Linguistics and In-\n",
    "                telligent Text Processing, 16th International Conference, CICLing 2015, Cairo, Egypt, April 14-20,\n",
    "                2015\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "<div class=\"panel-group\">\n",
    "    <div class=\"panel panel-default\">\n",
    "        <div class=\"panel-heading\">\n",
    "            <a name=\"contact\"></a>\n",
    "            <label> Contact </label>\n",
    "            <p>\n",
    "                <a title=\"Armin Sajadi\" href=\"http://web.cs.dal.ca/~sajadi/\">Armin Sajadi</a>\n",
    "            </p>\n",
    "            We appreciate and value any question, special feature request or bug report, just let us know at:\n",
    "            <p>sajadi[at][cs][dal][ca]</p>\n",
    "            <p>asajadi[at][gmail][dot][com]</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
